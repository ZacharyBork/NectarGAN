{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to NectarGAN docsite This site is a work in progress. You may encounter incorrect formatting, broken links, or outdated information as you read through. If you are interested in helping to maintain this site, please refer to the contribution guide.Help is always appreciated!"},{"location":"api/","title":"NectarGAN API - Home","text":""},{"location":"api/#the-nectargan-api-is-a-fully-modular-highly-extensible-pytorch-based-framework-comprised-of-easy-to-use-building-blocks-for-assembling-training-and-testing-conditional-gan-models","title":"The NectarGAN API is a fully modular, highly extensible PyTorch-based framework comprised of easy to use building blocks for assembling, training, and testing conditional GAN models.","text":""},{"location":"api/#the-api-documentation-is-broken-down-into-a-number-of-sections","title":"The API documentation is broken down into a number of sections:","text":""},{"location":"api/#config","title":"Config","text":"<p>At the core of the NectarGAN API's functionality is the JSON-based configuration system. Config files are parsed by the <code>ConfigManager</code> into a set of <code>Config</code> dataclasses, allowing for easy access to any config value anywhere you need it. The configuration system is also at the core of the experiment tracking mechanisms in the NectarGAN API.</p> <p>For more information, please see here</p>"},{"location":"api/#models","title":"Models","text":"<p>The API provides a modular UNet-style generator and PatchGAN discriminator model, both of which allow you to drop in your own convolutional blocks if you so choose.</p> <p>For more information about the UNet model, please see here. For more information about the PatchGAN model, please see here</p>"},{"location":"api/#dataset","title":"Dataset","text":"<p>The API provides a helper class called <code>PairedDataset</code> for loading dataset data for paired images translation models. These is also a helper class called <code>Transformer</code> for performing Albumentations-based dataset augmentations.</p> <p>For more information, please see here.</p>"},{"location":"api/#trainers","title":"Trainers","text":"<p>The API's <code>Trainer</code> class is an inheritable baseline class for training GAN models with a overrideable training methods and a number of helper functions for speeding up model creation. The Pix2pixTrainer serves as an example implementation of a Pix2pix-style[^1] cGAN model using the <code>Trainer</code> class as a base.</p> <p>For more information, please see here.</p>"},{"location":"api/#testers","title":"Testers","text":"<p>The API's <code>Tester</code> class allows you to test your trained model on a test dataset.</p> <p>For more information, please see here.</p>"},{"location":"api/#losses","title":"Losses","text":"<p>The API includes a robust and hightly extensible loss tracking and management system, the core of which is the <code>LossManager</code>, a drop-in solution for handling all things related to loss in your models.</p> <p>For more information about the <code>LossManager</code>, please see here. For information about loss specs, simple functions that can be used to define reuseable objective functions in combination with the <code>LossManager</code>, please see here. For more information about the loss Modules included with the NectarGAN API, please see here.</p>"},{"location":"api/#scheduling","title":"Scheduling","text":"<p>The API has a simple schedule wrapper, allowing you to define schedules as Python functions and apply them to learning rates and loss weights.</p> <p>For more information, please see here.</p>"},{"location":"api/#visdom","title":"Visdom","text":"<p>The API includes a utility class called <code>VisdomVisualizer</code> for live loss graphing and visualization of [x, y_fake, y] image sets during training via Visdom.</p> <p>For more information, please see here.</p>"},{"location":"api/#onnx-tools","title":"ONNX Tools","text":"<p>The API includes a simple ONNX converter class and model tester script for convenience. </p> <p>For more information, please see here.</p> <p>[^1]: Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2017)</p>"},{"location":"faq/","title":"NectarGAN \u2013 Frequently Asked Questions (FAQ)","text":"<p>## Sections 1. Installation and Environment 2. Dataset and Configuration 3. Training (Toolbox and CLI) 4. Toolbox Overview 5. Testing and Reviewing Results 6. ONNX Export and Deployment 7. Scheduling, Losses and Internals 8. Troubleshooting</p>"},{"location":"faq/#installation-and-environment","title":"Installation and Environment","text":""},{"location":"faq/#1-what-platforms-are-supported","title":"1) What platforms are supported?","text":"<p>NectarGAN has been tested on Windows. Linux support is planned. See the Getting Started page for more info.</p>"},{"location":"faq/#2-what-python-versions-are-supported","title":"2) What Python versions are supported?","text":"<p>Python &gt;= 3.12</p>"},{"location":"faq/#3-how-do-i-install-it","title":"3) How do I install it?","text":"<ul> <li>Clone the repo: <code>git clone https://github.com/ZacharyBork/NectarGAN.git</code></li> <li>Create a fresh environment and install:</li> </ul> <pre><code>pip install .\n\n--- or ---\n\nconda env create -f environment.yml\n</code></pre> <p>Dev/testing install:</p> <pre><code>pip install -e \".[dev]\"\n\n-- or --\n\npip install -r requirements.dev.txt\n</code></pre>"},{"location":"faq/#4-how-can-i-verify-my-installation-works","title":"4) How can I verify my installation works?","text":"<p>Run the test suite with pytest. See here for more information.</p>"},{"location":"faq/#5-i-tried-running-the-toolboxcli-but-it-says-im-missing-pytorch","title":"5) I tried running the Toolbox/CLI, but it says I'm missing PyTorch?","text":"<p>PyTorch is not included in the core dependencies. It must be installed separately. PyTorch has multiple versions, each tied to a compute platform, and you should choose the one that best suits your needs based on the system you are running NectarGAN on. Installation instructions for PyTorch can be found on their website.</p> <p>[!NOTE] At this time, NectarGAN has been tested on the CUDA 12.6, CUDA 12.8, and CPU compute platforms.</p>"},{"location":"faq/#dataset-and-configuration","title":"Dataset and Configuration","text":""},{"location":"faq/#1-what-dataset-layout-does-paired-training-expect","title":"1) What dataset layout does paired training expect?","text":"<p>Your root dataset directory should be laid out as follows:</p> <pre><code>root/\n\u251c\u2500 train/\n\u251c\u2500 val/\n\u251c\u2500 test/\n</code></pre> <p>[!NOTE] The <code>test</code> directory is optional. The dataset images in this directory can be used in a separate post-train model validation. See here.</p>"},{"location":"faq/#2-how-do-i-setoverride-config-values","title":"2) How do I set/override config values?","text":"<p>Toolbox: The Toolbox generates training configs dynamically. When using it, all config values can be set via the UI. See here.</p> <p>CLI: The default config file used by the training scripts can be found at <code>nectargan/config/default.json</code>. You can open this file in any text editor to override values. See here for more information.</p> <p>Custom Training Script: There are a number of ways to manage configuration options when writing a custom training script. See here for more information. </p>"},{"location":"faq/#3-can-i-train-on-grayscale-or-multi-channel-images","title":"3) Can I train on grayscale or multi-channel images?","text":"<p>This can be controlled by setting the value of <code>in_channels</code> on the UNet and PatchGAN models. 1 for grayscale, 3 for RGB. I haven't tested multi-channel inputs much; you may encounter problems. I will check on this and fix it if necessary at some point in the future. ONNX model conversion currently only supports 3 channels. See here and here for more information on ONNX conversion.</p>"},{"location":"faq/#4-how-do-i-select-the-mapping-direction-a-b-vs-b-a","title":"4) How do I select the mapping direction (A-&gt;B vs B-&gt;A)?","text":"<p>By setting the value of <code>config.dataloader.direction</code> in your config file. Valid values are <code>AtoB</code> and <code>BtoA</code>. See here for more information regarding dataset loading.</p>"},{"location":"faq/#training-toolbox-and-cli","title":"Training (Toolbox and CLI)","text":""},{"location":"faq/#1-how-do-i-start-pix2pix-training-from-the-cli","title":"1) How do I start Pix2pix training from the CLI?","text":"<p>See here.</p>"},{"location":"faq/#2-what-loss-setups-are-available","title":"2) What loss setups are available?","text":"<p>There are 4 pre-build loss subspecs for the Pix2pix model objective. These are: - <code>basic</code> - <code>basic+vgg</code> (adds VGG19 perceptual) - <code>extended</code> (adds L2, Sobel, Laplacian) - <code>extended+vgg</code></p> <p>See the documentation on loss functions and the Pix2pix objective function for more information on the behaviour of these loss subspecs.</p>"},{"location":"faq/#3-how-can-i-control-loss-weights","title":"3) How can I control loss weights?","text":"<p>Toolbox: On the Training panel, under the loss tab, you will find sliders for the various loss functions.</p> <p>CLI: In the config file, under <code>config.train.loss</code>, you can set weights for the various loss functions by changing the correspoding value.</p> <p>Custom Training Script: See the LossManager documentation for information on setting loss weights and applying weight schedules.</p>"},{"location":"faq/#4-can-i-resume-training-from-a-checkpoint","title":"4) Can I resume training from a checkpoint?","text":"<p>Toolbox: Yes. On the Training tab, check the <code>Continue Train</code> checkbox and input the epoch you would like to load. When training begins, Toolbox will look for the config file for the given epoch in the experiment directory and load it, if present, to continue training.</p> <p>CLI: Yes. Just use the <code>-f</code> <code>--config_file</code> flag and pass it the path the the config file for the epoch you would like to resume training from. See the CLI documentation for more information.</p> <p>Custom Training Script: See here.</p>"},{"location":"faq/#5-how-are-learning-rates-scheduled","title":"5) How are learning rates scheduled?","text":"<p>Toolbox: On the Training panel, you can set a learning rate schedule with the <code>Epochs</code>, <code>Epochs Decay</code>, and <code>Initial</code> and <code>Target</code> learning rate. See here for more info. </p> <p>CLI: <code>Epochs</code>, <code>Epochs Decay</code>, and <code>Initial</code> and <code>Target</code> learning rate can all be set in the config file, under <code>config.train.generator.learning_rate</code> and <code>config.train.discriminator.learning_rate</code>.</p> <p>Custom Training Script: See here for more information on scheduling.</p>"},{"location":"faq/#6-what-input-resolution-can-i-use","title":"6) What input resolution can I use?","text":"<p>Any reasonable resolution. Higher resolutions take longer to train and are oftentimes more difficult to train in a stable manner. They also require more memory which is ultimately the limiting factor. Individual training images (not paired), should have a 1:1 aspect ratio, and a power of two resolution. Popular choices are <code>256x256</code> and <code>512x512</code>. Sometimes higher if batch size is kept low. </p> <p>Just be sure to use a reasonable number of layers for your chosen resolution. Higher resolutions generally require a higher layer count for both generator and discriminator to produce acceptable results. But too many layers will cause training to fail, as the tensor will be downsampled too far before hitting the bottleneck.</p> <p>See here for more information.</p>"},{"location":"faq/#7-im-seeing-checkerboard-artifacts-how-can-i-reduce-them","title":"7) I\u2019m seeing checkerboard artifacts. How can I reduce them?","text":"<p>These artifacts are incredibly common with pixel to pixel GAN models. Sometimes training longer will help, or just increasing the decay epochs to give the model longer to settle. You can also try using the transposed convolution upsampling method for the generator (see here), or using the Residual UNet block instead of the standar UNet block. Both of these solutions are frequently able to reduce or completely remove the checkerboarding.</p> <p>If none of the above solutions work, you may also try adjusting your loss values. Sometimes adding L2 or VGG Perceptual, and/or reducing L1, can also help to eliminate this artifacting. </p>"},{"location":"faq/#toolbox-overview","title":"Toolbox Overview","text":""},{"location":"faq/#1-what-are-the-toolbox-sections-and-shortcuts","title":"1) What are the Toolbox sections and shortcuts?","text":"<p>See here.</p>"},{"location":"faq/#2-where-do-outputs-go-and-how-are-experiments-versioned","title":"2) Where do outputs go and how are experiments versioned?","text":"<p>Experiments will be exported to the <code>Output Root</code>, in directories named <code>Experiment Name</code>. These directories are versioned automatically (see here). <code>Output Root</code> and <code>Experiment Name</code> are set in different places depending upon how training is initiated:</p> <p>Toolbox: Both can be set on the Experiment panel.</p> <p>CLI: Both are set in the config file under <code>config.common</code>.</p> <p>Custom Training Script: Same as CLI.</p>"},{"location":"faq/#3-can-i-change-ui-performance-vs-feedback-rate","title":"3) Can I change UI performance vs feedback rate?","text":"<p>Yes. When training with Toolbox, the actual training happens in a separate thread from UI, and that thread sends updates back to the UI every so often. Doing this too frequently is performance intensive and slows down training significantly. You could have it send back an update every iteration, though, if you wanted. </p> <p>This update frequency can be changed by going to the Settings panel and setting the value of <code>Training Update Rate</code>. This value can be changed at runtime, so you can decrease it briefly to get a better look at the models output, then increase it again to revert to your initial training speed.</p>"},{"location":"faq/#testing-and-reviewing-results","title":"Testing and Reviewing Results","text":""},{"location":"faq/#1-how-do-i-test-a-trained-model","title":"1) How do I test a trained model?","text":"<p>Toolbox: Using the Testing panel. See here</p> <p>CLI: See here for information on model testing via CLI.</p> <p>Custom Testing Script: See the Tester class documentation.</p>"},{"location":"faq/#2-how-do-i-review-my-experiments","title":"2) How do I review my experiments?","text":"<p>The Toolbox Review panel offers an easy way to review the results of your model test. See here and here for more information.</p>"},{"location":"faq/#3-how-do-i-log-losses-during-training","title":"3) How do I log losses during training?","text":"<p>Toolbox: On the Training panel, under the Loss tab, you can enable <code>Log Losses During Training</code> and configure the logging behaviour.</p> <p>CLI: Using the <code>--log_losses</code> flag. See here.</p> <p>Custom Training Script: See the LossManager documentation.</p>"},{"location":"faq/#onnx-export-and-deployment","title":"ONNX Export and Deployment","text":""},{"location":"faq/#1-how-do-i-export-to-onnx","title":"1) How do I export to ONNX?","text":"<p>Toolbox: On the Utilities panel, you will find a set of tools that allow you to convert your models to run on the ONNX runtime, and to test your converted models. See here for more information. </p> <p>CLI: This is currently not supported.</p> <p>Custom Script: See the ONNX tools documentation.</p>"},{"location":"faq/#2-why-do-i-get-instance-norm-warnings-when-exporting","title":"2) Why do I get instance-norm warnings when exporting?","text":"<p>See here.</p>"},{"location":"faq/#3-can-i-test-an-exported-onnx-model-inside-the-toolbox","title":"3) Can I test an exported ONNX model inside the Toolbox?","text":"<p>Yes, see here.</p>"},{"location":"faq/#4-does-onnx-export-support-non-rgb-inputs","title":"4) Does ONNX export support non-RGB inputs?","text":"<p>No, currently the ONNXConverter only supports 3 channels (RGB).</p>"},{"location":"faq/#scheduling-losses-and-internals","title":"Scheduling, Losses and Internals","text":""},{"location":"faq/#1-what-is-the-lossmanager-and-why-use-it","title":"1) What is the LossManager and why use it?","text":"<p>The Loss Manager is a comprehensive module for managing everything related to model loss during training. It is highly flexible and configurable, and allows you to easily and accurately manage complex objective functions.</p> <p>See the Loss Manager documentation for more information.</p>"},{"location":"faq/#2-what-are-loss-specs","title":"2) What are \u201closs specs\u201d?","text":"<p>Loss specs are drop in objective functions which you can pre-define, and feed in to a Loss Manager, allowing you to more carefully build, track, and reuse objectives from model to model.</p> <p>See the Loss Spec documentation for more information, and the Pix2pix Objective Function for an example of a loss spec.</p>"},{"location":"faq/#3-how-do-schedules-integrate-with-training","title":"3) How do schedules integrate with training?","text":"<p>NectarGAN offers a generic Scheduler, and a wrapper around the native PyTorch learning rate scheduler called TorchScheduler. This allows you to use the same Schedule Functions for each. </p> <p>The TorchScheduler is predominantly used for learning rate, to take advantage of the inherant integration with the PyTorch optimizers. The generic Scheduler is predominantly used for loss weight scheduling, though you could use it for whatever you want in your own models.</p> <p>See here to get started with scheduling in NectarGAN.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#1-training-is-slow-in-the-toolbox-any-tips","title":"1) Training is slow in the Toolbox. Any tips?","text":"<p>Increase Training Update Rate (see here), and avoid very low dump frequencies for loss logs, as these can potentially cause lag spikes.</p>"},{"location":"faq/#2-i-cant-find-my-outputscheckpoints-where-are-they","title":"2) I can\u2019t find my outputs/checkpoints. Where are they?","text":"<p>All files related to a given training session will be exported to your <code>Output Root</code> directory, in a subdirectory named after your current <code>Experiment Name</code>. These directories will be automatically versioned, so be sure to look for the latest one.</p>"},{"location":"faq/#3-cli-wont-see-my-config-whats-used-by-default","title":"3) CLI won\u2019t see my config. What\u2019s used by default?","text":"<p>If the <code>-f</code> flag isn't used, the training/testing scripts will instead use the default config file located at <code>nectargan/config/default.json</code>.</p>"},{"location":"getting_started/","title":"NectarGAN - Getting Started","text":""},{"location":"getting_started/#this-document-will-guide-you-through-the-process-of-installing-nectargan-and-get-you-started-using-the-nectargan-toolbox-training-and-testing-models-from-the-command-line-and-interacting-with-the-nectargan-api","title":"This document will guide you through the process of installing NectarGAN, and get you started using the NectarGAN Toolbox, training and testing models from the command line, and interacting with the NectarGAN API.","text":"<p>[!IMPORTANT] Currently, NectarGAN has only been tested on Windows. Linux validation is planned for the near future and with it will come Linux-specific install instructions.</p> <p>This is high on the list of planned features but if you want to give it a try before I get to it, please do let me know what your experience is like.</p>"},{"location":"getting_started/#installing-nectargan","title":"Installing NectarGAN","text":""},{"location":"getting_started/#windows","title":"Windows","text":"<p>[!TIP] It is recommended to install NectarGAN in a fresh environment to avoid dependency conflicts, especially as it is still in active development. </p> <p>NectarGAN requires a Python version &gt;= <code>3.12</code>.</p> <ol> <li>Clone the repository: <code>git clone https://github.com/ZacharyBork/NectarGAN.git</code></li> <li>Navigate to the repository root and run:</li> <li>Pip:</li> </ol> <pre><code>pip install .\n</code></pre> <ul> <li>Anaconda:</li> </ul> <pre><code>conda env create -f environment.yml\n</code></pre> <ol> <li>Install your preferred version of PyTorch: </li> </ol>"},{"location":"getting_started/#from-httpspytorchorgget-startedlocally","title":"From: https://pytorch.org/get-started/locally/","text":"Version Command CPU-Only <code>pip install torch torchvision</code> CUDA 11.8 <code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118</code> CUDA 12.6 <code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126</code> CUDA 12.8 <code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128</code>"},{"location":"getting_started/#nectargan-docker","title":"NectarGAN Docker","text":"<p>It is also possible to run headless training and testing with NectarGAN from a Docker container, using Visdom for runtime data visualization!</p> <p>Please see the Docker quickstart guide for more information.</p>"},{"location":"getting_started/#nectargan-toolbox","title":"NectarGAN Toolbox","text":"<p>The Toolbox quickstart guide is split in to three sections which are intended to be followed in order.</p> <p>Section 1: Training a Model Section 2: Testing a Model Section 3: Reviewing Results</p>"},{"location":"getting_started/#nectargan-cli","title":"NectarGAN CLI","text":"<p>For more information regarding the CLI options currently offered by NectarGAN, please see the NectarGAN CLI quickstart.</p>"},{"location":"getting_started/#nectargan-api","title":"NectarGAN API","text":"<p>Please see the API documentation for more information on getting started with the NectarGAN API.</p>"},{"location":"getting_started/#testing-your-installation","title":"Testing your installation","text":"<p>If you would like to test whether your installation is working correctly, NectarGAN includes a number of prebuilt tests which are run in different ways depending upon your installation type. See here for more information.</p>"},{"location":"getting_started/#downloading-a-premade-dataset","title":"Downloading a premade dataset","text":"<p>NectarGAN includes a script that allows you to automatically download a number of premade datasets to test your models on. </p> <p>Please see here for more information. </p>"},{"location":"scripts/","title":"NectarGAN \u2013 Scripts","text":"<p>Here you will find documentation related to the additional helper scripts included with NectarGAN.</p>"},{"location":"scripts/#pix2pix-dataset-download-script","title":"Pix2pix Dataset Download Script","text":"<p>This script allows you to automatically download premade Pix2pix datasets. </p> <p>[!IMPORTANT] This script requires the Python <code>requests</code> module (<code>python -m pip install requests</code>).</p> <p>[!NOTE] This script pulls from: https://efrosgans.eecs.berkeley.edu/pix2pix/datasets/</p> <p>They very kindly host these datasets to assist in research and development of ML models. Please be kind to their servers!</p> <p>Most of these datasets are also available on Hugging Face if you'd prefer: https://huggingface.co/datasets/huggan/facades https://huggingface.co/datasets/huggan/cityscapes https://huggingface.co/datasets/huggan/edges2shoes https://huggingface.co/datasets/huggan/maps https://huggingface.co/datasets/huggan/night2day</p>"},{"location":"scripts/#usage","title":"Usage","text":"<p>From the project root, run:</p> <pre><code>python scripts/dowload_pix2pix_dataset.py -d facades\n</code></pre>"},{"location":"scripts/#flags","title":"Flags","text":""},{"location":"scripts/#-d-dataset","title":"<code>-d</code> / <code>--dataset</code>:","text":"<p>Allows you to specify a dataset to download. Valid options are:</p> <ul> <li><code>cityscapes</code></li> <li><code>edges2handbags</code></li> <li><code>edges2shoes</code></li> <li><code>facades</code></li> <li><code>maps</code></li> <li><code>night2day</code></li> </ul> <p>If no value is supplied, the script will default to <code>facades</code>.</p>"},{"location":"scripts/#result","title":"Result","text":"<p>The dataset will be downloaded as a <code>.tar.gz</code> to <code>NectarGAN/data/datasets</code>, then the dataset files will be extracted to a new subdirectory of <code>NectarGAN/data/datasets</code> named after the dataset. After the files are extracted, the archive is deleted automatically, and you are left with just the directory containing the dataset files.</p>"},{"location":"testing_your_installation/","title":"NectarGAN - Testing Your Installation","text":""},{"location":"testing_your_installation/#this-document-will-guide-you-through-the-process-of-testing-your-nectargan-installation-to-ensure-the-various-components-are-functioning-correctly","title":"This document will guide you through the process of testing your NectarGAN installation to ensure the various components are functioning correctly.","text":"<p>Before we can run the tests, we first need to install the requisite packages. This can be accomplished by running the following command inside of the environment in which you installed NectarGAN:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This command will install the pytest and pytest-cov packages. After installation has completed, we can run:</p> <pre><code>pytest\n</code></pre> <p>This will begin the automatic testing which will take a few seconds. When it is complete, you will see something along the lines of:</p> <pre><code>================================================== 8 passed in 9.66s ==================================================\n</code></pre> <p>If the testing was successful. Otherwise you will get an assertation error informing you which component is failing. The testing suite currently covers:</p> <ul> <li>General module imports for critical components.</li> <li>Testing of the loss management framework.</li> <li>Testing of the UNet and PatchGAN models.</li> <li>Validation of the Scheduler module and the base schedule functions. </li> </ul>"},{"location":"toolbox/","title":"NectarGAN Toolbox - Home","text":""},{"location":"toolbox/#a-graphical-tool-for-training-and-testing-models-reviewing-results-of-previous-tests-converting-models-to-onnx-and-testing-the-resulting-model-and-processing-dataset-files-all-packaged-into-a-single-modern-and-easy-to-use-interface","title":"A graphical tool for training and testing models, reviewing results of previous tests, converting models to ONNX and testing the resulting model, and processing dataset files, all packaged into a single modern and easy to use interface.","text":"<p>[!TIP] It is recommended to start with the Toolbox quickstart guide, a short walkthrough which will teach you the basics of training and testing model, and reviewing training data using the NectarGAN Toolbox.</p>"},{"location":"toolbox/#sections","title":"Sections","text":"<p>The Toolbox interface is broken down into seven core sections, each of which can be accessed via the buttons of the left-hand bar, or by pressing <code>Ctrl+[1-7]</code>. These sections are:</p>"},{"location":"toolbox/#experiment","title":"Experiment","text":"<p>Here you will find settings related to experiment output, naming, and versioning, as well and settings related to the architecture of the generator and the discriminator. There is also an option on this page to select and load a JSON config file and initialize all of the UI settings from the values defined in the file.</p> <p>See the NectarGAN Toolbox - Experiment documentation for more information.</p>"},{"location":"toolbox/#dataset","title":"Dataset","text":"<p>In this section, you will find settings related to dataset file loading and training-time data augmentations.</p> <p>See the NectarGAN Toolbox - Dataset documentation for more information.</p>"},{"location":"toolbox/#training","title":"Training","text":"<p>Here you will find settings related to model training including checkpoint loading, learning rate scheduling, loss function weighting, and checkpoint and example saving.</p> <p>See the NectarGAN Toolbox - Training documentation for more information.</p>"},{"location":"toolbox/#testing","title":"Testing","text":"<p>This section houses settings related to testing trained models and visualizing test results.</p> <p>See the NectarGAN Toolbox - Testing documentation for more information.</p>"},{"location":"toolbox/#review","title":"Review","text":"<p>The section allows you to review the results of previous training sessions, including the loading of post-epoch example images and graphing of loss log data.</p> <p>See the NectarGAN Toolbox - Review documentation for more information.</p>"},{"location":"toolbox/#utilities","title":"Utilities","text":"<p>This section contains a number of additional tools that do not fit neatly into one of the above categories. These tools allow you to convert you models to ONNX and test the resulting <code>.onnx</code> model, and process dataset images in a variety of ways.</p> <p>See the NectarGAN Toolbox - Utilities documentation for more information.</p>"},{"location":"toolbox/#settings","title":"Settings","text":"<p>This section contains some general setting related to the Toolbox interface such as update rate during training and always-on-top behavior of the main Toolbox window.</p> <p>See the NectarGAN Toolbox - Settings documentation for more information.</p> <p>Relevant sections: Experiment | Dataset | Training | Testing | Review | Utilities | Settings |</p>"},{"location":"toolbox/#note-there-is-currently-no-developer-documentation-related-to-the-nectargan-toolbox-it-will-be-added-in-a-future-update-if-you-are-curious-how-something-behaves-or-would-like-to-alter-the-functionality-of-the-interface-the-toolbox-source-can-be-found-at-nectargantoolbox-and-is-broken-down-into-various-submodules-related-to-ui-sections-andor-functionality-much-of-it-is-well-documented-in-line-and-in-method-docstrings","title":"NOTE: There is currently no developer documentation related to the NectarGAN Toolbox. It will be added in a future update. If you are curious how something behaves or would like to alter the functionality of the interface, the Toolbox source can be found at <code>nectargan.toolbox</code>, and is broken down into various submodules related to UI sections and/or functionality. Much of it is well documented in-line and in method docstrings.","text":""},{"location":"api/config/","title":"NectarGAN API - Config","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/config/#the-nectargan-api-is-entirely-driven-by-json-configuration-files-all-data-related-to-training-testing-and-testing-models-and-visualizing-results-with-the-exception-of-some-toolbox-wrappers-is-managed-by-these-config-files","title":"The NectarGAN API is entirely driven by JSON configuration files. All data related to training, testing and testing models and visualizing results (with the exception of some Toolbox wrappers) is managed by these config files.","text":""},{"location":"api/config/#configuration-files-in-nectargan-are-managed-by-two-core-components-the-configmanager-class-and-the-config-dataclass-both-of-which-will-be-explained-in-this-document","title":"Configuration files in NectarGAN are managed by two core components, the <code>ConfigManager</code> class, and the <code>Config</code> dataclass, both of which will be explained in this document.","text":""},{"location":"api/config/#the-config-file","title":"The Config File","text":"<p>The default config file can be found at: <code>nectargan/config/default.json</code> It is broken down into some core sections: | Section | Description | | :---: | --- | <code>common</code> | Basic values like output directory, experiment name, and target device. <code>dataloader</code> | Values related to loading and augmenting dataset files. <code>train</code> | Values related to loading checkpoints, learning rate schedules, and loss weighting. <code>save</code> | Values related to checkpoint and example image saving. <code>visualizer</code> | Values related to progress visualization(Currently only for Visdom, the Toolbox has its own system for managing these settings.) </p>"},{"location":"api/config/#the-configmanager","title":"The ConfigManager","text":"<p>In the NectarGAN API, config files management is done via the <code>ConfigManager</code>. The main job of the <code>ConfigManager</code> is to take a JSON config file and parse the values into a much more usable set of dataclasses.</p>"},{"location":"api/config/#using-the-configmanager","title":"Using the ConfigManager","text":"<p>Generally speaking, you likely will not be using the <code>ConfigManager</code> directly. At least not if you are building a trainer/tester that inherits from the core <code>Trainer</code> class, as it creates it's own <code>ConfigManager</code> as one of the first steps in its <code>__init__()</code>. However, there are some cases where you might want to, such as writing an entirely new base class or even using a pre-built <code>ConfigManager</code> to initialize a <code>Trainer</code> via <code>Trainer.init_config()</code>, so we will go over the process here.</p> <p>First, let's have a quick look at the <code>ConfigManager.__init__()</code> function. We can see that it only takes one argument: <code>input_config</code>. And that can be a few things. Here is a brief walkthrough of what each expects, and the behaviour of the config manager when it is initialized with each:</p> Type Behaviour <code>str</code> This is expecting a string with the system path to a <code>.json</code> config file. It will attempt to parse the config file and assign the values, or return various exception types if it fails depending on the reason for the failure. <code>PathLike</code> The is expecting an <code>os.PathLike</code> object containing the system path to a <code>.json</code> config file. It behaves exactly as the same as <code>str</code> does. <code>dict[str,Any]</code> This is expecting a pre-parsed config file (i.e. <code>with open('config.json', 'r') as f: data = json.load(f)</code>).It will first run a key check on the input vs. the default config, and if the input passes, it will simply assign the values directly to the <code>ConfigManager</code>'s <code>raw</code> config data, then parse them into the dataclasses. This one is a little strange because it technically bypasses the JSON stuff altogether, which is actually why it exists, to allow the Toolbox to just dump <code>Config</code>-like data directly into a <code>ConfigManager</code>, rather than needing an intermediate <code>.json</code> export step. <code>None</code> If <code>input_config</code> is <code>None</code> (default), the <code>ConfigManager</code> will attempt to load the default config file located at: <code>nectargan/config/default.json</code>. If it is unable to do so, it will raise an exception."},{"location":"api/config/#so-then-with-these-options-in-mind-here-are-some-of-the-ways-in-which-you-could-initialize-a-configmanager","title":"So then, with these options in mind, here are some of the ways in which you could initialize a <code>ConfigManager</code>:","text":"<pre><code>import json\nfrom pathlib import Path\nfrom nectargan.config.config_manager import ConfigManager\n\nfilepath = '/path/to/my/config_file.json'\n\nconfig_manager = ConfigManager()                          # Init from default config file\nconfig_manager = ConfigManager(config_file=None)          # Also from default config file\n\nconfig_manager = ConfigManager(filepath)                  # From a string\nconfig_manager = ConfigManager(Path(filepath))            # or a pathlib.Path\nconfig_manager = ConfigManager(Path(filepath).as_posix()) # or a string from a pathlib.Path\n\nwith open('/path/to/my/config_file.json', 'r') as file:   # Or with JSON-like data\n    config_data = json.load(file)                         # Generally, don't do this though. Give the ConfigManger\nconfig_manager = ConfigManager(config_data)               # the filepath and let the it parse it directly instead\n</code></pre>"},{"location":"api/config/#config-dataclasses","title":"Config Dataclasses","text":"<p>The <code>ConfigManager</code> relies on a dataclass called <code>Config</code>, and a number of other dataclasses which all get packed into it.</p> <p>Let's have a quick look over the file that houses all of the dataclasses related to the <code>ConfigManager</code>: <code>nectargan.config.config_data</code></p> <p>Looking over the dataclasses and their member variables, you may notice that the structure and all of the names and datatypes exactly match the structure and values of the config file. Each dict-type object in the config json has it's own dataclass, all of which are, eventually anyway, loaded into the main <code>Config</code> dataclass. All the <code>ConfigManager</code> does is load the config file, loop through the <code>Config</code> dataclass recursively, find the corresponding entry in the config file's data, and assigns the value, turning config lookups from this:</p> <pre><code>device = config['common']['device']\n</code></pre> <p>To this:</p> <pre><code>device = config_manager.data.common.device\n</code></pre> <p>Which is a little nicer to work with in my opinion.</p>"},{"location":"api/config/#exporting-a-config-copy","title":"Exporting a Config Copy","text":"<p>The <code>ConfigManager</code> has a simple helper function called<code>export_config()</code>. When you call this function and pass it a directory filepath for the <code>output_directory</code> argument, the <code>ConfigManager</code> will export a copy of it's current config data as a <code>.json</code> file to the provided directory. Before exporting the file, the <code>ConfigManager</code> will first scan the input directory looking for previous config files, and it it finds any, as would be the case if the user in continuing training on an existing model, the file will be versioned as such: (<code>train1_config.json</code>, <code>train2_config.json</code>) </p>"},{"location":"api/config/#adding-a-value-to-the-config-file","title":"Adding a Value to the Config File","text":""},{"location":"api/config/#adding-a-value-to-an-existing-group","title":"Adding a value to an existing group","text":"<p>Adding a value to an existing group is very simple. Let's say we wanted to add a floating point value called 'my_cool_value' to the <code>common</code> section of the config file. Fist, we would open our config file and add the value:</p> <pre><code>{\n    \"config\": {\n        \"common\": {\n            \"my_cool_value\": 42.0, &lt;--- Add your value\n            \"device\": \"cuda\",\n            ...\n</code></pre> <p>Then, we would go to our config data file, and we would find the related dataclass for the <code>common</code> section, <code>ConfigCommon</code>, and we would add our new value with the corresponding type:</p> <pre><code>@dataclass\nclass ConfigCommon:\n    my_cool_value: float\n    device: str\n</code></pre> <p>[!IMPORTANT] The variable name must be all lowercase and the name and data type must match exctly with what is in the config file.</p> <p>And that's it, now when you initialize a <code>ConfigManager</code> with your updated config file, the value of <code>my_cool_variable</code> will be accessable with:</p> <pre><code>config_manager = ConfigManager('/path/to/your/updated/config.json')\nvalue = config_manager.data.common.my_cool_variable\n</code></pre> <p>Basically any datatype allowed by JSON is viable to add. Have a quick look through the <code>config_data</code> file to see how various datatypes are implemented.</p>"},{"location":"api/config/#adding-a-new-config-group","title":"Adding a new config group","text":"<p>Adding a new group is very similar to the above steps. First, we add our group to the config file, with a variable:</p> <pre><code>{\n    \"config\": {\n        \"my_cool_group\": {         &lt;--- First add your group,\n            \"my_cool_value\": 42.0, &lt;--- then add your value\n        },\n        \"common\": {\n            \"device\": \"cuda\",\n            ...\n</code></pre> <p>Then, in <code>config_data</code>, we first create a new dataclass and add our value to it:</p> <pre><code>@dataclass\nclass ConfigMyCoolGroup:\n    my_cool_value: float\n</code></pre> <p>[!NOTE] Dataclass names are not strict. You can follow the established pattern, or name them whatever you want. Variable names must be all lowercase though!</p> <p>Then, we just need to add out new dataclass to the main <code>Config</code> dataclass like this:</p> <pre><code>@dataclass\nclass Config:\n    my_cool_group: ConfigMyCoolGroup # \"my_cool_group\" here must be lowercase\n    common: ConfigCommon\n    dataloader: ConfigDataloader\n    train: ConfigTrain\n</code></pre> <p>And that's it, now the value of <code>my_cool_variable</code> can be accessed as follows:</p> <pre><code>config_manager = ConfigManager('/path/to/your/updated/config.json')\nvalue = config_manager.data.my_cool_group.my_cool_variable\n</code></pre>"},{"location":"api/dataset/","title":"NectarGAN API - Dataset","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/dataset/#the-nectargan-api-currently-provides-a-dataset-helper-class-and-a-robust-albumentations-transform-wrapper-geared-towards-paired-image-translation-tasks-there-are-plans-to-expand-this-for-unpaired-training-in-the-near-future","title":"The NectarGAN API currently provides a dataset helper class and a robust Albumentations transform wrapper geared towards paired image translation tasks. There are plans to expand this for unpaired training in the near future.","text":""},{"location":"api/dataset/#the-paireddataset-class","title":"The PairedDataset class","text":"<p>Reference: <code>nectargan.dataset.paired_dataset</code></p> <p>This is a simple helper class for loading and processing dataset images for paired adversarial training. As with most components of the NectarGAN API, the <code>PairedDataset</code> class takes a <code>Config</code> at init (see here for more info). It also takes an <code>os.Pathlike</code> object for its <code>root_dir</code> argument. This <code>PathLike</code> should point to the root directory of your dataset, the directory which contains your <code>train</code>, <code>val</code>, and, optionally, <code>test</code> subdirectories housing your dataset image files. </p> <p>[!NOTE] If you are using a <code>Trainer</code> subclass, the base <code>Trainer</code> class provides the <code>build_dataloader()</code> wrapper for convenience. This function will use the values in the <code>Trainer</code>'s config to build a <code>PairedDataset</code>, and from it, a <code>torch.utils.data.Dataloader</code>. All you need to tell it is what type of loader (i.e. <code>test</code>, <code>train</code>, <code>val</code>) to create so it knows which dataset subdirectory to load the images from.</p>"},{"location":"api/dataset/#creating-a-paireddataset-instance","title":"Creating a PairedDataset instance","text":"<p>A <code>PairedDataset</code> can be instantiated as follows:</p> <pre><code>from nectargan.config.config_manager import ConfigManager\nfrom nectargan.dataset.paired_dataset import PairedDataset\n\nconfig_manager = ConfigManager('path/to/config.json') # First we make a ConfigManager from a config file.\ndataset = PairedDataset(                              # Then we create a PairedDataset instance,\n    config=config_manager.data,                       # passing it the Config object from the ConfigManager\n    root_dir='/path/to/dataset/root/directory')       # and the path to our dataset root directory.\n</code></pre> <p>Then is can be passed directory to a <code>torch.utils.data.DataLoader</code>, again using our config values, like this:</p> <pre><code>from torch.utils.data import DataLoader\nbatch_size=config_manager.data.dataloader.batch_size\nnum_workers=config_manager.data.dataloader.num_workers\n\nloader = DataLoader(\n    dataset, \n    batch_size=batch_size, \n    shuffle=True, # Or False if you don't want shuffling\n    num_workers=num_workers)\n</code></pre>"},{"location":"api/dataset/#dataset-image-loading","title":"Dataset Image Loading","text":"<p>Reference: <code>nectargan.dataset.paired_dataset</code></p> <p>The following are the exact steps which are taken when a <code>PairedDataset</code> loads an image:</p> <ol> <li>It first loads the image from the dataset based on the <code>index</code> argument as a <code>PIL.Image</code>.</li> <li>Then it resizes the image based on the dataloader load size in the config that the <code>PairedDataset</code> was passed when it was initialized.</li> <li>Then it converts the <code>PIL.Image</code> to a <code>numpy.ndarray</code>.</li> <li>After that, it checks for the dataloader direction in the config file. Based on that value, it crops the images and assigns the <code>input_image</code> and <code>target_image</code> (or raises an exception if the provided direction is invalid. Valid directions are <code>AtoB</code> and <code>BtoA</code>).</li> <li>It uses the <code>Augmentations</code> class (described below) to apply augmentations based on the augmentation settings in the provided config.</li> <li>Returns the two images as a tuple of <code>torch.Tensors</code> (i.e. (<code>input_image</code>, <code>target_image</code>))</li> </ol>"},{"location":"api/dataset/#the-augmentations-class","title":"The Augmentations class","text":"<p>Reference: <code>nectargan.dataset.augmentations</code></p> <p>[!NOTE] This class used to be called <code>Transformer</code> until I decided that probably wasn't a great name. Some things in the class's code still reflect this old name though.</p> <p>The NectarGAN API manages train-time dataset augmentations via a helper class called <code>Augmentations</code>. This class is not meant to be interacted with directly. It is instead used by the <code>PairedDataset</code> class to apply augmentations to images at run time based on the dataset augmentations settings in the config file. The <code>Augmentations</code> class is very simple, consisting only of a few private functions to build an Albumentations <code>Compose</code> for transformations applied to both images, another for transformations applied only to the input images, and another still for those transforms which are only applied to the target image (although that's not particularly common in image-to-image tasks so all that one actually does in its standard implementation is normalizes the input and converts it to a tensor).</p> <p>The only public function is <code>apply_transforms()</code>. It takes two <code>numpy.ndarrays</code> as input (the input and target image from the dataset file, as passed to it via the <code>PairedDataset</code>'s <code>__getitem__</code> method), applies the relevant transforms to each, and returns them as a tuple of <code>torch.Tensors</code>: (<code>input</code>, <code>target</code>).</p> <p>Following are a list of currently supported transforms, broken down by category. Note that while they are not listed, both the input and the target image are also normalized (-1, 1) and converted to tensors as part of their corresponding transform function.</p>"},{"location":"api/dataset/#augmentations-input","title":"Augmentations (Input)","text":"<ul> <li>Colorjitter</li> <li>Gaussian Noise</li> <li>Motion Blur</li> <li>Random Gamma</li> <li>Image Compression</li> </ul>"},{"location":"api/dataset/#augmentations-both","title":"Augmentations (Both)","text":"<ul> <li>Horizontal Flip</li> <li>Vertical Flip</li> <li>90\u00b0 Stepped Rotation</li> <li>Elastic Transform</li> <li>Optical Distortion</li> <li>Coarse Dropout</li> </ul>"},{"location":"api/onnx_tools/","title":"NectarGAN API - ONNX Tools","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/onnx_tools/#the-nectargan-api-includes-a-simple-utility-class-for-converting-trained-models-to-onnx-and-a-script-for-testing-the-resulting-model-on-real-test-images","title":"The NectarGAN API includes a simple utility class for converting trained models to ONNX, and a script for testing the resulting model on real test images.","text":"<p>[!NOTE] The <code>ONNXConverter</code> is primarily meant to be used with the Toolbox UI. It is fully functional as a standalone class, it's just a little rough to use. It and the ONNX tester script will likely be rewritten in the future to make them a little easier to use outside of the Toolbox.</p>"},{"location":"api/onnx_tools/#what-is-onnx","title":"What is ONNX?","text":"<p>Reference: https://onnx.ai/</p> <p>ONNX (or Open Neural Network eXchange) is an inference-only, open source, platform agnostic interchange format for pre-trained neural networks. Models from most any widely used machine learning framework can be converted to <code>.onnx</code> files, which can then be run in anything that supports the ONNX Runtime. It is a popular solution for sharing and deploying trained models.</p>"},{"location":"api/onnx_tools/#the-onnxconverter-class","title":"The ONNXConverter Class","text":"<p>Reference: <code>nectargan.onnx.converter</code></p> <p>The <code>ONNXConverter</code> class provides a simple wrapper around <code>torch.onnx.export</code>, allowing you to more easily convert your models trained with NectarGAN to the <code>.onnx</code> format. </p>"},{"location":"api/onnx_tools/#creating-an-onnxconverter-instance","title":"Creating an ONNXConverter Instance","text":"<p>Let's start by briefly going over the class's <code>__init__</code> function to see how we can create an <code>ONNXConverter</code> instance in our own scripts. We will start with the input arguments. Looking at the function, we can see there are only a few of them, and many are optional. Following is a list of all of the possible input arguements, with a short description of what each one is and how it is used: | Argument | Description | | :---: | --- | <code>experiment_dir</code> | Required. This is an <code>os.PathLike</code> object representing the system path to the experiment directory which houses the checkpoint <code>.pth</code> you would like to convert to <code>.onnx</code>. <code>config</code> | Required. This can be any number of things representing a NectarGAN config (see here for more info). <code>load_epoch</code> | Optional. If this is not <code>None</code>, it should be an integer representing the epoch value of the checkpoint you would like to load for conversion. However, if it is <code>None</code> (default), the <code>load_epoch</code> value will instead be taken from the input <code>config</code>'s <code>[\"config\"][\"train\"][\"load\"][\"load_epoch\"]</code>. <code>in_channels</code> | Optional. If this is not <code>None</code>, it should be an integer representing the number of input channels for the generator (i.e. 1 for mono, 3 for RGB). Currently, the only number of <code>in_channels</code> supported by the API is 3, although there are plans to expand that in the future. However, if it is <code>None</code> (default), the <code>in_channels</code> value will instead be taken from the input <code>config</code>'s <code>[\"config\"][\"dataloader\"][\"load\"][\"input_nc\"]</code>. <code>crop_size</code> | Optional. If this is not <code>None</code>, it should be an integer value representing the desired input tensor width and height of the converted model. However, if it is <code>None</code> (default), the <code>crop_size</code> value will instead be taken from the input <code>config</code>'s <code>[\"config\"][\"dataloader\"][\"load\"][\"crop_size\"]</code>. <code>device</code> | Optional. If this is not <code>None</code>, it should be a string representing the desired target device of the converted model (i.e. 'cpu', 'cuda'). However, if it is <code>None</code> (default), the <code>device</code> value will instead be taken from the input <code>config</code>'s <code>[\"config\"][\"common\"][\"device\"]</code>.</p> <p>Alright, pretty simple. The only things we are required to pass it are the path to an experiment directory, and a config. The rest of the arguments are just convenience overrides. If they are not set, the values will instead just be taken from the input <code>config</code>. So with that in mind then, let's have a quick look at how we can create an <code>ONNXConverter</code> instance:</p> <pre><code>from nectargan.onnx.converter import ONNXConverter\n\nconverter = ONNXConverter(\n    experiment_dir='/path/to/experiment/directory', # Path to experiment directory with model to convert\n    config='/path/to/config_file.json',             # Path to .json config file for model settings\n    load_epoch=200,                                 # Load the epoch200 checkpoint for conversion\n    in_channels=3,                                  # Model input channels, 3 = RGB\n    crop_size=256,                                  # Target input tensor shape: [1, 3, 256, 256]\n    device='cpu')                                   # Target cpu (or 'cuda') for converted model\n</code></pre> <p>[!NOTE] In it's current state at least, the <code>ONNXConverter</code> is pretty deeply tied in to the pipeline conventions of the NectarGAN API. It expect that the input model for conversion is using the directory structure and naming conventions which are standard to training with the API.</p> <p>In practical terms, this means that the <code>ONNXConverter</code> is expecting that the directory which is passed for <code>experiment_dir</code> contains within it a <code>.pth.tar</code> file for the given <code>load_epoch</code> called <code>epoch{load_epoch}_netG.pth.tar</code>. You can convert any other mode, as long as it is a <code>.pth.tar</code>, by just dropping it in any directory, calling it <code>epoch1_netG.pth.tar</code>, and then instantiating the <code>ONNXConverter</code> with the path the that directory and a <code>load_epoch</code> value of 1.</p> <p>Please also note that this is taken a step further in the Toolbox's <code>ONNXConverter</code> wrapper, in that it also requires the <code>train{x}_config.json</code> file, which is automatically exported with every train when run from the Toolbox, to be present in the <code>experiment_dir</code>. This is because in the Toolbox, everything related to conversion is set in the interface save for the generator architecture settings which are required to remain static across training sessions regardless, so those are just grabbed automatically from the latest config in the <code>experiment_dir</code> instead.</p>"},{"location":"api/onnx_tools/#converting-a-model","title":"Converting a Model","text":"<p>Great, now we have our <code>ONNXConverter</code> instance ready to go. Let's now take a look at how we can use it to convert the model we pointed it to. Looking over the <code>ONNXConverter</code> class, we can see that it actually only has one public function that is unique to itself, and not inherited from the <code>Trainer</code> class: <code>convert_model</code></p> <p>Let's again start by having a quick look over the input arguments for <code>convert_model</code>: | Argument | Description | | :---: | --- | <code>export_params</code> | If <code>True</code> (default), model weights will be exported. <code>opset_version</code> | What ONNX opset version to use for the converted model. <code>do_constant_folding</code> | Fold constants for model optimization (This is deprecated in later versions but included for compatibility. Note, if <code>suppress_onnx_warnings=True</code> (default), the compatibility warning related to this will be silenced from printing to the console). <code>input_names</code> | Names to assign to the graph inputs. <code>output_names</code> | Names to assign to the graph outputs. <code>suppress_onnx_warnings</code> | If <code>True</code>, the normalization training mode warning and a warning related to constant folding incompatibility will be silenced from printing to the console. </p> <p>[!NOTE] Save for <code>suppress_onnx_warnings</code>, all of the input arguments for <code>convert_model</code> are just direct passthrough arguments for <code>torch.onnx.export()</code>.</p> <p>And now let's have a look at how we can use the <code>convert_model</code> function to convert our trained model. Using our <code>converter</code> which we created above, we simply need to do this:</p> <pre><code>converter.convert_model(\n    export_params=True,              # Generally we want to export our model weights\n    opset_version=11,                # Or whatever version you want. Higher version = more features\n    do_constant_folding=True,        # This actually won't do anything if opset_version&gt;10\n    input_names=['my_input_name'],   # Whatever input names you want, or none for default: ['input']\n    output_names=['my_output_name'], # Whatever output names you want, or none for default: ['output']\n    suppress_onnx_warnings=True)     # Or False if you want it to print the warnings\n</code></pre> <p>And that's it. When run, this will export the converted model to the given <code>experiment_dir</code>. The converted file will the called <code>epoch{load_epoch}.onnx</code> (this can be changed by altering the <code>onnx_filename</code> variable in <code>ONNXConverter._build_output_path()</code>)</p>"},{"location":"api/onnx_tools/#onnx-model-test-script","title":"ONNX Model Test Script","text":"<p>Reference: <code>nectargan.onnx.tester</code></p> <p>There is also a very simple test script your converted models. It will take the path to the <code>.onnx</code> model, and the path to a test image you would like to run the model inference on. The script will load the model with <code>onnxruntime</code>, run the model's inference on the given test image, and display the result via <code>matplotlib</code>. This is just a super simple validator, mostly provided as a convenience. You can also use the ONNX tester in the Toolbox to test your converted models, though. It is a bit more full-featured and allows you to test the model on more than a single image at a time.</p>"},{"location":"api/scheduling/","title":"NectarGAN API - Scheduling","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/scheduling/#the-nectargan-api-provides-a-modular-highly-configurable-solution-to-managing-scheduling-for-any-parameter-in-your-models","title":"The NectarGAN API provides a modular, highly configurable solution to managing scheduling for any parameter in your models.","text":""},{"location":"api/scheduling/#scheduling-dataclass","title":"Scheduling dataclass","text":"<p>The core of the NectarGAN scheduling system is the <code>Schedule</code> dataclass, an easy to use tool which (along with the scheduling function system) allows you to quickly define complex schedules which can be applied to any paramaeter in your model with relative ease.</p> <p>Please see here for more information.</p>"},{"location":"api/scheduling/#schedule-functions","title":"Schedule Functions","text":"<p>The NectarGAN scheduling system works via a drop-in schedule function system. This system allows you to define reusable schedule functions, and easily drop them in to anything you'd like to schedule.</p> <p>Please see here for more information.</p>"},{"location":"api/scheduling/#scheduler-classes","title":"Scheduler Classes","text":"<p>Scheduling in the NectarGAN API is managed by two main classes: a general scheduler, which can be used for basically anything, and a compatibility wrapper around PyTorch's native <code>torch.optim.lr_scheduler</code>, allowing you to use scheduling functions interchangeably while retaining the native scheduler's deep integration with the native optimizers.</p> <p>Please see here for more information.</p>"},{"location":"api/scheduling/#scheduler-integrations","title":"Scheduler Integrations","text":"<p><code>Schedules</code> are deeply integrated into other core components of the NectarGAN API.</p> <p>Please see here for more information.</p>"},{"location":"api/testers/","title":"NectarGAN API - Testers","text":"<p><code>NectarGAN API - Home</code> <code>Getting Started (Toolbox | Testing)</code></p>"},{"location":"api/testers/#nectargan-provides-a-tester-class-and-a-paired-testing-helper-script-to-allow-you-to-test-your-trained-models-on-real-input-data","title":"Nectargan provides a <code>Tester</code> class and a paired testing helper script to allow you to test your trained models on real input data.","text":""},{"location":"api/testers/#the-tester-class","title":"The <code>Tester</code> class","text":"<p>Reference: <code>nectargan.testers.tester</code></p> <p>[!NOTE] Right now, as there is only one type of model trainer (for Pix2pix-style paired image translation), the <code>Tester</code> class is only suitable for paired image translation model testing.</p> <p>Support for unpaired models is planned for the future, and when it is added, this <code>Tester</code> will be refactored into a base <code>Tester</code> class, and child classes for paired and unpaired testing, as it is with the <code>Trainer</code>/<code>Pix2pixTrainer</code>. </p> <p>Let's start by having a look at the <code>Tester</code> class.</p> <p>The first thing we notice is that it inherits from the base <code>Trainer</code> class. The <code>Tester</code> uses its parent class to: - Manage configuration and output settings - Handle model checkpoint loading. - Build a dataloader for testing.</p>"},{"location":"api/testers/#initializing-a-tester","title":"Initializing a <code>Tester</code>","text":"<p>Looking at the <code>__init__</code> method, we can see that a <code>Tester</code> can take up to four possible input arguments: | Argument | Description | | :---: | --- | <code>config</code> | This can be any number of things representing a config (see here), or <code>None</code> (default). This config is primarily used to define generator architecture settings, although it may also be used for a few other things depending on the other arguments which are passed to it. If the generator you are loading for testing uses different architecture settings than the default config, you will need to pass a config (like the default train config, exported automatically for each train). Otherwise you can use the default <code>None</code> <code>experiment_dir</code> | The experiment directory containing the checkpoints of the model you would like to test. This should almost always be set. It defaults to <code>None</code> for Toolbox compatibility reasons, but the process of actually using that <code>None</code> value is complicated. You will likely get exceptions if you try to use the default. <code>dataroot</code> | An <code>os.PathLike</code> object pointing to the root directory of a dataset, or <code>None</code> (default). The <code>dataroot</code> directory must contain a subdirectory called <code>test</code> containing the dataset images to use for testing. If this value is <code>None</code>, the dataroot from the config will be used instead. <code>load_epoch</code> | The epoch checkpoint to load for testing (i.e. <code>epoch{load_epoch}_netG.pth.tar</code>), or <code>None</code> (default). If this value is <code>None</code>, the value of <code>[\"config\"][\"train\"][\"load\"][\"load_epoch\"]</code> from the provided config will be used instead.</p> <p>Now let's walk step by step through the function to see exactly what happens when we initialize a <code>Tester</code>: 1. We initialize the parent <code>Trainer</code> class, passing it the input config, and disabling the quicksetup since we don't need many of the components which are required for training. 2. Force the <code>continue_train</code> boolean value in the config to <code>True</code>. We do this so that we can call the parent class' <code>build_output_directory</code> method to initialize the current <code>self.experiment_dir</code> (a member variable from the parent <code>Trainer</code> class which defines the current experiement directory) path without it raising an exception. See here for more info.  3. Override the config load epoch if one was passed as an input argument. 4. Then check if the input <code>experiment_directory</code> is <code>None</code>:     - If it is: We run <code>Tester._build_output_directory()</code>. This calls <code>Trainer.build_output_directory()</code>, as discussed above, then calls <code>Tester._init_test_output_root()</code>. All this function does is perform a quick check of the current <code>experiment_directory</code> to see if it contains a subdirectory called <code>test</code>. If not, it tries to make one.     - If it isn't: We just set <code>self.experiment_dir</code> to the input <code>experiement_directory</code>, then call <code>Tester._init_test_output_root()</code>. 5. Override the config <code>dataroot</code> if one was passed. 6. Initialize the <code>test</code> dataloader and the generator. </p> <p>So then, with these input arguments in mind, let's see how we can initialize a <code>Tester</code> instance:</p> <pre><code>from nectargan.testers.tester import Tester\n\ntester = Tester(\n    config='/path/to/config_file.json',\n    experiment_dir='/path/to/experiment/directory',\n    dataroot='/path/to/dataset/root',\n    load_epoch=200\n)\n</code></pre> <p>And that's all there is to it. After that, we're ready for...</p>"},{"location":"api/testers/#testing","title":"Testing","text":"<p>Reference: <code>Tester.run_test</code></p> <p>With our <code>Tester</code> instance created, we can now perform a test like so:</p> <pre><code>tester.run_test(image_count=10)\n</code></pre> <p>This will tell the tester to select 10 random images (as defined by the <code>image_count</code> argument) from the <code>test</code> dataset. It will then create a new output directory for the test inside of <code>experiment_directory/test</code> and, inside of that output directory, create a base <code>.json</code> log to dump test data to. </p> <p>Then, for each image, it will perform a number of steps: 1. Run the generator's inference on the input image. 2. Evalute the generator's output using a few loss function (currently <code>L1</code>, <code>Sobel</code>, <code>Laplacian</code>) 3. Save the [<code>x</code>, <code>y_fake</code>, <code>y</code>] image set to the test output directory. 4. Write the data from that test iteration (i.e. input image path, output image paths, loss values, etc.) to the test log.</p> <p>[!NOTE]  Sampling from the <code>test</code> dataset is random, so if you have more <code>test</code> images than you initially set <code>image_count</code> to, running it again will pick a new random set of images to test on.</p>"},{"location":"api/testers/#paired-testing-script","title":"Paired Testing Script","text":"<p>Reference: <code>nectargan.start.testing.paired</code></p> <p>NectarGAN provides a prebuild script for interacting with the <code>Tester</code>, allowing you to run model tests from the command line. For more information, please see here.</p>"},{"location":"api/testers/#toolbox-testing","title":"Toolbox Testing","text":"<p>Model testing in the Toolbox is currently much more \"built-out\", so to speak. Naturally, it is deeply integrated in to the interface, but it may serve as a good example of a more complex testing integration, so we will discuss it briefly here.</p> <p>Toolbox testing is facilitated by two core components: 1. <code>TesterWorker</code></p> <pre><code>A worker class which performs the actual testing inside of a `QThread`. `TesterWorker` is a child of `Tester`, and uses basically all of its functionality wholesale. The only difference is that it implements its own version of `Tester.run_test()`, [`TesterWorker.run()`](https://github.com/ZacharyBork/NectarGAN/blob/main/nectargan/toolbox/workers/testerworker.py#L29).\n\n`run` is effectively just a `QThread` compliant override of `Tester.run_test()`. Where `run_test` prints progress updates to the console, `run` instead emits a `progress` signal. And when all the test iterations are complete, it also emits a `finished` signal. These signals are picked up and processed by the...\n</code></pre> <ol> <li> <p><code>TesterHelper</code></p> <p>A helper class which handles all things related to model testing in the NectarGAN Toolbox. It sets up <code>TesterWorkers</code>, manages their <code>QThreads</code>, processes signals during testing, and loads the resulting data when testing is complete.</p> <p>We won't go too deep in to this class, as most of its functionality is really only pertinent to the UI. There are a couple functions worth noting, though, for more general handling of test results.</p> <ul> <li> <p><code>_parse_test_log</code></p> <p>This function serves as an example of programmatic loading of the test log generated by the tester, and extraction of just the test results for each iteration (the base test log includes some additional metadata, not strictly related to the test itself).</p> </li> <li> <p><code>_sort_results</code></p> <p>Called in <code>_parse_test_log</code>, this function is used to sort the results from the test log by a number of metrics from the logs.</p> </li> <li> <p><code>load_test_results</code></p> <p>This function is very much tied to PySide, but it serves as an abstract example of iterating through the test results from the log and extracting the paths to the example images exported during testing.</p> </li> </ul> </li> </ol>"},{"location":"api/visdom/","title":"NectarGAN API - Visdom","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/visdom/#the-nectargan-api-includes-a-utility-class-for-managing-runtime-training-data-visualization-with-visdom-useful-for-testing-configs-quickly-andor-headlessly-and-deugging-training-related-features-without-needing-to-launch-the-toolbox-ui","title":"The NectarGAN API includes a utility class for managing runtime training data visualization with Visdom, useful for testing configs quickly and/or headlessly, and deugging training-related features without needing to launch the Toolbox UI.","text":""},{"location":"api/visdom/#what-is-visdom","title":"What is Visdom?","text":"<p>Reference: <code>ai.meta.com/tools/visdom</code> <code>github.com/fossasia/visdom</code></p> <p>Visdom is a commonly used, open-source solution for data visualization. It provides simple tools for graphing data in 2D and 3D, rendering images, and displaying text via a locally hosted web server.</p> <p>See here for a guide on how to set up a local Visdom server, which is required in order to use the NectarGAN <code>VisdomVisualizer</code>.</p>"},{"location":"api/visdom/#visdomvisualizer","title":"VisdomVisualizer","text":"<p>Reference: <code>nectargan.visualizer.visdom.visualizer</code></p>"},{"location":"api/visdom/#creating-a-visdomvisualizer-instance","title":"Creating a VisdomVisualizer instance","text":"<p>This class is relatively simple. Looking at its <code>__init__</code> function, we can see it only takes two arguments: 1. <code>env</code>: A string which is used to define the Visdom Environment name. 2. <code>port</code>: An integer represent the port number over which the client should send updates to the server.</p> <p>A <code>VisdomVisualizer</code> instance can be created as follows:</p> <pre><code>from nectargan.visualizer.visdom.visualizer import VisdomVisualizer\n\nvis = VisdomVisualizer(\n    env='my_env_name', # Can be anything. Or ignored for default ('main')\n    port=8097          # 8097 is Visdom's default port number\n) \n</code></pre> <p>If you are reusing the same environment for multiple tests (as is done in the base <code>Trainer</code> class), it is generally best to also clear any old data after initialization. This can be done like so:</p> <pre><code>vis.clear_env()\n</code></pre>"},{"location":"api/visdom/#multithreading","title":"Multithreading","text":"<p>The <code>VisdomVisualizer</code> has two \"modes\" it can run it. The first will run the visualization in the same thread as the training, the second will spin up a new thread specifically for managing Visdom updates. Which mode it is running in significantly alters the core functionality of update functions so we will briefly discuss how they function first.</p> <p>By default, the <code>VisdomVisualizer</code> will run in the first mode. To run visualization in a separate thread, after you initialize the <code>VisdomVisualizer</code>, you must call this function:</p> <pre><code>vis.start_thread()\n</code></pre> <p>This will do a few things: 1. Change the value of the <code>VisdomVisualizer</code>'s member variable <code>is_threaded</code> to <code>True</code>. This flag is checked by the update functions to decide what exactly should be done with the data when it is passed to push to the server. 2. Initialize a <code>queue.Queue</code> for storing example images, and another for storing loss graph values. 3. Register and start a thread for Visdom updates.</p> <p>In this mode, whenever data is passed to the <code>VisdomVisualizer</code> via one of its update functions, rather than being immediately pushed to the Visdom server, it is instead stored in the associated <code>Queue</code>. It will then be picked up during the next <code>_update</code> cycle (every two seconds by default) and pushed to the server.</p> <p>Then, whenever we are finished with the visualizer thread (at the end of the training loop, for example), we must also call:</p> <pre><code>vis.stop_thread()\n</code></pre> <p>This will ensure that the thread is shut down safely. Please refer to <code>Trainer.train_paired()</code> for an safe example implementation which also accounts for keyboard interrupts.</p>"},{"location":"api/visdom/#graphing-loss-data","title":"Graphing Loss Data","text":"<p>Loss data can be graphed at runtime by calling <code>VisdomVisualizer.update_loss_graphs()</code>. Currently, this is primarily intended to be used in conjunction with the <code>LossManager</code>, as the data format it is expecting is exactly that which is returned by <code>LossManager.get_loss_values()</code>.</p>"},{"location":"api/visdom/#input-arguments","title":"Input Arguments:","text":"Argument Description <code>graph_step</code> A <code>float</code> value defining where along the X axis to graph the provided data. <code>losses_G</code> The generator losses, as returned with <code>LossManager.get_loss_values(query=['G'])</code> using the default Pix2pix objective loss spec. <code>losses_D</code> The discriminator losses, as returned with <code>LossManager.get_loss_values(query=['D'])</code> using the default Pix2pix objective loss spec."},{"location":"api/visdom/#single-threaded","title":"Single-Threaded","text":"<p>If run from a single thread, this function calls <code>VisdomVisualizer._update_loss_graphs_core()</code>, directly updating the graphs for generator and discriminator loss.</p>"},{"location":"api/visdom/#multi-threaded","title":"Multi-Threaded","text":"<p>If run in a separate thread, this function instead calls <code>VisdomVisualizer._store_graph_data</code>, storing all of the input data in the <code>_graph_queue</code> to be picked up during the next <code>_update</code>.</p>"},{"location":"api/visdom/#example","title":"Example","text":"<p>See: <code>nectargan.trainers.pix2pix_trainer.Pix2pixTrainer.update_display()</code></p>"},{"location":"api/visdom/#visualizing-x-y-y_fake-tensors","title":"Visualizing <code>[x, y, y_fake]</code> Tensors","text":"<p>Example outputs can be visualized during training via <code>VisdomVisualizer.update_images()</code>.</p>"},{"location":"api/visdom/#input-arguments_1","title":"Input Arguments:","text":"Argument Description <code>x</code> A <code>torch.Tensor</code>. The first image to display. <code>y</code> A <code>torch.Tensor</code>. The second image to display. <code>z</code> A <code>torch.Tensor</code>. The third image to display. <code>title</code> The title for the image display window. <code>image_size</code> The resolution (^2) to display each image at in the Visdom dashboard."},{"location":"api/visdom/#single-threaded_1","title":"Single-Threaded","text":"<p>If run from a single thread, this function directly updates the <code>[x, y, y_fake]</code> image display in the Visdom dashboard via <code>VisdomVisualizer._update_images_core()</code>.</p>"},{"location":"api/visdom/#multi-threaded_1","title":"Multi-Threaded","text":"<p>If run from a separate thread, this function will instead store the input data in the <code>_image_queue</code> via <code>VisdomVisualizer._store_images()</code> to be picked up during the next <code>_update</code>.</p>"},{"location":"api/visdom/#example_1","title":"Example","text":"<p>See: <code>nectargan.trainers.pix2pix_trainer.Pix2pixTrainer.update_display()</code></p>"},{"location":"api/visdom/#utility-functions","title":"Utility Functions","text":"<p>The <code>VisdomVisualizer</code> has one notable utility function: <code>VisdomVisualizer._denorm_tensor()</code></p> <p>This function is called once for each of the three tensors (<code>x</code>, <code>y</code>, <code>z</code>) in <code>VisdomVisualizer._update_images_core()</code>. It takes the input tensor which, from Pix2pix at least since the last upsampling layer uses Tahn activation, is normalized (-1, 1), and renormalizes it in the (0, 1) range for viewing.</p>"},{"location":"api/losses/loss_functions/","title":"NectarGAN API - Loss Functions","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/losses/loss_functions/#the-nectargan-api-currently-includes-three-non-standard-loss-functions-with-more-planned-in-the-near-future-these-are","title":"The NectarGAN API currently includes three non-standard loss functions (with more planned in the near future). These are:","text":""},{"location":"api/losses/loss_functions/#sobel-loss","title":"Sobel Loss","text":"<p>Reference: <code>nectargan.losses.losses.Sobel</code></p> <p>Sobel loss is effectively large-scale structure loss. It takes a real ground truth and a generator fake, converts each to grayscale, and applies a Sobel filter to each to approximate an edge map. It then takes the two results and compares them with a traditional pixel-wise loss (L1 in this implementation) to derive the final loss value. </p> <p>Here is an example of a Sobel loss map[^1]:  On the left, we have an example <code>y_real</code> and a <code>y_fake</code> generated by the NectarGAN Pix2pix implementation during training. Below each is the output of a Sobel filter applied to the respective image.</p> <p>On the right is the result of an L1 loss function applied to the two Sobel maps. The <code>Mean</code> at the bottom is the final loss value (at least in the default implementation which uses a mean reduction on the returned L1 loss result), unweighted and normalized (0, 1).</p>"},{"location":"api/losses/loss_functions/#laplacian-loss","title":"Laplacian Loss","text":"<p>Reference: <code>nectargan.losses.losses.Laplacian</code></p> <p>Laplacian loss is very similar to Sobel loss, except that, instead of applying a Sobel filter to the ground truth and generator fake, it instead applies a Laplacian filter. This can encourage the generator to preserve more small-scale textural detail from the ground truth image. For some tasks, it can be used alongside Sobel loss as a very effective replacement for L1 loss. It can also help to dissuade the generator from averaging inputs which can sometimes result in blurry outputs.</p> <p>Here is an example of a Laplacian loss map[^1]:  The layout here is the same as in the Sobel example. Note the \"grittier\" appearance and the less well defined edges in the example Laplacian filter images, and also in the resulting L1 loss map when compared with that of the Sobel loss.</p>"},{"location":"api/losses/loss_functions/#vggperceptual","title":"VGGPerceptual","text":"<p>Reference: <code>nectargan.losses.losses.VGGPerceptual</code></p> <p>[!IMPORTANT] The first time you run this loss function, either by calling it yourself in your own training script, using it in the Toolbox, or by initializing the <code>Pix2pixTrainer</code> with a <code>loss_subspec</code> which includes <code>+VGG</code>, the VGG19 default weights (<code>IMAGENET1K_V1</code>) will be downloaded from PyTorch if you do not already have them installed.</p> <p>I find VGGPerceptual loss to be particularly interesting, both in how it works, and in the results it can produce. It takes a real ground-truth and a generator fake as input, and feeds each to a pre-trained image classification model (VGG19 in this case). We don't actually care about what the classifier thinks it is though. You may be creating images of something which the model has never seen. Instead, we extract the feature maps from the model at various depths, and we use a traditional pixel-wise loss (L1 in this case), to compare them. The resulting loss values for each depth are then added together to calculate the final loss.</p> <p>This process gives us a cost function which behaves in sort of a unique way. It strongly encourages the generator to make images which are visually similar to the ground truth, as with traditional L1 loss. But in differs in that it punishes the generator far less harshly for small inaccuracies, allowing it some room for \"creativity\" and oftentimes reducing the blurring (or averaging) which is common in traditional L1 loss.</p> <p>Depending on the task, this can allow the generator to produce extremely realistic results, and I find that adding it alongside traditional L1 loss can oftentimes allow the generator to produce much more visually believable images noticably earlier in training than with just L1 loss alone.</p> <p>[^1]: Disclaimer: While the <code>y_fake</code> in these examples is a real example output from the NectarGAN Pix2pix implementation, the example Sobel and Laplacian images, as well as the loss maps, were instead generated post-train in Houdini via Copernicus. The mean loss values were then derived from the resulting loss maps using VEX. Please note, however, that it is possible to extract loss maps as tensors during training natively. Please see here for more information.</p>"},{"location":"api/losses/loss_spec/","title":"NectarGAN API - Loss Specs","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/losses/loss_spec/#loss-specifications-specs-are-a-novel-way-to-define-reusable-objective-functions-for-your-models-at-their-core-loss-specs-are-just-standard-python-functions-but-with-one-specific-requirement-the-must-return-a-dictionary-of-string-mapped-lmloss-objects","title":"Loss specifications (specs) are a novel way to define reusable objective functions for your models. At their core, loss specs are just standard Python functions but with one specific requirement, the must return a dictionary of string-mapped <code>LMLoss</code> objects.","text":""},{"location":"api/losses/loss_spec/#what-is-a-loss-spec","title":"What is a loss spec?","text":"<p>To better explain the concept, let's first have a quick look at an example loss spec function: <code>nectargan.losses.pix2pix_objective</code></p> <p>Let's quickly break down exactly what this function is doing:</p> <ol> <li>We grab the current device from the config so that we can cast our loss functions to it.</li> <li>We instantiate the core losses from the original Pix2pix paper[^1], BCEWithLogits for cGAN, L1 for G pixel-wise loss (casting each to our current device).</li> <li>We create a dictionary, and add our base loss functions.<ul> <li>We set the dict key for each to the name we want to use for lookup (equivalent to <code>LossManager.register_loss_fn(loss_name='loss_fns_dict_key')</code>).</li> <li>We instantiate an <code>LMLoss</code> object, setting the name to the same value as the dict key, the function to the <code>nn.Module</code> of the associated loss function (i.e. 'G_GAN'=BCE, 'G_L1'=L1, etc.).</li> <li>We assign loss weights, if applicable. <code>D_real</code> and <code>D_fake</code> do not have lambda functions so we do not set the value of the <code>loss_weight</code> argument for either, giving them the default weight of <code>1.0</code>.</li> <li>We assign tags for each. In this spec, the tags are utilized to differentiate which network the given loss belongs to, so that they can be looked up by group for Visdom/Toolbox visualization. You can have as many tags as you want and use them for whatever you want though.</li> </ul> </li> <li>Then, we do a couple checks on the value of the <code>subspec</code> argument. If the requested subspec is <code>extended</code>, we also add <code>MSE</code>, <code>Sobel</code>, and <code>Laplacian</code> losses to the dictionary, and if <code>+vgg</code> was in the <code>subspec</code> value, we also add <code>VGGPerceptual</code>.  <p>[!NOTE] <code>VGGPerceptual</code> is in a separate category here because, the first time a user runs it, it will automatically download the VGG19-basic weights from Torch.</p> </li> <li>We return the dictionary of LMLoss'.</li> </ol> <p>Pretty simple, right? Just a function which defines a dict containing all the losses you want to use for your model. Now let's have a look at how they're used. First, we will create a <code>LossManager</code> instance: </p> <pre><code>from nectargan.config.config_manager import ConfigManager\nfrom nectargan.losses.loss_manager import LossManager\n\nconfig_manager = ConfigManager('path/to/config.json')\nloss_manager = LossManager(\n    config=config_manager.data,\n    experiment_dir='/path/to/experiment/output/directory')\n</code></pre> <p>Should look pretty familiar from the first section. However, in the first section, we then went on to register a loss function with <code>loss_manager.register_loss_fn()</code>. And that is fine for quickly building model objectives or experimenting with different losses, but now we have a known objective that we want to use, and a function that defines it. So, instead of registering all of our losses one by one, now, we can instead just do this:</p> <pre><code>import nectargan.losses.pix2pix_objective as spec # Import our loss spec function\nloss_subspec = 'extended+vgg' # Define a subspec, since our loss spec expects one\n\nloss_manager.init_from_spec(  # Run 'LossManager.init_from_spec()'\n    spec.pix2pix,             # Pass it a reference to our loss spec function,\n    config_manager.data,      # our config data, since the spec is expecting it,\n    loss_subspec)             # and our string defining the subspec\n</code></pre> <p>And that's it, now the <code>LossManager</code> will run that function, parse the returned dict, and register all of the losses contained within it, then you are free to use them how you would with any other loss registered with a <code>LossManager</code> instance. And you also now have a loss spec which can be reused whenever you'd like to perfectly replicate that objective function.</p> <p>Now let's have a quick look at <code>LossManager.init_from_spec()</code> to see what it's doing. It serves one main purpose, setting the value of the <code>LossManager</code>'s <code>self.loss_fns</code>. Let break down exactly how it does that, though:</p> <ol> <li>We have a few input arguments, one required argument, <code>spec</code>, which takes a <code>Callable</code> (our loss spec function), and also optional <code>*args</code> and <code>**kwargs</code>. This allows you to pass whatever extra arguments you want to your loss spec function. This is how the <code>pix2pix_objective</code> loss spec is passed the <code>config</code> and <code>subspec</code>. These are not required arguments. The only requirement for a loss spec is that is be a <code>Callable</code> that returns a <code>dict[str, LMLoss]</code>.</li> <li>We first do a quick check to see if the value of the <code>spec</code> argument is <code>None</code>. If it is, we just set <code>self.loss_fns</code> to an empty dict. This is done when the <code>LossManager</code> if first instatiated to initialize it's base loss dict, which is what losses are added to when you call <code>LossManager.register_loss_fn()</code>.</li> <li>If <code>spec</code> is not <code>None</code>, however, we then try to run the loss spec function, then we loop through all the values in the spec function's return dict and assert that they are <code>LMLoss</code> instances.</li> <li>If the return dict values check out, we then assign the return values to <code>self.loss_fns</code> and run a helper function which creates dummy tensors for each <code>LMLoss</code>'s <code>last_loss_map</code>.</li> </ol>"},{"location":"api/losses/loss_spec/#summary","title":"Summary","text":""},{"location":"api/losses/loss_spec/#so-in-short-loss-specs-let-you-pre-define-model-objectives-quickly-and-in-a-format-that-encourages-reproducability-from-a-technical-standpoint-they-are-just-python-functions-that-return-a-dictstr-lmloss-what-arguments-those-functions-take-though-and-how-they-construct-that-return-value-is-entirely-up-to-you-they-are-extremely-open-ended","title":"So, in short, loss specs let you pre-define model objectives quickly, and in a format that encourages reproducability. From a technical standpoint, they are just Python functions that return a <code>dict[str, LMLoss]</code>. What arguments those functions take, though, and how they construct that return value is entirely up to you. They are extremely open-ended.","text":"<p>[^1]: Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2017)</p>"},{"location":"api/losses/lossmanager/","title":"NectarGAN API - LossManager","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/losses/lossmanager/#the-loss-manager-is-one-of-the-core-features-of-the-nectargan-api-it-is-a-drop-in-solution-for-managing-tracking-and-logging-everything-related-to-loss-in-your-model","title":"The loss manager is one of the core features of the NectarGAN API. It is a drop-in solution for managing, tracking, and logging everything related to loss in your model.","text":""},{"location":"api/losses/lossmanager/#source-nectarganlossesloss_managerlossmanager","title":"Source: nectargan.losses.loss_manager.LossManager","text":""},{"location":"api/losses/lossmanager/#key-features","title":"Key Features","text":"<ul> <li>Builds an easy to use wrapper around around any loss function, allowing you to evaluate loss functions in your training script in a way which is as easy as calling loss functions traditionally, but which dramatically expands the backend functionality of any loss function registered with the <code>LossManager</code>.</li> <li>Caches loss function results in multiple formats with easy to use mechanisms for recalling the values during training.</li> <li>An intelligent cache management system allows mean loss values to be cached to memory, and dumped to a JSON log at your discretion, or automatically if a configurable cache limit is reached.</li> <li>Quickly initialize a configurable objective function with a pre-built loss spec, or register your own loss functions with the <code>LossManager</code> to build your own model objective from scratch, while still being able to use all the QOL features that the LossManager offers. You can even define your own reusable loss specs to feed to the <code>LossManager</code>, and it will take care of the rest. </li> </ul>"},{"location":"api/losses/lossmanager/#lossmanager-dataclasses","title":"LossManager Dataclasses","text":"<p>To understand how the <code>LossManager</code> functions, and how it manages the data for the losses that are registered with it, we first have to take a quick look at two dataclasses which are at the core of it's functionality. These are:</p>"},{"location":"api/losses/lossmanager/#1-nectarganlosseslm_datalmhistory","title":"1. <code>nectargan.losses.lm_data.LMHistory</code>","text":"<p>Starting with the simpler of the two dataclasses, <code>LMHistory</code> only has one job: store previous loss value history.</p> <p>Every loss function registered with a <code>LossManager</code> instance has an <code>LMHistory</code> instance assigned to it in a way which will be explained momentarily. An <code>LMHistory</code> instance contains just two lists, they are dual-purpose, however. If loss logging is enabled, i.e. <code>LossManager(enable_logging=True)</code>, these two lists will be used to store the mean value of the loss result tensor and the current weight value of the loss, both as 32bit floating point values, every time the parent loss function is called via <code>LossManager.compute_loss_xy()</code>.</p> <p>If logging is disabled, however, each time <code>LossManager.compute_loss_xy()</code> is called for a given loss, both lists in that loss's <code>LMHistory</code> are cleared, after which time the new values are appending to each list. In practice, this means that if <code>enable_logging=False</code> each list will only store a single value, the most recent loss mean and weight respectively, at any given time.</p>"},{"location":"api/losses/lossmanager/#2-nectarganlosseslm_datalmloss","title":"2. <code>nectargan.losses.lm_data.LMLoss</code>","text":"<p>This dataclass is responsible for storing all information about a registered loss. For every loss function that is registered with a <code>LossManager</code> instance via <code>LossManager.register_loss_fn()</code>, an <code>LMLoss</code> instance is created which describes the loss function. A full description of the values contained with an <code>LMLoss</code> instance can be seen by clicking on the above link, but here is a rough outline:</p> <ul> <li><code>name</code>: a string, unique to this registered loss, which is used for lookup by various <code>LossManager</code> functions.</li> <li><code>function</code>: a reference to the <code>torch.nn.Module</code> for the loss function. This can be almost any <code>Module</code> as long as it has a forward function that returns a tensor. One caveat is that, currently, loss functions registered with the <code>LossManager</code> can only accept two input tensors for loss computation (<code>y</code>, <code>y_fake</code>), although I do plan to expand that at some point in the future.</li> <li><code>loss_weight</code>: The weight value (lambda) to apply to the resulting loss tensor when it is called, before the tensor is returned by <code>LossManager.compute_loss_xy()</code>.</li> <li><code>schedule</code>: A <code>Schedule</code> object defining a weight schedule for the given loss. If no <code>Schedule</code> is provided when the LMLoss is initialized, the provided <code>loss_weight</code> will be used for the duration of training.</li> <li><code>last_loss_map</code>: This is not set when initializing an <code>LMLoss</code> object. It is instead initialized as a dummy tensor, and then used by the <code>LossManager</code> each time the parent loss function is run to store a detached version of the resulting loss tensor so they can be recalled for visualization. store history.</li> <li><code>history</code>: This is also not set at init-time. A unique <code>LMHistory</code> object is automatically created and assigned to every loss registered with the loss manager.</li> <li><code>tags</code>: An optional list of strings containing identifier tags which can be used to search for and filter registered losses in various <code>LossManager</code> functions.</li> </ul>"},{"location":"api/losses/lossmanager/#using-the-lossmanager","title":"Using the LossManager","text":"<p>Initializing a new <code>LossManager</code> instance:</p> <pre><code>from nectargan.config.config_manager import ConfigManager\nfrom nectargan.losses.loss_manager import LossManager\n\nconfig_manager = ConfigManager('path/to/config.json')\nloss_manager = LossManager(\n    config=config_manager.data,\n    experiment_dir='/path/to/experiment/output/directory')\n</code></pre> <p>Register a new loss function with a <code>LossManager</code> instance:</p> <pre><code>import torch.nn as nn\nL1 = nn.L1Loss().to(config_manager.data.common.device)\n\nloss_manager.register_loss_fn(\n    loss_name='mylossfunction',\n    loss_fn=L1,\n    loss_weight=100.0,\n    tags=['descriptive_lookup_tag'])\n</code></pre> <p>[!WARNING] The <code>loss_name</code> you assign to your loss function when you register it must be unique amongst all other loss functions registered with that <code>LossManager</code> instance. If you attempt to register a loss function with a name that is already registered, the <code>LossManager</code> will raise an exception.</p> <p>Running your loss function via the loss manager will return the result of the given loss function's <code>forward()</code> function as a <code>torch.Tensor</code>. The Tensor that is return has had weights pre-applied by <code>LossManager.compute_loss_xy() -&gt; LossManager._weight_loss()</code>, based on whatever the current weight value of the registered loss is:</p> <pre><code>import torch\n\ny = torch.Tensor()      # Ground truth\ny_fake = torch.Tensor() # Generator output \n\nresult: torch.Tensor = loss_manager.compute_loss_xy(loss_name='mylossfunction', x=y_fake, y=y)\n</code></pre>"},{"location":"api/losses/lossmanager/#querying-registered-loss-data","title":"Querying Registered Loss Data","text":""},{"location":"api/losses/lossmanager/#data-relating-to-losses-registered-with-a-given-lossmanager-instance-can-be-retrived-in-a-variety-of-ways-dependent-upon-exactly-what-data-you-are-trying-to-query-and-what-format-you-would-like-it-returned-in","title":"Data relating to losses registered with a given <code>LossManager</code> instance can be retrived in a variety of ways, dependent upon exactly what data you are trying to query, and what format you would like it returned in.","text":""},{"location":"api/losses/lossmanager/#querying-lmloss-objects-directly","title":"Querying LMLoss Objects Directly","text":"<p>The most flexible method is to just query the raw <code>LMLoss</code> objects directly. This can be done as follows:</p> <pre><code>losses: dict[str, LMLoss] = loss_manager.get_registered_losses(query=None)\n</code></pre> <p>This will return all registered loss functions as a dict. The key for each loss will be the name it was registered with. So for our above example, we could then query any info related to our <code>mylossfunction</code> function like this:</p> <pre><code>mylossfn: LMLoss = losses['mylossfunction'] # Query the LMLoss object\nlossfn = mylossfn.function                  # Get the loss function module\nloss_map = mylossfn.last_lost_map           # Get the most recent loss result as a torch.Tensor\n</code></pre> <p>[!NOTE] There is one thing to be aware of when querying the <code>LMLoss</code> objects directly like this. Were you to do this, expecting to get the loss value and weights history lists: <code>python values : dict[str, float] = mylossfn.history.losses weights: dict[str, float] = mylossfn.history.weights</code> You would find that <code>weights</code> and <code>values</code> are empty lists. This is intentional. <code>LossManager.get_registered_losses()</code> has an optional flag called <code>strip</code> which defaults to <code>True</code>. If this flag is not overridden with a value <code>Dalse</code>, the losses <code>history.values</code> and <code>history.weights</code> lists are cleared. </p> <p>The reasoning behind this is that dependent upon what the <code>history_buffer_size</code> of the <code>LossManager</code> is set at, these lists can get fairly long. And if you have a significant amount of registered losses, passing them around can become a fairly heavy task, so they are stripped by default to reduce the memory overhead. If you need these values for whatever reason, though, just call <code>LossManager.get_registered_losses()</code> with <code>strip=False</code>. All that said, however, as long as the <code>LossManager</code>'s <code>history_buffer_size</code> is kept to below a reasonable value (i.e. ~100,000), the cost realistically isn't all that concerning.</p>"},{"location":"api/losses/lossmanager/#querying-loss-values-as-a-dict","title":"Querying Loss Values (as a dict)","text":"<p>Loss values can be retrieved in dictionary form as follows:</p> <pre><code>values: dict[str, float] = loss_manager.get_loss_values(precision=2)\n</code></pre> <p>This will return a dictionary of floating point values with lookup keys matching the corresponding loss's name. The optional <code>precision</code> argument tells the function how many digits after the decimal point to round the returned loss values to. The default is <code>2</code> to keep things tidy if you want to print or otherwise log the values, but you can increase it if you need more precise return values.</p> <p>[!IMPORTANT] <code>LossManager.get_loss_values()</code> uses the last value stored in <code>LossManager.history</code> for each loss. As such, this function should generally be called AFTER all of the registered loss functions have been run for the batch. Calling it before running some or all of the loss funtions could lead to unexpected results.</p>"},{"location":"api/losses/lossmanager/#querying-loss-values-as-a-tensor","title":"Querying Loss Values (as a tensor)","text":"<p>The most recent loss tensor, which is detached and stored in the loss function's <code>LMLoss</code> every time the loss is called with <code>LossManager.compute_loss_xy()</code>, can be queried with:</p> <pre><code>tensors: torch.Tensor = loss_manager.get_loss_tensors()\n</code></pre> <p>[!IMPORTANT] <code>LossManager.get_loss_tensors()</code> uses the last value stored in the <code>LMLoss</code> objects <code>last_loss_map</code>. As such, this function should generally be called AFTER all of the registered loss functions have been run for the batch. Calling it before running some or all of the loss funtions could lead to unexpected results.</p>"},{"location":"api/losses/lossmanager/#querying-loss-weights","title":"Querying Loss Weights","text":"<p>The current weight value of all registered losses can be retrieved as follows:</p> <pre><code>weights: dict[str, float] = loss_manager.get_loss_weights(precision=2)\n</code></pre> <p>This will return a dictionary of floating point values with lookup keys matching the corresponding loss's name. The values will represent the current weight of the loss at the time that the function was run. The optional <code>precision</code> argument tells the function how many digits after the decimal point to round the returned weight values to. The default is <code>2</code> to keep things tidy if you want to print or otherwise log the values, but you can increase it if you need more precise return values.</p> <p>[!IMPORTANT] <code>LossManager.get_loss_weights()</code> uses the last weight value stored in <code>LossManager.schedule.current_value</code>. Since this value is updated immediately after the associated loss has been run, the weight values that this function returns are the ones which will be applied the next time the loss is run.</p>"},{"location":"api/losses/lossmanager/#using-tags-to-query-registered-losses","title":"Using Tags to Query Registered Losses","text":"<p>All of the above loss querying functions accept an optional argument called <code>query</code>, which is a list of strings. This can be used to query loss values by tag; only loss values which have a tag matching one of the strings in the input <code>query</code> argument will be returned. </p> <p>For example, when we registered our loss function above, we assigned it a tag of <code>descriptive_lookup_tag</code>. Were we to then want to query losses with that tag, it would look something like this:</p> <pre><code>lossfns: dict[str, LMLoss] = loss_manager.get_registered_losses(query=['descriptive_lookup_tag'])\nvalues : dict[str, float]  = loss_manager.get_loss_values(query=['descriptive_lookup_tag'])\ntensors: dict[str, Tensor] = loss_manager.get_loss_tensors(query=['descriptive_lookup_tag'])\nweights: dict[str, float]  = loss_manager.get_loss_weights(query=['descriptive_lookup_tag'])\n</code></pre>"},{"location":"api/losses/lossmanager/#dumping-cached-values-to-loss-log","title":"Dumping Cached Values to Loss Log","text":"<p>[!NOTE] This section is only applicable if the given <code>LossManager</code> instance was intialized with <code>enable_logging=True</code>.</p> <p>When a <code>LossManager</code> instance is initialized, an optional argument can be passed called <code>history_buffer_size</code>. This value defines how many values (stored as 32bit floating point values) can be stored in each list (i.e. <code>losses</code>, <code>weights</code>) of each LMLoss's LMHistory container. By default, this is <code>50000</code>. So, with the default value, each registed loss is allowed to store 50,000 unique previous loss values and 50,000 unique previous loss weight values. 100,000 32bit floats per registered loss. This is a totally acceptable fallback value on any modern system (you could mutiply this value by 100 and still be totally safe) and this value felt fine on every dataset I tested it on, but you are also welcome to set the value higher when you initialize the <code>LossManager</code>. The Toolbox <code>Pix2pixTrainerWorker</code> actually bypasses this cap altogether by always setting the buffer size to a value that is slightly higher than what would be dictated by the options selected in the UI.</p> <p>Whenever any registered loss is run from any <code>LossManager</code> instance, the <code>LossManager</code> first does a quick check to see if the buffer for that loss is full. If it is, it will dump the loss's buffer to the log, clear the buffer, then run the loss function it was originally going to run and appends the result to the now freed up buffer. This is a fine way to handle loss logging with the <code>LossManager</code>. It is set and forget, just give it a value for the <code>history_buffer_size</code> and the <code>LossManager</code> will take care of the loss logging and memory management from there.</p> <p>However, if you would like more control over when exactly the logs are dumped (at the end of each epoch, for example, as in the core <code>Pix2pixTrainer</code>'s <code>on_epoch_end()</code> function. Or maybe each <code>x</code> number of epochs, as in the Toolbox <code>Pix2pixTrainerWorker</code>'s <code>run()</code> function), you can instead force the <code>LossManager</code> to dump its buffers to the loss log with:</p> <pre><code>loss_manager.update_loss_log(silent=True, capture=False)\n</code></pre> <p>[!TIP] <code>LossManager.update_loss_log()</code> has two optional boolean arguments, <code>silent</code> (default=True) and <code>capture</code> (default=False). If <code>silent</code> is <code>False</code>, the <code>LossManager</code> will print a string to the console after it has dumped it values to the log with a timestamp showing how long it took to perform the operation. If silent is <code>False</code> and the other optional argument, <code>capture</code>, is <code>True</code> however, the string that would have been printed is instead returned by the function. </p>"},{"location":"api/losses/lossmanager/#loss-specs","title":"Loss Specs?","text":"<p>Loss specifications (specs) are a novel way to define reusable objective functions for your models. At their core, loss specs are just standard Python functions but with one specific requirement, the must return a dictionary of string-mapped <code>LMLoss</code> objects.</p>"},{"location":"api/losses/lossmanager/#see-here-for-more-information-on-loss-specifications","title":"See here for more information on loss specifications","text":""},{"location":"api/losses/lossmanager/#convenience-functions","title":"Convenience Functions","text":""},{"location":"api/losses/lossmanager/#the-lossmanager-includes-two-convenience-functions-for-printing-debug-values-to-the-console-during-training-these-are","title":"The <code>LossManager</code> includes two convenience functions for printing debug values to the console during training. These are:","text":""},{"location":"api/losses/lossmanager/#lossmanagerprint_losses","title":"<code>LossManager.print_losses()</code>","text":"<p>From: nectargan.losses.loss_manager.print_losses()</p> <pre><code>Prints (or returns) a string of all the most recent loss values.\n\nNote: This function uses the last value stored in LossManager.history \nfor each loss. As such, this function should generally be called AFTER \nall of the registered loss functions have been run for the batch. \nCalling it before running some or all of the loss funtions could lead \nto unexpected results.\n\nBy default, this function will print a string of all registered losses \nand their most recent values, tagged with epoch and iter, formatted as:\n\n\"(epoch: {e}, iters: {i}) Loss: {L_1_N}: {L_1_V} {L_2_N}: {L_2_V} ...\"\n\nKey:\n    e : input epoch\n    i : input iter\n    L_X_N : Loss X name\n    L_X_V : Loss X value\n</code></pre>"},{"location":"api/losses/lossmanager/#lossmanagerprint_weights","title":"<code>LossManager.print_weights()</code>","text":"<p>From: nectargan.losses.loss_manager.print_weights()</p> <pre><code>Prints (or optionally returns) loss weight information.\n\nNote: This function uses the last weight value stored in:\n    - `LossManager.schedule.current_value`\nSince this value is updated immediately after the associated loss has\nbeen run, the weight values that this function prints (or returns) are \nthe ones which will be applied the next time the loss is run.\n\nBy default, this function will print a string of all registered losses \nand their most recent weights formatted as:\n\n\"Loss weights: {L_1_N}: {L_1_W} {L_2_N}: {L_2_W} ...\"\n\nKey:\n    L_X_N : Loss X name\n    L_X_W : Loss X weight\n</code></pre>"},{"location":"api/models/patchgan/","title":"NectarGAN API - PatchGAN","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/models/patchgan/#nectargan-includes-a-modular-configurable-patchgan-style-discriminator-model","title":"NectarGAN includes a modular, configurable PatchGAN-style discriminator model.","text":""},{"location":"api/models/patchgan/#what-is-a-patchgan","title":"What is a PatchGAN?","text":"<p>A PatchGAN is essentially just a convolutional neural network. Where a traditional GAN disciminator reduces input to a single scalar value (real or fake), a PatchGAN instead convolves the input to an array of values, each of which represents whether the corresponding patch (a 70x70 pixel square by default) in the image is real or fake.</p> <p>I strongly encourage you to read this article. The author provides a fantastic visual walkthrough of the inner workings of the architecture.</p>"},{"location":"api/models/patchgan/#patchgan-model","title":"PatchGAN Model","text":"<p>Reference: <code>nectargan.models.patchgan.model</code></p> <p>The root of the PatchGAN implementation in NectarGAN is the <code>Discriminator</code> class. This class assembles a PatchGAN style discriminator and provides a <code>forward</code> function for making predictions using the PatchGAN model.</p>"},{"location":"api/models/patchgan/#discriminator__init__","title":"<code>Discriminator.__init__()</code>","text":"<p>Let's first look at the <code>Discriminator</code> class's <code>__init__</code> function. We can see it only takes 4 arguments: | Argument | Type | Description | | :---: | :---: | --- | <code>in_channels</code> | <code>int</code> | The number of channels in the input images (i.e. <code>1</code> for mono, <code>3</code> for RGB). <code>base_channels</code> | <code>int</code> | The number of output channels on the first conv layer. <code>n_layers</code> | <code>int</code> | The number of conv layers to add to the discriminator model. The actual layer count of the discriminator will be <code>n_layers+1</code> because a final layer is added which reduces the channel count to <code>1</code> for the final predictions on each patch. <code>max_channels</code> | <code>int</code> | The maximum allowed channels for any given conv layer in the model.</p> <p>Now let's quickly walk through this <code>__init__</code> function to see how it is used to assemble the network: 1. We super init the parent <code>torch.nn.Module</code> class. 2. We create a new list, <code>self.layers</code>, to store our convolutional layers. 3. We run the function <code>add_initial_layer()</code>. This function:     - Appends a <code>torch.nn.Conv2d</code> module, setting the in channels to <code>in_channels * 2</code> (because remember, this network is passed two tensors, an input and ground truth, or a fake and ground truth), and set the output channels to <code>base_channels</code>. This layer uses a <code>4x4</code> kernel, <code>stride=2</code>, and reflection padding of <code>1</code>.     - Appends a <code>torch.nn.LeakyReLU</code> for activation.  4. We initialize a member variable, <code>self.in_ch</code>, to keep track of the current channel counts in the next stage, and set its value to the value of the <code>base_channels</code> argument. 5. We then run the function <code>add_n_layers()</code>. This function adds <code>n_layers-1</code> conv layers to the discriminator. In does this in a loop, where each iteration:     - Creates a variable, <code>out_channels</code>, and sets it to the min of <code>self.in_ch * 2</code>, and <code>max_channels</code>, doubling the channel count while enforcing the hard channel cap.     - Picks a stride value for the current iteration's conv layer. This value is <code>2</code> for every layer except the final one, which instead uses a stride of <code>1</code>.     - Create a <code>CNNBlock</code> module (explained below), setting the input channels to <code>self.in_ch</code> and the output channels to our new <code>out_ch</code> value, and passing it the <code>stride</code> value we just discussed. This <code>CNNBlock</code> module also uses a <code>4x4</code> kernel with reflection padding of <code>1</code>. However, it also includes a <code>torch.nn.InstanceNorm2d</code> and a <code>torch.nn.LeakyReLU</code> module.     - Then we update <code>self.in_ch</code> with the new <code>out_ch</code> value for the next iteration. 6. After that, we add our final conv layer. This layer uses a <code>4x4</code> kernel with <code>stride=1</code> and <code>padding=1</code>. With a 256x256 input resolution, and with the layers we've assembled up to this point, this kernel size corresponds to a 70x70 pixel receptive field on the input image (hence the common name <code>70\u00d770 PatchGAN</code>). This layer also reduces the channels from whatever value <code>self.in_ch</code> was after the final loop iteration, down to a value of <code>1</code>, representing a real/fake prediction on the given patch.  7. Then we take our list of layers, unpack them into a <code>torch.nn.Sequential</code>, and store the result in <code>self.model</code>.</p> <p>The best way to illustrate this is by just looking at what exactly happens to a tensor's shape as it is passed through the network: | Layer | Input Shape | Output Shape | Kernel | Stride | Padding | Receptive Field | Notes | | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | Initial | [1,\u00a06,\u00a0256,\u00a0256] | [1,\u00a064,\u00a0128,\u00a0128] | 4x4 | 2 | 1 | 4x4 | Takes channels from <code>in_channels*2</code> -&gt; <code>base_channels</code> and performs a <code>4x4</code> convolution with <code>stride=2</code>, halving the tensor's spatial resolution. <code>n_layer</code>\u00a0#1 | [1,\u00a064,\u00a0128,\u00a0128] | [1,\u00a0128,\u00a064,\u00a064] | 4x4 | 2 | 1 | 10x10 | Doubles channel count, halves spatial resolution. <code>n_layer</code>\u00a0#2 | [1,\u00a0128,\u00a064,\u00a064] | [1,\u00a0256,\u00a064,\u00a064] | 4x4 | 1 | 1 | 22x22 | Doubles channel count, but since <code>stride=1</code> on this final <code>n_layer</code>, spatial resolution remains at <code>64x64</code>. Final | [1,\u00a0256,\u00a064,\u00a064] | [1,\u00a01,\u00a064,\u00a064] | 4x4 | 1 | 1 | 70x70 | Reduces channel count from <code>256</code> (final value from <code>n_layers</code> loop) to <code>1</code>. And again, since <code>stride=1</code>, spatial resolution remains unchanged.</p> <p>[!NOTE] This example assumes RGB input images (<code>in_channels=3</code>) with a resolution of <code>256x256</code>, <code>base_channels=64</code>, and <code>n_layers=3</code>.</p> <p>And that's really all there is to the PatchGAN <code>Discriminator</code> model. When fed a set of input tensors, the class's forward function simply concatenates them together along the channel dimension and runs the resulting tensor through the <code>Sequential</code> outlined above, then it returns the final prediction map tensor.</p>"},{"location":"api/models/patchgan/#cnnblock","title":"CNNBlock","text":"<p>Reference: <code>nectargan.models.patchgan.blocks</code></p> <p>Let's have a really quick look at the <code>CNNBlock</code> class. This class is super simple, so we won't spend much time on it, but we will briefly touch on what exactly it does. It's basically just a hard coded wrapper which assembles a simple <code>torch.nn.Sequential</code> defining a single downsampling layer. The modules in this <code>Sequential</code> are: 1. <code>torch.nn.Conv2d</code> : A <code>Conv2d</code> with <code>kernel_size=4</code>, reflection padding of <code>1</code>, <code>bias=False</code>, and a <code>stride</code> equal to the value of the input <code>stride</code> argument (decided by the <code>stride</code> logic in the <code>n_layers</code> loop of the <code>Discriminator</code>). 2. <code>torch.nn.InstanceNorm2d</code> : Instance normalization. 3. <code>torch.nn.LeakyReLU</code> : <code>LeakyReLU</code> for activation with a negative slope of <code>0.2</code> and <code>inplace=True</code>.</p> <p>The <code>forward</code> function is equally simple. All it does is take an input tensor <code>x</code>, runs it through this <code>Sequential</code>, and returns the result as a tensor.</p>"},{"location":"api/models/unet/","title":"NectarGAN API - UNet","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/models/unet/#the-nectargan-api-includes-a-modular-configurable-unet-style-generator-model","title":"The NectarGAN API includes a modular, configurable UNet-style generator model.","text":""},{"location":"api/models/unet/#unet-model","title":"UNet Model","text":"<p>Reference: <code>nectargan.models.unet.model</code></p>"},{"location":"api/models/unet/#initializing-a-unetgenerator","title":"Initializing a <code>UnetGenerator</code>","text":"<p>We'll start by having a look at the <code>UnetGenerator</code>. Looking at it's <code>__init__</code> function, we can see that it takes the following arguments: | Arguments | Description | | :---: | --- | <code>input_size</code> | The intended input resolution (^2) for the generator. This is used for validation of training image resolution against downsampling layer count. <code>in_channels</code> | The number of channels in the input dataset images (i.e. <code>1</code> for <code>mono</code>, <code>3</code> for <code>RGB</code>). Currently, 3 is the only known supported value. Though technically, that's not actually true of the generator model. It should work with other values. The <code>dataset loader</code> will not in its current state, however. <code>features</code> | The number of features on the first downsampling layer. Feature count for any given conv layer is capped at <code>features * 8</code>.  <code>n_downs</code> | The number of downsampling layers (Note: This value does not include the first layer or the bottleneck. This will be explained in greater detail below.). <code>use_dropout_layers</code> | The number of upsampling layers (starting from the deepest layer) to apply dropout on. <code>block_type</code> | What block type to use when assembling the generator model (see here for more info). <code>upconv_type</code> | What upsampling method to use:- <code>Transposed</code> : Transposed convolution.- <code>Bilinear</code> : Bilinear upsampling, then convolution.Transposed convolution is the upsampling method traditionally used in the Pix2pix model. However, bilinear upsampling + convolution can help to eliminate the checkboard artifacting which is commonly seen in these models. See here for more info.</p> <p>So then, with these arguments in mind, a <code>UnetGenerator</code> can be instantiated as follows: </p> <pre><code>from nectargan.models.unet.model import UnetGenerator\nfrom nectargan.models.unet.blocks import UnetBlock\n\ngenerator = UnetGenerator(\n    input_size=256,          # Train on 256^2 resolution images\n    in_channels=3,           # with 3 channels: (R, G, B)\n    features=64,             # 64 output features on first down layer (with a cap of 512)\n    n_downs=6,               # 6 downsampling layers (+1 for initial, +1 for bottleneck)\n    block_type=UnetBlock,    # Or ResidualUnetBlock\n    upconv_type='Transposed' # Or 'Bilinear'\n)\n</code></pre>"},{"location":"api/models/unet/#unet-architecture","title":"UNet Architecture","text":"<p>Let's quickly go over how the UNet architecture actually works, so that we can better understand how it's implemented in the <code>UnetGenerator</code>. This will be a relatively brief explanation, and it will have some prerequisite knowledge, so if you are totally unfamiliar with UNet, I first encourage you to check out the resources below. If you already have a good understanding of the UNet architecture, feel free to skip this section and move on to the NectarGAN implementation.</p> <p>Resources: - Original UNet Paper - Aladdin Persson's Videos on UNet: 1, 2, 3 - Towards Data Science | Understanding U-Net</p> <p>Let's walk through it, starting at the top left:</p> <p>[!NOTE] Skip connections: (This is explained in more depth below, but here is a quick explanation to start.)</p> <p>For every <code>down</code> layer, the output tensor is passed to the next down layer as input, but it is also passed via the skip connection to the corresponding <code>up</code> layer, as denoted by the rightward facing arrows (not directly passed as input, but concatenated with the output tensor of the up layer below). So, the output tensor from <code>init_down</code> is passed to the <code>final_up</code>, same for <code>down1</code> and <code>up7</code>, etc. - We take our input (<code>[1, 3, 512, 512]</code>) and feed it into the <code>init_down</code> layer. This gives us an output tensor with the shape <code>[1, 64, 256, 256]</code>. So we halved the spatial resolution of the tensor, and increased the feature count to <code>64</code>, as defined by the generator's <code>features</code> value.  - We take that output tensor (<code>[1, 64, 256, 256]</code>) and feed it in to <code>down1</code>, giving us an output shape of <code>[1, 128, 128, 128]</code>. Double the features, halve the resolution. - We do this again for <code>down2</code> and <code>down3</code>. Now, though, we've hit our feature cap (<code>features * 8</code>, or <code>512</code>). - So, for <code>down4</code>, <code>down5</code>, and <code>down6</code>, we continue to halve the spatial resolution, but the feature count remains at the cap of <code>512</code> - Then we hit the bottleneck. As with the previous few downsampling layers, we halve the resolution, but keep the same feature count. This time, though, note that there is no skip connection, since there's obviously nothing to connect it to, as the output of the bottleneck is fed directly into the first upsampling layer, <code>up1</code>.</p> <p>Then, we have our upsampling path. As noted, the first upsampling layer, <code>up1</code>, takes the output tensor of the bottneck layer directly. This layer doubles the spatial resolution of the tensor, but, mirroring the feature counts on the downsampling path, this layer keeps the feature count at the <code>512</code> cap. This results in an output tensor shape of <code>[1, 512, 4, 4]</code>.</p> <p>Then things start to become interesting. Note that the output tensor from <code>down6</code> has the same shape as the output tensor from <code>up1</code>. This allows us to take those two tensors and stack the feature maps before passing them to the next up layer. These <code>skip connections</code> are the magic of the UNet architecture (and a couple others. ResNet, for example, also uses skip connections, albeit in a slightly different manner and for a slightly different reason). </p> <p>By stacking the feature map taken directly from the corresponding downsampling layer with the one from the previous upsampling layer, we allow the network to preserve sharp or fine details that would otherwise be lost with upsampling alone. </p> <p>A visual example of the concept:  We can see that, were we to use just directly use the result of the upsampling path, we would lose a significant amount of detail. However, if we concatenate it with the output of the corresponding downsampling layer, we preserve a lot of that fine detail.</p> <p>[!NOTE] The above is just a visual example. This isn't exactly what's happening, but the concept and result are very similar.</p>"},{"location":"api/models/unet/#from-nectarganmodelsunetmodel","title":"From <code>nectargan.models.unet.model</code>","text":"<p>```python                 UNet-esque generator architecture</p> <p>Input: [1, 3, 512, 512] (512^2 RGB)                     Output:  [1, 3, 512, 512]                         \u2193                                 \u2191   |Input Shape|    |Down Layer|    |Output/Skip Shape| |Up Layer|  |Output Shape|                         \u2193                                 \u2191 [1, 3, 512, 512] ---&gt; init_down -&gt; [1, 64, 256, 256] --&gt; final_up --&gt; [1, 3, 512, 512]                         \u2193                                 \u2191 [1, 64, 256, 256] --&gt; down1 -----&gt; [1, 128, 128, 128] -&gt; up7 -------&gt; [1, 64, 256, 256]                         \u2193                                 \u2191 [1, 128, 128, 128] -&gt; down2 -----&gt; [1, 256, 64, 64] ---&gt; up6 -------&gt; [1, 128, 128, 128]                         \u2193                                 \u2191 [1, 256, 64, 64] ---&gt; down3 -----&gt; [1, 512, 32, 32] ---&gt; up5 -------&gt; [1, 256, 64, 64]                         \u2193                                 \u2191 [1, 512, 32, 32] ---&gt; down4 -----&gt; [1, 512, 16, 16] ---&gt; up4 -------&gt; [1, 512, 32, 32]                         \u2193                                 \u2191 [1, 512, 16, 16] ---&gt; down5 -----&gt; [1, 512, 8, 8] -----&gt; up3 -------&gt; [1, 512, 16, 16]                         \u2193                                 \u2191  [1, 512, 8, 8] -----&gt; down6 -----&gt; [1, 512, 4, 4] -----&gt; up2 -------&gt; [1, 512, 8, 8]                         \u2193                                 \u2191 [1, 512, 4, 4] ---&gt; bottleneck --&gt; [1, 512, 2, 2] \u2192 \u2192 \u2192 \u2192 up1 -&gt; [1, 512, 4, 4] <code>`` This is a diagram I made to help me conceptualize the tensor shapes of the inputs and skip connections at various depths in a UNet-style architecture (pardon the formatting, I wrote it directly inside of the python file). **This diagram shows a network with an input which has 3</code>in_channels<code>(R, G, B), 64</code>features`, and an input resolution of 512^2.**</p>"},{"location":"api/models/unet/#channel-mapping","title":"Channel Mapping","text":"<p>Reference: <code>nectargan.models.unet.model.UnetGenerator.build_channel_map()</code></p> <p>Alright, now that we've covered the basics of how the UNet architecture functions, let's now have a look at how it's implemented in NectarGAN. The core of this is the channel mapping function. This function assembles two lists of tuples, one for the downsampling channels, and another for the upsampling channels. Each tuple in each list corresponds to a layer, and contains two <code>int</code> values:</p> <ol> <li>The number of input channels for the layer.</li> <li>The number of output channels for the layer.</li> </ol> <p>Let's walk through the function step by step to understand how those lists are created: 1. We create the first of our two lists, <code>down_channels</code>. In doing so, we also add the first tuple to it with the input and output channel counts for the first downsampling layer, since we know this is always going to be (<code>in_channels</code>, <code>features</code>). 2. We create two variables, <code>in_features</code> and <code>out_features</code>, and initialize both to the <code>features</code> value used when initializing the <code>UnetGenerator</code>. 3. We create a loop with an iteration count of <code>n_downs</code>, in which we:     - First, set <code>in_features</code> to the max of <code>out_features</code>, and <code>features*8</code>, because we want to make sure it doesn't go beyond that hard cap.     - Then, we set <code>out_features</code> to the min of <code>out_features*2</code> and <code>features*8</code>. This defines the doubling of the feature maps with each encoder layer.     - We append a new entry to <code>down_channels</code> containing our new <code>in_channels</code> and <code>out_channels</code>. Then do it again for the next layer, and the next, and so on. So the <code>out_channels</code> value of the previous iteration becomes the <code>in_channels</code> value of the next iteration. Up to the point where they cap out at <code>features*8</code>. The best way to illustrate this is just to list the results for each iteration, like so:     <code>``     NOTE: This example assumes an</code>input_channels<code>value of</code>3<code>and a</code>features` value of 64.</p> <pre><code>First Layer: (3, 64)\nIteration 1: (64, 128)\nIteration 2: (128, 256)\nIteration 3: (256, 512)\nIteration 4: (512, 512)\n\nAnd now we've reached our maximum feature count, all subsequent layers will have a value of (512, 512).\n```\n</code></pre> <ol> <li>Now, we create a list for our skip connection channel sizes. To do this, we perform two operations at once:<ul> <li>We reverse the list of <code>down_channels</code>.</li> <li>While doing so, we also flip the <code>in_channels</code> and <code>out_channels</code> of each entry, since the direction is reversed on the upsampling path and the channel sizes need to reflect that.</li> </ul> </li> <li>Now, we create our other list, <code>up_channels</code>, which holds the channel sizes for the layers in the upsampling path. To do this, we just make a new list from our <code>skip_channels</code> list, but for each entry, we double the <code>in_channels</code> value, to reflect the fact that we are going to be concatenting the skip tensor with the previous upsampling tensor for the input of each of these layers.</li> <li>Finally, we insert one additional layer at the beginning of <code>up_channels</code>. This represents the deepest upsampling layer, which takes its input directly from the bottleneck layer.</li> </ol> <p>Then at the very end, we make a dictionary from the lists to make it a bit easier to look up our values when defining our actual layers.</p>"},{"location":"api/models/unet/#layer-definitions","title":"Layer Definitions","text":"<p>The layers in the <code>UnetGenerator</code> are defined by these three functions:</p> <ol> <li><code>define_downsampling_blocks</code></li> <li><code>define_bottleneck</code></li> <li><code>define_upsampling_blocks</code></li> </ol> <p>We won't go too deep in to these, this document is getting long enough as it is. But in short, these three functions use the channel map which we discussed above to define the blocks in the upsampling, downsampling, and bottleneck layers, setting the correct values based on layer type and depth. The default configuration sets the values exactly as a traditional UNet does.</p>"},{"location":"api/models/unet/#forward","title":"Forward","text":"<p>Reference: <code>nectargan.models.unet.model.UnetGenerator.forward()</code></p> <p>Lastly, we will have a look at the <code>forward</code> function for the <code>UnetGenerator</code> class. This function uses the layers defined by the above three functions to assemble a UNet style architecture and passes the input tensor <code>x</code> through it.</p> <p>Walking through this function, we: 1. Run the tensor through the initial downsampling layer, halving the spatial resolution, and taking the feature count from the value of <code>in_channels</code> to the value of <code>features</code>. 2. Create a new list, <code>skips</code>, to store our skip connection tensors, and add the result of the initial downsampling layer to it. 3. We loop through all of our additional downsampling layers, running the tensor through each subsequent one and appending the resulting tensor to our list of <code>skips</code>. 4. Reverse our list of skips to align them with the input tensors in the upsampling path. 5. Run our <code>x</code> tensor through the bottleneck layer. Note that we do not append the result to <code>skips</code>, since the bottleneck just passes its results directly to the first upsampling layer. 6. Run out <code>x</code> tensor through our first upsampling layer. 7. Loop through our additional upsampling layers, first grabbing the corresponding skip connection, then concatenating the <code>x</code> tensor with the correspondinding skip tensor along the channel dimension and, finally, inputting that concatenated tensor into the given upsampling layer. 7. After we finish the loop of upsampling layers, we take the resulting <code>x</code> tensor, concatenate it with the final skip tensor, and feed it in to the final upsampling layer, returning the result.</p> <p>And that's basically all there is to the <code>UnetGenerator</code>. We've now seen how the channel map and layers are assembled when the generator is initialized, and we've seen how that architecture is used whenever the generator's <code>forward</code> function is called. The last note to touch on is...</p>"},{"location":"api/models/unet/#unet-blocks","title":"UNet Blocks","text":"<p>Reference: <code>nectargan.models.unet.blocks</code></p> <p>Please see here for more information about the drop-in block system used by the <code>UnetGenerator</code>.</p>"},{"location":"api/models/unet/#references","title":"References","text":"<ul> <li>U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)</li> <li>Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) (Md Zahangir Alom et al.)</li> <li>ResUNet: A Deep Neural Network for Semantic Segmentation of Urban Scenes (Diakogiannis et al.)</li> <li>Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2017)</li> </ul>"},{"location":"api/models/unet_blocks/","title":"NectarGAN API - UNet Blocks","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/models/unet_blocks/#the-nectargan-unet-generator-accepts-drop-in-convolutional-blocks-which-which-can-dramatically-change-the-behaviour-of-the-model-currently-there-are-two-built-in-block-types","title":"The NectarGAN UNet generator accepts drop-in convolutional blocks which which can dramatically change the behaviour of the model. Currently, there are two built in block types.","text":"<p>[!NOTE] This document is intended as a followup of the UNet API documentation. It is advisable to read that document before beginning this one.</p>"},{"location":"api/models/unet_blocks/#what-is-a-unet-block","title":"What is a UNet Block?","text":"<p>In the simplest terms, a Unet block in NectarGAN is just a class which inherits from <code>torch.nn.Module</code>, and which has a forward function that takes a tensor as input, and which returns a tensor. However, there are a few additional requirements.</p> <ol> <li>Each UNet block class in NectarGAN is responsible for both upsampling and downsampling.</li> <li>Currently all UNet blocks in NectarGAN must take a set list of arguments, which are set by the UNet generator based on the layer type and depth. They don't need to use every argument, but they must accept them (This may change in the future though. This was a fairly early API component and could do with an overhaul.).These arguments are:     | Argument | Type | Description |     | :---: | :---: | --- |     <code>in_channels</code> | <code>int</code> | The input channel count for the layer. Defined by the layer type and depth based on the result of the <code>UnetGenerator</code> channel mapping function.     <code>out_channels</code> | <code>int</code> | The output channel count for the layer. Defined by the layer type and depth based on the result of the <code>UnetGenerator</code> channel mapping function.     <code>upconv_type</code> | <code>str</code> | What upsampling method to use:- <code>Transposed</code> : Transposed convolution.- <code>Bilinear</code> : Bilinear upsampling, then convolution.Transposed convolution is the upsampling method traditionally used in the Pix2pix model. However, bilinear upsampling + convolution can help to eliminate the checkboard artifacting which is commonly seen in these models. See here for more info.Note, though, that bilinear upsampling is static. Unlike <code>torch.nn.ConvTranspose2d</code>, no filters are learned during bilinear upsampling. This can cause the model to have a more difficult time learning fine details and, depending on the use case, can stop it from doing so altogether. To help alleviate this, consider pairing bilinear upsampling with a perceptual loss, like the included <code>VGGPerceptual</code>.     <code>activation</code> | <code>str\\|None</code> | The activation type for the layer. There are four values which are passed to the blocks by the generator, and which should be processed accordingly by the block. These are:- <code>'leaky'</code> : <code>torch.nn.LeakyReLu()</code>- <code>'relu'</code> : <code>torch.nn.ReLU()</code>- <code>'tanh'</code> : <code>torch.nn.Tanh()</code>- <code>None</code> : <code>torch.nn.Identity()</code>Technically, these can be processed however you want. The generator model passes each at the stages they are supposed to be used in the UNet architecture though, so using modules which behave differently will likely cause strange behaviour. <code>norm</code> | <code>str\\|None</code> | What type of normalization to use. Currently, there are only two values which will ever be passed by the UNet generator model:- <code>'instance'</code> : <code>torch.nn.InstanceNorm2d()</code>- <code>None</code> : <code>torch.nn.Identity()</code>Again, these can techincally be whatever, but these values are what the model is expecting it to be. This will also likely change in the future once normalization methods get plugged in to the Toolbox UI. <code>down</code> | <code>bool</code> | On downsampling layers (including the bottleneck layer), the <code>UnetGenerator</code> will pass a value of <code>True</code> for this argument. On upsampling layers, it will pass a value of <code>False</code>. This should be used to determine whether to run convolution or transposed convolution (or bilinear + conv2d) when the block is called.     <code>bias</code> | <code>bool</code> | The <code>UNetGenerator</code> will pass a value of <code>True</code> for this argument on the first downsampling layer, the bottleneck, and the last upsampling layer. On every other layer, it will be passed a value of <code>False</code>. Used as a direct passthrough for the bias argument in <code>torch.nn.Conv2d</code> and <code>torch.nn.ConvTranspose2d</code> in the base <code>UnetBlock</code>.     <code>use_dropout</code> | <code>bool</code> | This will be passed a value of <code>False</code> for every layer except the first X upsampling layers (starting from the deepest), with X defined by the <code>use_dropout_layers</code> argument used to initialize the generator (default <code>3</code>). For these layers, it will be passed a value of <code>True</code>. Used in the Unet blocks to define whether dropout should be applied to a given layer.</li> </ol> <p>Now then, let's have a quick look at the two block types currently included in the NectarGAN API.</p>"},{"location":"api/models/unet_blocks/#unetblock","title":"UnetBlock","text":"<p>Reference: <code>nectargan.models.unet.blocks.UnetBlock</code></p>"},{"location":"api/models/unet_blocks/#unetblock__init__","title":"<code>UnetBlock.__init__()</code>","text":"<p>This is a very standard UNet block. Walking through it, we: 1. Super init the <code>torch.nn.Module</code> parent class. 2. Create an empty list to append our modules to. 3. Check whether this is a downsampling block:     - If yes: We just append a <code>Conv2d</code> module with <code>kernel_size=4</code>, <code>stride=2</code>, and <code>padding=1</code>. We also pass it the input <code>bias</code> and set its <code>padding_mode</code> to <code>reflect</code>.     - If No: We do another check on the <code>upconv_type</code>. There are only two values it could be:         1. <code>Transposed</code> : We just append a <code>torch.nn.ConvTranspose2d</code>, again with <code>kernel_size=4</code>, <code>stride=2</code>, and <code>padding=1</code>, and pass it the <code>bias</code> argument.         2. <code>Bilinear</code> : Here we instead first append a <code>torch.nn.Upsample</code> with a <code>scale_factor</code> of <code>2</code> (double the spatial resolution). Then we apply reflection padding and use use a standard <code>Conv2d</code>, this time with a <code>stride</code> of <code>1</code> though, since we've already increased the resolution. 4. Check the value of the <code>norm</code> argument. If it's <code>instance</code>, we apply a <code>torch.nn.InstanceNorm2d</code>. If it's <code>None</code>, we do nothing. 5. Check the value of the <code>activation</code> argument, and apply the corresponding activation function, or pass if <code>None</code> (see the table above for more info). 6. Unpack the modules list into a <code>torch.nn.Sequential</code> and store it in <code>self.conv</code>. 7. Finally, store the value of the <code>use_dropout</code> argument and set <code>self.dropout</code> to <code>torch.nn.Dropout</code> with a probability of 50%.</p> <p>And that's it for setup, let's have a quick look at the <code>forward</code> function to see how we're using the modules we just assembled.</p>"},{"location":"api/models/unet_blocks/#unetblockforward","title":"<code>UnetBlock.forward()</code>","text":"<p>The forward function for the <code>UnetBlock</code> class is super simple. </p> <p>It takes a <code>torch.Tensor</code>, <code>x</code>, as input, generally intended to be an input image from a training dataset as a tensor. All we do is run the <code>torch.nn.Sequential</code> we created in the <code>__init__</code> method, <code>self.conv</code>, and feed it the input tensor <code>x</code>. Then assign the output back to <code>x</code>. </p> <p>Then we check if we are using dropout for the current layer and, if so, apply dropout to the <code>x</code> tensor, then return it. Otherwise we just return it directly.</p>"},{"location":"api/models/unet_blocks/#residualunetblock","title":"ResidualUnetBlock","text":"<p>Reference: <code>nectargan.models.unet.blocks.ResidualUnetBlock</code></p> <p>The <code>ResidualUnetBlock</code> is very similar to the <code>UnetBlock</code>. In fact, it actually inherits from the <code>UnetBlock</code> class, and just uses its <code>self.conv</code> directly for upsampling and downsampling. The only difference is that this block also includes a residual connection.</p> <p>You can think of these like the skip connections we discussed in the previous document, only \"shorter\". Where the skip connections in the UNet architecture skip over all layers deeper than themselves, these residual connections skip only over the current layer's convolution or transposed convolution operation. They instead apply the same spatial transform operation, but with a <code>1x1</code> kernel, to match the main path's tensor shape, or just return a <code>torch.nn.Identity</code> in cases where the input and output channel counts are the same. This is often the case at the bottleneck, and is always the case once the generator's feature cap has been reached.</p> <p>Residual connections are used for different things in different architectures. ResNet, the origin of residual connections in deep learning, primarily uses them to stabilize training, and to help avoid the vanishing gradient problem. ResNets are extremely deep networks, with popular variants having 34 and 50 layers, and with some even having upwards of 150. This presents a challenge, as the large number of sequential activation functions causes the backpropagation gradients to shrink to effectively nothing. This, in turn, causes the shallower layers in the network to learn at a slower and slower rate, until they eventually stop learning altogether. The residual connections in ResNet provide an alternate route for gradients to flow which avoids this shrinkage.</p> <p>Here though, their task is more simple. They basically are serving exactly as the skip connections in the UNet are, except at a more \"local\" scale. The intent is the same though: to preserve sharp or fine details which might otherwise be lost during the encoding/decoding process.</p>"},{"location":"api/scheduling/integrations/","title":"NectarGAN API (Scheduling) - Integrations","text":"<p><code>NectarGAN API - Home</code> <code>NectarGAN API - Scheduling</code></p>"},{"location":"api/scheduling/integrations/#the-schedule-system-is-deeply-integrated-with-other-core-components-of-the-nectargan-api-the-ways-in-which-it-interacts-with-those-components-are-detailed-here","title":"The <code>Schedule</code> system is deeply integrated with other core components of the NectarGAN API. The ways in which it interacts with those components are detailed here.","text":"<p>[!NOTE] Currently, the main <code>Schedule</code> integration is with the <code>LossManager</code>. This document has been left open-ended though, so that it can be expanded in the future as more integrations are built out.</p> <p>[!WARNING] It is strongly encouraged to read the other scheduling related documentation before this one. While an effort will be made in this document to thoroughly explain all relevant concepts, much of it would be outside the scope of this document alone, and many concepts discussed here may be confusing without the prior context which those documents offer.</p> <p>It is also encouraged to read the documentation relevant to the API component which is being discussing, which will be linked at the top of each section.</p>"},{"location":"api/scheduling/integrations/#schedules-and-the-lossmanager","title":"Schedules and the LossManager","text":"<p>Reference <code>nectargan.losses.loss_manager.LossManager</code> <code>docs/api/losses/lossmanager</code></p> <p>The <code>LossManager</code> is deeply integrated with the <code>Schedule</code> system, allowing you to easily assign independent weight schedules to any loss registered with a given <code>LossManager</code> instance.</p> <p>[!NOTE] Peeling back the curtain just a little bit, I didn't originally intend for the scheduling system to have the level of complexity that it ended up having. Originally, the <code>Scheduler</code> was just intended to be a simple helper class for the loss manager to allow for scheduling of loss weights, since PyTorch already offers a robust and simple to use learning rate scheduler (<code>torch.optim.lr_scheduler.LRScheduler</code>).</p> <p>However, then I built it, and I thought it would be cool to be able to use drop-in functions to define the schedules. Then once that was working, I started thinking it might also be cool to be able to use those schedule functions to drive the native PyToch <code>LRScheduler</code>. Things just kinda kept growing, and that's how we ended up here. There are still some mentions of the <code>LossManager</code> and its related components in docstrings and comments and whatnot in the scheduling components, though, and this is why.</p>"},{"location":"api/scheduling/integrations/#to-understand-how-this-interaction-works-we-first-need-to-break-it-down-in-to-parts","title":"To understand how this interaction works, we first need to break it down in to parts.","text":"<p>Let's start by having a quick look as the <code>LMLoss</code>. Looking through the member variables, we can see that one of them is a <code>Schedule</code> object called <code>schedule</code>. We can also see that at init, the <code>schedule</code> variable, if it is not overridden by the declaration, is assigned a default factory <code>Schedule</code>.</p> <p>Next, let's have a look at the <code>LMLoss</code>'s <code>__post_init__</code> method:</p> <pre><code>def __post_init__(self) -&gt; None:\n    '''Post-init function for `LMLoss`.\n\n    This function will:\n        A.) Set the starting value of `self.schedule`'s `current_value` to \n            `self.loss_weight` if the user didn't initialize the `Schedule` \n            with their own values.\n\n            --------------------------- Or: ---------------------------\n\n        B.) Set `self.current_weight` to `self.schedule.initial_value` if \n            they did.\n\n    This is done so that the current weights of all registered `LMLoss` \n    objects can be applied in the same way regardless of whether scheduling \n    is being used for the loss. \n    '''\n    if self.schedule == Schedule():\n        self.schedule.current_value = self.loss_weight\n    else: self.loss_weight = self.schedule.initial_value   \n</code></pre>"},{"location":"api/scheduling/integrations/#okay-pretty-simple-in-theory-lets-see-how-it-functions-though","title":"Okay, pretty simple in theory. Let's see how it functions, though.","text":"<p>To do that, we will head over <code>LossManager</code>, and look for the member function called <code>_weight_loss()</code>, which is called during <code>LossManager.compute_loss_xy()</code>, just after the given loss function was run, to apply weighting to the loss result before it is returned:</p> <p>Let's now walk through step by step to see what exactly it's doing: 1. First, we extract the <code>Schedule</code> from the input <code>LMLoss</code>. 2. We then do a check to see if the <code>Schedule</code> is a default <code>Schedule</code> (i.e. the user did not pass a custom schedule as an argument when creating the given <code>LMLoss</code>).     - If is is a default schedule, we just get the current value from the schedule (because remember, looking back at the <code>LMLoss</code>'s <code>__post_init__</code> method, if the <code>Schedule</code> that the <code>LMLoss</code> was initialized with was a default <code>Schedule</code>, we simply assign the <code>LMLoss</code>'s <code>loss_weight</code> to the <code>Schedule</code>'s <code>current_value</code>), multiply the input <code>loss_value</code> by this current value, and return it.     - If the <code>LMLoss</code> does not have a default schedule, however, we instead: 3. Retrieve the schedule function from the <code>Schedule</code>. 4. Then we check to see if the <code>Schedule</code>'s schedule definition is a <code>Callable</code> (i.e. the <code>Schedule</code> was created with a custom schedule function)     - If it is a <code>Callable</code>, we just run the function directory, passing it the <code>Schedule</code> object reference, the current epoch value, and any <code>weight_kwargs</code> which were passed through, and assigning the return value to the <code>Schedule</code>'s <code>current_weight</code>.     - Otherwise we: 5. Check if the schedule definition is a <code>str</code> (i.e. one of the default schedules (<code>linear</code>, <code>exponential</code>)):     - If it is, we also check to make sure it is a valid key based on the <code>schedule_map</code>. If the key is valid, we run the related function and assign the return value to the <code>Schedule</code>'s <code>current_weight</code>.      - If it's invalid, we raise a <code>KeyError</code>. 6. Finally, we take the input <code>loss_value</code>, multiply it by the <code>Schedule</code>'s <code>current_weight</code> which we just calculated via one of the above methods, and return the result.</p> <p>So, based on this, we can see that the <code>Schedule</code> is pretty integral to how the <code>LossManager</code> actually manages the losses that are registered with it. Regardless of whether a given loss is scheduled, the LossManager uses the internal <code>Schedule</code> object of each <code>LMLoss</code> registered with it to track and apply weight values every time the given loss function is called.</p>"},{"location":"api/scheduling/integrations/#from-nectarganlossesloss_managerlossmanager","title":"From <code>nectargan.losses.loss_manager.LossManager</code>","text":"<p>```python def _weight_loss(         self,          loss_entry: LMLoss,         loss_value: torch.Tensor,         epoch: int,         **weight_kwargs: Any     ) -&gt; torch.Tensor:     '''Applies weighting to a loss value from an LMLoss object definition.</p> <pre><code>Args:\n    loss_entry : The LMLoss object which defines the weighting.\n    loss_value : The loss value to apply the weighting to.\n    epoch : The epoch that the loss value was calculated during.\n'''\ns = loss_entry.schedule # Get LMLossSchedule\n# If the loss isn't scheduled, just apply weight and return\nif s == Schedule(): return loss_value * s.current_value\n\n# Otherwise get and apply currently scheduled weights\nfn = s.schedule # Schedule function definition\nif isinstance(fn, Callable): s.current_value = fn(s, epoch)\nelif isinstance(fn, str) and fn in schedule_map.keys():\n    s.current_value = schedule_map[fn](s, epoch, **weight_kwargs)\nelse: \n    message = (\n        f'Invalid schedule type: {type(fn)}: ({fn})\\n'\n        f'Valid types are: Literal[\"linear\"] | '\n        f'Callable[[Schedule, int], None]')\n    raise TypeError(message)\nreturn loss_value * s.current_value\n</code></pre> <p><code>`` **Alright so looking at the function, we can see it takes as input:** - An</code>LMLoss<code>object, the current loss which is being run from a call to</code>LossManager.compute_loss_xy()<code>. - A</code>torch.Tensor<code>representing the returned result of the given loss function to apply weighting to. - An</code>integer<code>representing the current epoch at the time the loss is called. This is used to sample the scheduling function (please see [here](../losses/loss_functions.md) for more information). - Optional kwargs for the schedule function, passed to</code>compute_loss_xy<code>as a dict of</code>loss_kwargs<code>. The</code>weight_kwargs<code>dict is extracted from</code>loss_kwargs<code>, unpacked, and passed to</code>_weight_losses`.</p>"},{"location":"api/scheduling/integrations/#in-conclusion-lets-quickly-look-at-how-we-can-create-an-lmloss-with-a-weight-schedule","title":"In conclusion, let's quickly look at how we can create an <code>LMLoss</code> with a weight schedule:","text":"<p>An <code>LMLoss</code> object can be created with a <code>Schedule</code> as shown here:</p> <pre><code>import torch.nn as nn\nfrom nectargan.losses.lm_data import LMLoss\nfrom nectargan.scheduling.data import Schedule\n\nL1 = nn.L1Loss()\n\nschedule = Schedule(\n    schedule='linear',   # Linear decay\n    start_epoch=100,     # Starting at epoch 100\n    end_epoch=200,       # And ending at epoch 200\n    initial_value=100.0, # With an initial value of 100.0\n    target_value=0.0     # And a target value of 0.0\n)\n\nmy_loss = LMLoss(        # Create LMLoss\n    name='my_loss',      # With name 'my_loss'\n    function=L1,         # 'my_loss' is native PyTorch L1\n    schedule=schedule,   # Assign our weight schedule\n    tags=['my_tag']      # And, optionally, lookup tags\n)\n</code></pre> <p>An <code>LMLoss</code> can also be directly registered with the <code>LossManager</code> with a weight schedule as follows:</p> <pre><code>import torch.nn as nn\nfrom nectargan.losses.loss_manager import LossManager\nfrom nectargan.config.config_manager import ConfigManager\nfrom nectargan.scheduling.data import Schedule\n\nL1 = nn.L1Loss()\n\n# Dummy config for LossManager\nconfig_manager = ConfigManager('/path/to/config.json')   \nloss_mananger = LossManager( # Create LossManager                             \n    config=config_manager.data,\n    experiment_directory='/path/to/experiment/directory')\n\nschedule = Schedule( # Define Schedule\n    schedule='linear',\n    start_epoch=100,\n    end_epoch=200,\n    initial_value=100.0,\n    target_value=0.0)\n\n# Then register the loss with the LossManager\nloss_mananger.register_loss_fn(\n    name='my_loss',\n    loss_fn=L1,\n    schedule=schedule,\n    tags=['my_tag'])\n</code></pre>"},{"location":"api/scheduling/schedule_dataclass/","title":"NectarGAN API (Scheduling) - Schedule Dataclass","text":"<p><code>NectarGAN API - Home</code> <code>NectarGAN API - Scheduling</code></p>"},{"location":"api/scheduling/schedule_dataclass/#in-the-nectargan-scheduling-system-schedules-are-defined-by-schedule-objects-small-drop-in-dataclass-instances-which-define-everything-about-a-given-schedule-and-which-can-be-fed-to-either-scheduler-class-to-define-scheduling-for-any-parameter","title":"In the NectarGAN scheduling system, schedules are defined by <code>Schedule</code> objects, small drop-in dataclass instances which define everything about a given schedule, and which can be fed to either <code>Scheduler</code> class to define scheduling for any parameter.","text":"<p>Reference: <code>nectargan.scheduling.data</code></p>"},{"location":"api/scheduling/schedule_dataclass/#variables","title":"Variables","text":"Variable Description <code>schedule</code> Either the name of a built in scheduling function as a string, or a custom schedule function. <code>start_epoch</code> Epoch to start increasing or decreasing the loss values. <code>end_epoch</code> Epoch to stop increasing or decreasing the loss values. <code>initial_value</code> The value to use until start_epoch. <code>target_value</code> The value to interpolate to at, and hold after, end_epoch. <code>current_value</code> Not meant to be set directly, this value is instead set by the <code>Schedule</code>'s <code>__post_init__</code> function. It is primarily used by the <code>LossManager</code> to weight the return value of registered losses when they are run. &gt; [!NOTE] &gt; When a <code>Schedule</code> instance is first created, it the default values are left unchanged, it will be created with a schedule which doesn't really do anything. Were the default schedule actually used, it would start at 1.0, do nothing for 10,000,000 epochs, and end at 1.0. ### Creating a <code>Schedule</code> instance The following is an example of how you can create a <code>Schedule</code> instance using one of the default schedule functions, <code>linear</code>: <pre><code>from nectargan.scheduling.data import Schedule\n\nschedule = Schedule(\n    schedule='linear',\n    initial_value=100.0,\n    start_epoch=100,\n    target_value=0.0,\n    end_epoch=200 \n)\n</code></pre> <p>This will create a schedule whereby the value is held constant at <code>100.0</code> for the first 100 epochs. Then, over the next hundred epochs (i.e. epoch 100-200), it will decay linearly to the target value of <code>0.0</code>.</p> <p>It is also possible to define your own schedule functions (see here for more info). A <code>Schedule</code> object can be initialized with a custom schedule function like so. First, we will create our schedule function:</p> <pre><code>from nectargan.scheduling.data import Schedule\n\ndef my_schedule(schedule: Schedule, epoch: int) -&gt; float:\n    # Define schedule\n    return output\n\nschedule = Schedule(\n    schedule=my_schedule,\n    initial_value=100.0,\n    start_epoch=100,\n    target_value=0.0,\n    end_epoch=200 \n)\n</code></pre> <p>Here, we define the <code>Schedule</code> the same way we did previously, except, rather than passing it the name of one of the default schedules, we instead directly pass it a reference to the <code>Callable</code> which defines our schedule.</p>"},{"location":"api/scheduling/schedule_dataclass/#methods","title":"Methods","text":""},{"location":"api/scheduling/schedule_dataclass/#schedule_check_for_defaults","title":"<code>Schedule._check_for_defaults()</code>","text":"<pre><code>Used in post_init to check if all args are at their default values.\n\nReturns:\n    bool : True is all arguments are at default, otherwise False.\n</code></pre>"},{"location":"api/scheduling/schedule_dataclass/#schedule__post_init__","title":"<code>Schedule.__post_init__()</code>","text":"<pre><code>Post-init function for `Schedule` dataclass.\n\nThis function is used to set the starting value of the Schedule's\n`current_value` to its `initial_value` if the user initialized the\nSchedule with their own values (i.e. not all default).\n\nIt will also subtract 1 from `start_epoch` and `end_epoch` in that case\nto compensate for the fact that the Trainer classes treat epoch as \nthough it was indexed from 1 rather than 0.\n</code></pre>"},{"location":"api/scheduling/schedule_dataclass/#schedule__eq__","title":"<code>Schedule.__eq__()</code>","text":"<pre><code>Checks if `start_epoch` and `end_epoch` are at their default values.\n\nSince it's relatively safe to assume that no one is going to be \ntraining a model for 10,000,000 epochs([0, 1e07]), this basically \nserves as a check to see if the Schedule instance is being used for the \nparent object. This frees us up to initialize weight values when the \nparent object is first instantiated, since those are no longer checked\nwhen performing an __eq__ operation.\n\nArgs:\n    value : The other Schedule object which is being compared.\n\nReturns:\n    bool : True if start_epoch and end_epoch match, otherwise false.\n</code></pre>"},{"location":"api/scheduling/schedule_functions/","title":"NectarGAN API (Scheduling) - Schedule Functions","text":"<p><code>NectarGAN API - Home</code> <code>NectarGAN API - Scheduling</code></p>"},{"location":"api/scheduling/schedule_functions/#schedule-functions-allow-you-to-define-simple-mathematical-functions-which-can-then-by-plugged-in-to-the-schedule-dataclass-and-used-to-drive-scheduling-for-any-parameter-in-your-model","title":"Schedule functions allow you to define simple mathematical functions, which can then by plugged in to the <code>Schedule</code> dataclass and used to drive scheduling for any parameter in your model.","text":"<p>Reference: <code>nectargan.scheduling.schedules</code></p> <p>These are easy to use, but a little complex to understand at first, so let's start slow with...</p>"},{"location":"api/scheduling/schedule_functions/#what-is-a-schedule-function","title":"What is a Schedule function?","text":"<p>Just a simple Python function which itself describes a simple time-dependent mathematical function. A schedule function has two things passed to it as input arguments any time it is evaluated:</p> <ol> <li>An epoch value. This is used to define the sample point for the function.</li> <li>A <code>Schedule</code> object (see here for more info).</li> </ol> <p>Schedule functions can be broken down into a few parts: 1. An initial value. 2. A target value. 3. A start epoch, until which the initial value will be returned. 4. An end epoch, after which the target value will be returned.</p> <p>So, up until the input epoch passed to the schedule function is equal to the start epoch, the schedule function returns the initial value. Then, in the epochs between the start and end epoch, the schedule function interpolates from the initial value to the target value. And finally, in the epochs after the end epoch, the function returns the target value.</p>"},{"location":"api/scheduling/schedule_functions/#lets-have-a-look-at-an-example-schedule-to-help-clarify-things","title":"Let's have a look at an example schedule to help clarify things:","text":"<p>Alright, not too complicated. Looking at the function, we can see that we are: 1. Getting our initial and target values from the input <code>Schedule</code> object. 2. Calculating a sample position, where the <code>sample</code> value is <code>0.0</code> at the <code>schedule.start_epoch</code>, then interpolates linearly to <code>1.0</code> over the time between <code>schedule.start_epoch</code> and <code>schedule.end_epoch</code>. 3. Then we clamp the <code>sample</code> value to the (0, 1) range. 4. Next, we use that sample value to sample a linear growth/decay function, calculated as <code>initial_value + sample * (target_value - initial_value)</code> (see graph). 5. Get the lowest allowed value (note that the <code>linear</code> schedule function is clamped to a min of <code>0.0</code>) and the highest allowed value. 6. Return the minimum value if value &lt; minimum, the maximum value if value &gt; maximum, or the value itself if neither is true.</p> <p>Pretty simple, right? The other standard decay schedule, <code>exponential</code>, works in much the same way, with only a couple small differences. Since this function uses logorithmic interpolation, which can result in a <code>ZeroDivisionError</code>, it provides a couple extra variables: | Variable | Description | | :---: | --- | <code>allow_zero_weights</code> | If <code>True</code>, the function will allow a value of <code>0.0</code> for initial or target weight, but will add a small epsilon value to the <code>0.0</code> value to avoid the <code>ZeroDivisionError</code>. <code>epsilon</code> | The epsilon value to add if either initial or target weight is <code>0.0</code>. <code>silent</code> | If <code>allow_zero_weights</code> if <code>False</code> and this value is also <code>False</code>, this funtion will raise a <code>ZeroDivisionError</code>. If <code>allow_zero_weights</code> is <code>False</code> and this is <code>True</code>, however, rather than raising an exception, it will instead return the initial value until the <code>schedule.end_epoch</code>, at which point it will instead return the target value, effectively allowing it to fail silently.</p>"},{"location":"api/scheduling/schedule_functions/#from-nectarganschedulingschedulesscheduledefs","title":"From: <code>nectargan.scheduling.schedules.ScheduleDefs</code>","text":"<p>```python def linear(schedule: Schedule, epoch: int) -&gt; float:         '''Defines a linear loss weight schedule.</p> <pre><code>    Graph:\n    - https://www.desmos.com/calculator/xaponwctch\n    - e1, e2 : start, end epoch\n    - v1, v2 : start, end value\n\n    Args:\n        schedule : Schedule object to use when computing the new weight.\n        epoch : Current epoch that the time this function is called.\n    '''\n    initial, target = schedule.initial_value, schedule.target_value\n\n    # Normalized sample position from current epoch\n    sample = ((float(epoch) - float(schedule.start_epoch)) / \n              (float(schedule.end_epoch) - float(schedule.start_epoch)))\n    sample = max(0.0, min(1.0, sample)) # Clamp value [0.0, 1.0]\n\n    # Sample function at that position\n    value = initial + sample * (target - initial)\n\n    # Get largest and smallest of weight values\n    lowest = max(0.0, min(initial, target))\n    highest = max(initial, target)\n\n    # Return the current weight value.\n    return min(highest, max(lowest, value))\n</code></pre> <p>``` Clickable graph link: https://www.desmos.com/calculator/xaponwctch</p>"},{"location":"api/scheduling/schedulers/","title":"NectarGAN API (Scheduling) - Schedulers","text":"<p><code>NectarGAN API - Home</code> <code>NectarGAN API - Scheduling</code></p>"},{"location":"api/scheduling/schedulers/#the-nectargan-api-provides-two-classes-for-managing-schedules-depending-on-the-objective-these-are","title":"The NectarGAN API provides two classes for managing schedules, depending on the objective. These are:","text":""},{"location":"api/scheduling/schedulers/#the-scheduler-class","title":"The Scheduler class","text":"<p>The <code>Scheduler</code> is a wrapper class to simplify the process of interacting with <code>Schedule</code> objects.</p>"},{"location":"api/scheduling/schedulers/#creating-a-scheduler-instance","title":"Creating a Scheduler instance","text":"<p>A <code>Scheduler</code> can be created as follows. First, we will create a <code>Schedule</code> object:</p> <pre><code>from nectargan.scheduling.data import Schedule\n\nschedule = Schedule(\n    schedule='linear',   # Define a linear decay schedule\n    initial_value=100.0, # which starts at a value of 100.0,\n    start_epoch=100,     # and which begins decaying at epoch 100,\n    target_value=0.0,    # until finishing the decay at a value of 0.0\n    end_epoch=200        # at epoch 200\n)\n</code></pre> <p>[!NOTE] This schedule should look familiar if you have read the original Pix2pix paper, as this <code>Schedule</code> object defines a schedule which mirrors the one they used for the learning rate in their original implementation.</p> <p>Then, we can use our <code>Schedule</code> object to initialize a <code>Scheduler</code> like so:</p> <pre><code>from nectargan.scheduling.scheduler import Scheduler\n\nscheduler = Scheduler(schedule)\n</code></pre> <p>Pretty simple. Behind the scenes, this will do some quick schedule validation. Then, assuming the current schedule is valid, we're ready to use it. This is also very simple, although there is some pre-requisite knowledge which is required to better understand how we use <code>Schedules</code> in NectarGAN, so you are encouraged to quickly read through the schedule function documentation before continuing here.</p> <p>Now that we have created a <code>Scheduler</code> instance and assigned it our linear decay <code>Schedule</code>, we can evaluate our schedule function with <code>Scheduler.eval_schedule()</code> as follows:</p> <pre><code>for epoch in range(200):\n    current_value = scheduler.eval_schedule(epoch)\n    print(f'Epoch: {epoch+1} - Value: {current_value}')\n</code></pre>"},{"location":"api/scheduling/schedulers/#result","title":"Result","text":"<pre><code>Epoch: 1 - Value: 1.0\nEpoch: 2 - Value: 1.0\n[...]\nEpoch: 99 - Value: 1.0\nEpoch: 100 - Value: 1.0\nEpoch: 101 - Value: 0.99\nEpoch: 102 - Value: 0.98\nEpoch: 103 - Value: 0.97\n[...]\nEpoch: 198 - Value: 0.02\nEpoch: 199 - Value: 0.01\nEpoch: 200 - Value: 0.0\n</code></pre>"},{"location":"api/scheduling/schedulers/#the-torchscheduler-class","title":"The TorchScheduler class","text":"<p>The <code>TorchScheduler</code> is a compatibility wrapper around the native <code>torch.optim.lr_scheduler</code>, allowing you to use scheduling functions interchangeably while retaining the native scheduler's deep integration with the native optimizers.</p> <p>A <code>TorchScheduler</code> instance can be created as follows. First, we create a schedule in the same way we did for the base <code>Scheduler</code>:</p> <pre><code>from nectargan.scheduling.data import Schedule\n\nschedule = Schedule(\n    schedule='linear',   # Define a linear decay schedule\n    initial_value=100.0, # which starts at a value of 100.0,\n    start_epoch=100,     # and which begins decaying at epoch 100,\n    target_value=0.0,    # until finishing the decay at a value of 0.0\n    end_epoch=200        # at epoch 200\n)\n</code></pre> <p>For the <code>TorchScheduler</code>, however, we also need to pass it an optimizer. This should be the optimizer that is linked to the network you are trying to schedule the learning rate for. Then, we can can create a <code>TorchScheduler</code> instance like so:</p> <pre><code>from torch.optim import Adam\nfrom nectargan.scheduling.scheduler_torch import TorchScheduler\noptimizer = Adam(params, initial_lr, betas) # Should be the real optimizer of the net to schedule\n\nscheduler = TorchScheduler( # Create a TorchScheduler\n    optimizer=optimizer,    # and pass it the optimizer\n    schedule=schedule       # and the schedule\n)\n</code></pre> <p>Then you can just call the <code>TorchScheduler.step()</code> wrapper at the end of each epoch to update the learning rate as follows:</p> <pre><code>scheduler.step() # Simple wrapper around torch.optim.lr_scheduler.LRScheduler.step()\n</code></pre>"},{"location":"api/scheduling/schedulers/#get_lr-utility","title":"<code>get_lr()</code> Utility","text":"<p>The <code>TorchScheduler</code> also provides a utility function called <code>get_lr</code>, which is intended to be called after <code>TorchScheduler.step()</code>, and will return a tuple of the previous learning rate, from before the most recent <code>step()</code> call, and the current learning rate, after it has been updated by <code>step()</code></p>"},{"location":"api/trainers/building_a_trainer/","title":"NectarGAN API - Building a Trainer","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/trainers/building_a_trainer/#the-base-trainer-class-in-the-nectargan-api-is-intended-to-be-inherited-from-to-allow-for-the-easy-creation-of-new-trainer-subclasses-for-models-with-different-requirements","title":"The base <code>Trainer</code> class in the NectarGAN API is intended to be inherited from to allow for the easy creation of new <code>Trainer</code> subclasses for models with different requirements.","text":""},{"location":"api/trainers/building_a_trainer/#strap-in-this-one-is-dense","title":"Strap in, this one is dense :)","text":"<p>[!NOTE] Currently, the <code>Trainer</code> class, and the API as a whole, are intended specifically for paired adversarial training (Pix2pix-style[^1]). This is planned to be expanded to unpaired training in the future but it is worth noting that in it's current state, the framework is mostly suited for paired image translation models.</p> <p>[!IMPORTANT] This section will largely build upon topics addressed in the documentation for the base <code>Trainer</code> class. It is advisable to read that before document before beginning this one.</p>"},{"location":"api/trainers/building_a_trainer/#the-pix2pixtrainer-class","title":"The <code>Pix2pixTrainer</code> class","text":"<p>The <code>Pix2pixTrainer</code> serves as an example implementation of a <code>Trainer</code> subclass, a Pix2pix-style[^1] GAN model in this case.</p> <p>Here, we will walk step-by-step though the implementation, to better understand how it works and how we can implement trainers for other models in the same style.</p>"},{"location":"api/trainers/building_a_trainer/#the-pix2pixtrainers-__init__-function","title":"The Pix2pixTrainer's <code>__init__()</code> Function","text":"<p>Starting with our input arguments for  <code>Pix2pixTrainer.__init__()</code>, we can see that, like the base <code>Trainer</code> class, it requires some sort of config object. Makes sense, we need to pass it to the parent <code>Trainer</code> for its init function. We can also see that it takes a <code>log_losses</code> argument for the same reason. It also takes a <code>Literal</code> string called <code>loss_subspec</code>. This is expanded upon in much greater detail here but in short, this tells the <code>Pix2pixTrainer</code>'s internal <code>LossManager</code> what loss functions to register when it is initialized.</p> <p>Then, the first thing we do is initialize the parent <code>Trainer</code> class, passing it the input <code>config</code> and the <code>log_losses</code> boolean. We also pass it a value of <code>True</code> for <code>quicksetup</code>, telling the <code>Trainer</code> that we would like it to automatically build an experiment output directory and export a copy of the config as a <code>.json</code> to it, a <code>LossManager</code>, and a Visdom client, if applicable based on the current config.</p> <p>After that, we run a sequence of init functions. In order, these are: | Function | Description | | :---: | --- | <code>_init_lr_scheduling</code> | Checks the value of <code>separate_lr_schedules</code> in the current config. If it is <code>True</code>, it does nothing. If it is <code>False</code>, however, it will override the discriminator's learning rate with the value of the generator's learning rate, allowing learning rates for the discriminator to be accessed the same way, regardless of whether it is using a separate schedule. <code>_init_generator</code> | Initializes a <code>UnetGenerator</code> network, and a <code>torch.nn.optim.Adam</code> optimizer for the network. <code>_init_disriminator</code> | Initializes a PatchGAN-style <code>Discriminator</code> network, and a <code>torch.nn.optim.Adam</code> optimizer for the network. <code>_init_dataloaders</code> | Initializes one <code>torch.nn.data.DataLoader</code> each for the <code>train</code> and <code>val</code> dataset. <code>_init_gradscalers</code> | Initializes a gradient scaler for both G and D based on the device in the current config.</p> <p>Once those are completed, we initialize our losses with the <code>Trainer</code>'s <code>LossManager</code>. This is done here by running the function <code>Pix2pixTrainer._init_losses()</code>. You are encouraged to read the docstring for this function as it is extremely thorough, but in short, it takes the <code>loss_subspec</code> argument which was passed to the <code>Pix2pixTrainer</code> at init, validates it against a list of supported subspecs, and then passes that along with the input <code>config</code> and the a reference to the NectarGAN (see here and here) to the <code>init_from_spec()</code> function of the <code>Trainer</code>'s internal <code>LossManager</code>. This registers all the losses required loss functions for training.</p> <p>Finally, we do a quick check of the current config to see if we are continuing a train on a pre-existing model. If we are, we run the <code>load_checkpoint()</code> function from the parent <code>Trainer</code> class to load the weights for both the generator and the disriminator.</p> <p>And that's it really, our <code>Trainer</code> subclass is basically ready for training, save for one small thing...</p>"},{"location":"api/trainers/building_a_trainer/#training-callbacks","title":"Training Callbacks","text":""},{"location":"api/trainers/building_a_trainer/#at-the-heart-of-how-the-trainer-class-and-its-subclasses-function-are-the-three-main-training-loop-override-methods","title":"At the heart of how the <code>Trainer</code> class and its subclasses function are the three main training loop override methods.","text":"<p>Here are the three methods with a short description:</p> <p>Looking at each of the three functions, a few things stand out:</p> <ol> <li>In their implementation in the base <code>Trainer</code> class, all they do is raise an exception. This means that any <code>Trainer</code> subclass is expected to override all three methods, otherwise they will raise an exception during the training loop.</li> <li><code>on_epoch_start</code> and <code>on_epoch_end</code> have no required arguments apart from a non-descript dict of <code>kwargs</code>. This indicates here that they don't do anything on their own, and that they don't have any required functionality in a <code>Trainer</code> subclass. We will touch on this more in a second.</li> <li><code>train_step</code> is different. It does still accept the <code>kwargs</code> dict like the other two, but it also has 3 required arguments, all of which are passed to it during the training loop in <code>Trainer.train_paired()</code> -&gt; <code>Trainer._train_paired_core()</code>. These are:<ul> <li><code>x</code>: A <code>torch.Tensor</code> representing the input(s) for the current batch.</li> <li><code>y</code>: A <code>torch.Tensor</code> representing the ground truth(s) for the current batch.</li> <li><code>idx</code> : An integer representing the current train loop interation at the time the function was called, indexed from zero.</li> </ul> </li> </ol> <p>Now let's have a look at the function that executes all three of the callbacks, <code>Trainer.train_paired()</code>, to better contextualize how they function. </p> <p>Looking over the function, we first notice that it takes a slightly strange set of input arguments. Let's start by going through each one to see what it does: | Argument | Description | | :---: | --- | <code>epoch</code> | The iteration value of the training loop at the time this function is called. Used to set the <code>Trainer</code>'s <code>current_epoch</code> value (i.e. <code>Trainer.current_epoch == 1+epoch</code>) <code>on_epoch_start</code> | A <code>Callable</code>. Run once, right before the training loop begins. <code>train_step</code> | A <code>Callable</code>. Run once per batch in the dataset. <code>on_epoch_end</code> | A <code>Callable</code>. Run once, after all batches have been completed. <code>multithreaded</code> | If True (default), this function will start a new thread to update the Visdom visualizers. If False, it will update them in the same thread used for training. <code>callback_kwargs</code> | A dict[str, dict[str, Any]] with any keyword arguments you would like to pass to the callback functions during training. kwargs are parsed internally and passed to their repective callbacks. The dict should be formatted as follows:callback_kwargs = {\u00a0\u00a0\u00a0\u00a0'on_epoch_start': { 'var1': 1.0 },\u00a0\u00a0\u00a0\u00a0'train_step': { 'var2': True, 'var3': [1.0, 2.0] },\u00a0\u00a0\u00a0\u00a0'on_epoch_end': {'var4': { 'x': 1.0, 'y': 2.0 } }}See <code>Pix2pixTrainer.on_epoch_start()</code> for example implementation.</p> <p>Looking at these arguments, we can see that three of them have the same names as our training callbacks. If we quickly jump down to where we initialize our training functions, we can see that for each of the training loop functions (<code>start_fn</code>, <code>train_fn</code>, <code>end_fn</code>), we first check to see if the input argument for that value is <code>None</code>. If it's not, we use the <code>Callable</code> that was passed as an input to <code>Trainer.train_paired()</code>, but if it is (default), we instead use the corresponding callback function. This allows you to technically bypass the callback override methods altogether. More realistically, though, this can be used to quickly drop in a function for one of the steps to test a change non-destructively.</p> <p>Next, let's have a quick look at the <code>callback_kwargs</code>. As we touched on previously, all three callback methods take a dict of kwargs. These kwargs are first passed to <code>Trainer.train_paired()</code> as one large dict, formatted as shown in the table above, where each callback is a subdict in the main input <code>callback_kwargs</code> dict, whos key is the name of the function, and who's values can be... well, whatever you want really. Let's have a quick look at how this is used. We will use the core paired training script and the <code>on_epoch_start</code> method from the <code>Pix2pixTrainer</code> as an example:</p> <p>Looking at the training script, we can see that, when we called the <code>train_paired</code> function on our <code>Pix2pixTrainer</code>, we pass it a dictionary, structured as follows:</p> <pre><code>{ \n    'on_epoch_start': {\n        'print_train_start': True\n    } \n}\n</code></pre> <p>Now, let's have a look at how that dict is handled by the <code>Trainer</code> inside of the <code>train_paired</code> function when we pass it to the <code>callback_kwargs</code> argument:</p> <p>Stripping it down to just the relevant couple lines here, we can see that the <code>train_paired</code> first assigns the current <code>start_fn</code>, as described above. Then later on, it calls that function and, when doing so, gets the <code>on_epoch_start</code> dict from the input <code>callback_kwargs</code> dict (or an empty dict if there were no <code>on_epoch_start</code> dict), unpacks it into keyword arguments, and passes them as the <code>kwargs</code> argument. This same process is also done for <code>train_step</code> and <code>on_epoch_end</code>.</p> <p>Finally, let's have a brief look at how to use our <code>print_train_start</code> kwarg inside of our <code>on_epoch_start</code> callback. There are a couple ways to do this, so first, we'll see how it is done in the <code>Pix2pixTrainer</code>, and then we will see an alternative method, then we will discuss why the <code>Pix2pixTrainer</code> does not use this alternative method, despite them both being equally valid.</p> <p>First, the <code>Pix2pixTrainer</code>'s <code>on_epoch_start</code> callback:</p> <p>Now, an alternative method:</p> <pre><code>def on_epoch_start(self, print_train_start: bool) -&gt; None:\n    if print_train_start:\n        print(f'Beginning epoch: {self.current_epoch}')\n        self.loss_manager.print_weights()\n</code></pre> <p>We can see that the core functionality is exactly the same, they both print the same values based on the same input <code>print_train_start</code> conditional, but where the method used by the <code>Pix2pixTrainer</code> treats the incoming <code>kwargs</code> as a dict, this one takes advantage of the fact that the <code>Trainer</code>'s <code>train_paired</code> function unpacks the relevant subdict as it passes it to the callback to instead explicitly define the callback method's <code>print_train_start</code> argument.</p> <p>The <code>Pix2pixTrainer</code> chooses the first method in an effort to be more open-ended. That method makes the <code>print_train_start</code> argument effectively optional, whereas the second method would raise an exception if the kwarg was not passed when calling <code>train_paired</code>.</p> <p>Both approaches are totally valid though. Ultimately, they both acchieve the same goal.</p> <p>Lastly, let's have a look at arguably the most important of the three training callbacks, <code>train_step</code>.</p> <p>This method houses your core training function (i.e. forward and backward steps for G and D). It takes as input two <code>torch.Tensors</code>, <code>x</code>, the real input image(s) from the current batch of the dataset when the function is called, and <code>y</code>, the real ground truth image(s) from the current batch. It also takes an integer, representing the current training loop iteration at the time the function is called.</p> <p>Let's take a quick look at the <code>train_step</code> implementation in the <code>Pix2pixTrainer</code> to better understand exactly how it works.</p> <p>That is all the <code>train_step</code> callback is. Just a single step of the model's training loop.</p>"},{"location":"api/trainers/building_a_trainer/#from-docsapitrainerstrainermd","title":"From <code>docs/api/trainers/trainer.md</code>","text":"Function Description <code>on_epoch_start</code> Run at the very beginning of an epoch in <code>Trainer.train_paired()</code>, before the training loop begins. <code>train_step</code> Run once per batch in <code>Trainer.train_paired()</code> -&gt; <code>Trainer._train_paired_core()</code>. <code>on_epoch_end</code> Run at the very end of an epoch in <code>Trainer.train_paired()</code>, after all the batches for the epoch have been completed."},{"location":"api/trainers/building_a_trainer/#from-nectarganstarttrainingpairedpy","title":"From <code>/nectargan/start/training/paired.py</code>","text":"<p>```python trainer = Pix2pixTrainer(     config=args.config_file,      loss_subspec=args.loss_subspec,     log_losses=args.log_losses)</p> <p>for epoch in range(epoch_count):     trainer.train_paired( # Train generator and discriminator         epoch,          callback_kwargs={ 'on_epoch_start': {'print_train_start': True} })  ```</p>"},{"location":"api/trainers/building_a_trainer/#from-nectargantrainerstrainertrainertrain_paired","title":"From <code>nectargan.trainers.trainer.Trainer.train_paired</code>","text":"<p>```python [...]</p> <p>start_fn = on_epoch_start or self.on_epoch_start # Init pre-train fn</p> <p>[...]</p> <p>start_fn(**callback_kwargs.get('on_epoch_start', {})) </p> <p>[...] ```</p>"},{"location":"api/trainers/building_a_trainer/#from-nectargantrainerspix2pix_trainerpix2pixtrainer","title":"From <code>nectargan.trainers.pix2pix_trainer.Pix2pixTrainer</code>","text":"<p><code>python def on_epoch_start(self, **kwargs: Any) -&gt; None:     if 'print_train_start' in kwargs.items():         if kwargs['print_train_start']:             print(f'Beginning epoch: {self.current_epoch}')             self.loss_manager.print_weights()</code> Looking at our callback method, we can see that it treats the incoming <code>kwargs</code> argument as a dict. It first checks the items of the dict to see if <code>print_train_start</code> is present and, if it is, it then checks to see if the value is <code>True</code> or <code>False</code>. It it is <code>False</code>, nothing happens. If it is <code>True</code>, though, it will print a short string showing the index of the epoch which is about to begin, and then calls its <code>LossManager</code>'s <code>print_weights</code> to print all of the loss weights which will be used in the upcoming epoch.</p>"},{"location":"api/trainers/building_a_trainer/#from-nectargantrainerspix2pix_trainerpix2pixtrainer_1","title":"From <code>nectargan.trainers.pix2pix_trainer.Pix2pixTrainer</code>","text":"<p>```python def train_step(         self,          x: torch.Tensor,          y: torch.Tensor,          idx: int,         **kwargs: Any     ) -&gt; None:     with torch.amp.autocast('cuda'):          y_fake = self.gen(x)</p> <pre><code># Get discriminator losses, apply gradients\nlosses_D = self.forward_D(x, y, y_fake)\nself.backward_D(losses_D['loss_D'])\n\n# Get generator losses, apply gradients\nloss_G = self.forward_G(x, y, y_fake)\nself.backward_G(loss_G)\n\nif idx % self.config.visualizer.visdom.update_frequency == 0:\n    self.update_display(x, y, y_fake, idx)\n</code></pre> <p><code>`` Looking at our</code>train_step` method, we can see what looks like a single step of a very standard Pix2pix training loop. We run the generator's inference to create a y_fake, calculate and apply discriminator losses, then do the same for the generator. Then we have a small function to update the display (Visdom and console), if it is applicable this batch. </p>"},{"location":"api/trainers/building_a_trainer/#recap","title":"Recap","text":"<p>Alright, so we've seen our three different callback functions, and we've had a quick look at the function that invokes them. Now let's briefly recap each, and have a quick look at what is does, how and where exactly each is used in the base <code>Trainer</code>, and how the <code>Pix2pixTrainer</code> handles overriding each method, so that we can better understand how they can be used to quickly prototype training loops for our other models.</p>"},{"location":"api/trainers/building_a_trainer/#on_epoch_start","title":"<code>on_epoch_start</code>","text":"<p>Description: This callback is run once at the very beginning of the epoch, before the actual training loop is started.</p> <p>Use: The <code>Pix2pixTrainer</code> uses this method to print some useful data to the console (i.e. loss weights and current epoch value). </p>"},{"location":"api/trainers/building_a_trainer/#train_step","title":"<code>train_step</code>","text":"<p>Description: Run once per batch during each epoch, not directly by the <code>train_paired</code> function, but instead via the private <code>_train_paired_core</code></p> <p>Use: Forward and backward train step.</p>"},{"location":"api/trainers/building_a_trainer/#on_epoch_end","title":"<code>on_epoch_end</code>","text":"<p>Description: This callback is run once at the very end of the epoch, after all of the batches for that epoch have been completed.</p> <p>Use: The <code>Pix2pixTrainer</code> uses this method to dump cached loss values to the <code>.json</code> log and to update the models schedulers.</p> <p>[^1]: Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2017)</p>"},{"location":"api/trainers/trainer/","title":"NectarGAN API - Trainers","text":"<p><code>NectarGAN API - Home</code></p>"},{"location":"api/trainers/trainer/#the-trainer-class-is-one-of-the-core-components-of-the-nectargan-api","title":"The <code>Trainer</code> class is one of the core components of the NectarGAN API.","text":""},{"location":"api/trainers/trainer/#intro","title":"Intro","text":"<p>The NectarGAN API provides a base <code>Trainer</code> class which you can inherit from to create your own model-specific trainers. The base class provides a number of convenience functions to speed up the process of building and deploying new trainers. </p>"},{"location":"api/trainers/trainer/#initializing-a-trainer-subclass","title":"Initializing a <code>Trainer</code> (subclass)","text":"<p>First, let's have a quick look at the base <code>Trainer</code> <code>__init__()</code> function.</p> <p>We can see that, to initialize a <code>Trainer</code>, all that is required is a config. This will be used to initialize the <code>Trainer</code>'s internal <code>ConfigManager</code>, which is where all config information within the <code>Trainer</code> is pulled from. But what form that init config takes is up to you. Following is an explanation of what each input type is expecting:</p> <p>However, one thing to note is that, when initializing a <code>Trainer</code>, you may also pass it a pre-constructed <code>ConfigManager</code>. In this case, the <code>Trainer</code> will just replace its internal <code>ConfigManager</code> with the one it's passed.</p> <p>When a <code>Trainer</code> subclass is initialized, the base <code>Trainer</code> class will perform a series of setup functions: 1. It will initialize a number of core member variable, all of which will be discussed in more detail below. 2. Take the input config, whatever form that might take, and use it to initialize its internal ConfigManager. 3. Extract the selected device from the config's <code>config.common.device</code> and assign it to <code>self.device</code> since it's needed so frequently.</p> <p>If the trainer was initialized with <code>quicksetup=False</code>, then the <code>Trainer</code> init function ends here. However, if <code>quicksetup=True</code> (default), the <code>Trainer</code> init performs a few extra convenience steps. This will be expanded upon in a later section but, generally speaking, a <code>Trainer</code> will almost always be initialized with <code>quicksetup=True</code>. For now, though, these are the additional steps taken if <code>quicksetup=True</code>:</p> <ol> <li>The <code>Trainer</code> will build an output directory for the current experiment based on the output and experiment settings in the config. Or if <code>continue_train</code> is <code>True</code> in the <code>config</code> it was passed, it will instead just overwrite its <code>self.experiment_dir</code> with the path to the selected experiment to continue.</li> <li>It will then initialize a <code>LossManager</code> to manage all of the losses during training.</li> <li>After that, it will export a copy of its current config to the experiment directory.</li> <li>Then it will check the input <code>config</code> to see if <code>enable_visdom</code> is <code>True</code>. If it is, it will run <code>init_visdom()</code> to build and start a Visdom client. See here for more information.</li> </ol>"},{"location":"api/trainers/trainer/#from-docsapiconfigmd","title":"From <code>docs/api/config.md</code>","text":"Type Behaviour <code>str</code> This is expecting a string with the system path to a <code>.json</code> config file. It will attempt to parse the config file and assign the values, or return various exception types if it fails depending on the reason for the failure. <code>PathLike</code> The is expecting an <code>os.PathLike</code> object containing the system path to a <code>.json</code> config file. It behaves exactly as the same as <code>str</code> does. <code>dict[str,Any]</code> This is expecting a pre-parsed config file (i.e. <code>with open('config.json', 'r') as f: data = json.load(f)</code>).It will first run a key check on the input vs. the default config, and if the input passes, it will simply assign the values directly to the <code>ConfigManager</code>'s <code>raw</code> config data, then parse them into the dataclasses. This one is a little strange because it technically bypasses the JSON stuff altogether, which is actually why it exists, to allow the Toolbox to just dump <code>Config</code>-like data directly into a <code>ConfigManager</code>, rather than needing an intermediate &gt; <code>.json</code> export step. <code>None</code> If <code>input_config</code> is <code>None</code> (default), the <code>ConfigManager</code> will attempt to load the default config file located at: <code>nectargan/config/default.json</code>. If it is unable to do so, it will raise an exception."},{"location":"api/trainers/trainer/#member-variables","title":"Member Variables","text":"<p>The base <code>Trainer</code> class has a few important member variables to be aware of, some of which are expected to be set by the class's which inherit from it. Here is each, with a short description: | Variable | Description | | :---: | --- | <code>self.log_losses</code> | This is a boolean variable which is set based on the value of the <code>log_losses</code> input argument in <code>Trainer.__init__()</code>. It defines whether losses run in the context of the given <code>Trainer</code> instance should be cached and occasionally dumped to the loss log (see here for more info). <code>self.current_epoch</code> | An integer value which keeps track of the current epoch. At init, this is <code>None</code>. Then, each time the <code>Trainer</code>'s <code>train_paired()</code> function is called, the value which is passed for the <code>epoch</code> argument first has <code>1</code> added to it, then it is assigned to <code>self.current_epoch</code>. This <code>+1</code> operation is just so that you can pass the raw iter variable from the training loop, and still have the first epoch be called <code>Epoch 1</code> everywhere where that would be relevant, like strings printed to the console for example. <code>self.last_epoch_time</code> | A floating point variable used to keep track of the time each epoch takes. The time is checked at the beginning and end of each epoch (in <code>Trainer.train_paired()</code>), then the start is subtracted from the end and the results is assigned to this variable. <code>self.train_loader</code> | A <code>torch.utils.data.DataLoader</code> representing the training dataset. This is expected to be set by the <code>Trainer</code> subclass, although the base <code>Trainer</code> class provides a utility function to perform the heavy lifting. See here and here for more info. <code>self.val_loader</code> | Exactly the same as <code>self.train_loader</code>, but for the validation dataset.</p>"},{"location":"api/trainers/trainer/#member-functions","title":"Member Functions","text":"<p>What follows is an exhaustive list of all of the member functions of the <code>Trainer</code> class, broken down by category. A brief description of each will be provided. For a more complete decription, please see the docstring for the given function, which can be accessed here by clicking on the function name.</p>"},{"location":"api/trainers/trainer/#initialization-functions","title":"Initialization Functions","text":"Function Description <code>init_config</code> Initializes the <code>Trainer</code>'s internal <code>ConfigManager</code>. <code>build_output_directory</code> Builds and experiment output directory based on the current config settings (or assigns the current experiment directory to <code>self.experiment_dir</code> if continuing a train on an existing model). <code>export_config</code> Exports a copy of the <code>Trainer</code>'s current config to a <code>.json</code> file in the experiment directory. <code>init_visdom</code> Initializes a Visdom client for the training session. <code>load_checkpoint</code> Initializes a given network and optimizer with pre-trained weights from a checkpoint file contained within the current experiment directory. <code>quicksetup</code> Performs a sequence of other init functions to quickly set up all infrastructure needed for training."},{"location":"api/trainers/trainer/#component-builders","title":"Component Builders","text":"Function Description <code>init_loss_manager</code> Initializes a <code>LossManager</code> to manage losses for the <code>Trainer</code> during training. <code>build_dataloader</code> Initializes a <code>torch.utils.data.DataLoader</code> from a <code>PairedDataset</code> based on settings in the <code>Trainer</code>'s current config. <code>build_optimizer</code> Simple helper function initialize an optimizer for a given network."},{"location":"api/trainers/trainer/#training-callbacks","title":"Training Callbacks","text":"<p>Training callbacks are a core feature of training with the NectarGAN API. They are exremely flexible override methods that allow you to define complex training loops for your custom <code>Trainer</code> class with just a few simple function. See here for more information. | Function | Description | | :---: | --- | <code>on_epoch_start</code> | Run at the very beginning of an epoch in <code>Trainer.train_paired()</code>, before the training loop begins. <code>train_step</code> | Run once per batch in <code>Trainer.train_paired()</code> -&gt; <code>Trainer._train_paired_core()</code>. <code>on_epoch_end</code> | Run at the very end of an epoch in <code>Trainer.train_paired()</code>, after all the batches for the epoch have been completed.</p>"},{"location":"api/trainers/trainer/#training-loop","title":"Training Loop","text":"Function Description <code>_train_paired_core</code> The core <code>Trainer</code> paired training loop, called by <code>Trainer.train_paired()</code> after it has completed it's epoch init steps and run the <code>on_epoch_start</code> callback. <code>train_paired</code> This is the core paired adversarial training function. It largely serves as a public wrapper around <code>_train_paired_core</code>, however, it also performs some additional epoch init steps and runs the <code>on_train_start</code> callback before the main loop, and the <code>on_train_end</code> callback after it is complete."},{"location":"api/trainers/trainer/#console-logging","title":"Console Logging","text":"Function Description <code>print_end_of_epoch</code> A function, meant to be run at the end of each epoch, which will print the time that the epoch took to complete to the console. The <code>Pix2pixTrainer</code> also extends this function to print a couple extra lines related to learning rate changes for G and D."},{"location":"api/trainers/trainer/#modelexample-image-saving","title":"Model/Example Image Saving","text":"Function Description <code>export_model_weights</code> A utility function which will take a network and its associated optimizer, and save a checkpoint for the network as a <code>.pth.tar</code> to the current experiment directory, tagged with the current epoch value at the time the function was called. <code>save_xyz_examples</code> This function will swap a given network over into eval mode, select a random set of images from the <code>self.val_loader</code> dataset (the number of images is defined by the value of ['config']['save']['num_examples'] in the config file), run the network's inference on each image, and for each image, export 3 <code>.png</code> files to the current experient directory's example subdirectory. Then it switches the network back in to train mode. ## Building a New Trainer The base <code>Trainer</code> class can be easily inherited from to create new <code>Trainer</code> subclasses for additional models. <p>See here for more information.</p>"},{"location":"getting_started/cli/","title":"NectarGAN - Getting Started (CLI)","text":""},{"location":"getting_started/cli/#this-section-will-walk-you-through-the-process-of-using-nectargan-from-the-command-line","title":"This section will walk you through the process of using NectarGAN from the command line.","text":"<p><code>Getting Started - Home</code></p> <p>[!NOTE] Up to this point, my focus has primarily been on the Toolbox interface. As such, the CLI options currently are rather limited. What does exist is really just a slightly expanded version of the scripts I originally used for validation of the Pix2pix implementation.</p> <p>Expanding CLI support is a high-priority task, so this is likely to change significantly in the near future.</p>"},{"location":"getting_started/cli/#training","title":"Training","text":""},{"location":"getting_started/cli/#reference-nectarganstarttrainingpaired","title":"Reference: <code>nectargan.start.training.paired</code>","text":"<p>Pix2pix training can be performed via CLI in NectarGAN with the following command:</p> <pre><code>nectargan-train-paired -f /path/to/config.json -lss extended+vgg --log_losses\n</code></pre> <p>There are currently only three command line arguments, all of which are shown here. Following is a brief explanation of each:</p>"},{"location":"getting_started/cli/#-f-config_file","title":"<code>-f</code> <code>--config_file</code>","text":"<p>Type: <code>string</code> (optional) Description: The system path to the config file to use for training. If this flag is not set, the default config (<code>nectargan/config/default.config</code>) will be used.</p>"},{"location":"getting_started/cli/#-lss-loss_subspec","title":"<code>-lss</code> <code>--loss_subspec</code>","text":"<p>[!IMPORTANT] The first time you run a loss subspec which includes <code>+vgg</code>, the VGG19 default weights (<code>IMAGENET1K_V1</code>) will be downloaded from PyTorch if you have not previously downloaded them in the Python environment which you are running the command from.</p> <p>Type: <code>string</code> (optional) Description: What loss subspec to use when initializing the objective function. Currently, the valid <code>loss_subspecs</code> are: 1. <code>basic</code>: (default) Objective function as described in Isola et al., Image-to-Image Translation with Conditional Adversarial Networks, CVPR 2017. 2. <code>basic+vgg</code>: <code>basic</code>, but with added VGG19-based perceptual loss (see here). 3. <code>extended</code>: <code>basic</code>, but with a few added loss functions:     - Mean squared error (L2)     - Sobel (large-scale structure, see here)     - Laplacian (small-scale structure, see here) 4. <code>extended+vgg</code>: <code>extended</code>, but with added VGG19-based perceptual loss.</p> <p>See <code>pix2pix_objective</code> for more info.</p>"},{"location":"getting_started/cli/#-log-log_losses","title":"<code>-log</code> <code>--log_losses</code>","text":"<p>Type: <code>action</code>, <code>store_true</code> (optional) Description: If this flag is present, the <code>LossManager</code> will be set up to cache loss values and occasionally dump the cache to the loss log.</p>"},{"location":"getting_started/cli/#-help","title":"<code>--help</code>","text":"<p>Description: Shows information about the script's arguments.</p>"},{"location":"getting_started/cli/#summary","title":"Summary","text":"<p>So, in short, all the actual training data is still pulled from the config file (see here), as with any other type of training in NectarGAN. This means you could duplicate the default config, set it up however you'd like, then just point the train script to it via the <code>-f</code> flag when running the <code>nectargan-train-paired</code>. </p> <p>It also keeps the actual CLI simple which I generally prefer, as now, only things which aren't present in the config, like <code>loss_subspec</code> (since it's just a drop-in objective function, adding it's parameters to the config is not necessary), or things which you might want to override, need to have CLI arguments.</p> <p>[!NOTE] When training via CLI rather than the Toolbox interface, you will likely want to use the alternative training visualization method, Visdom.</p> <p>This is very simple, just: 1. Open the config file you will be using for training  2. Find the entry for <code>config</code>/<code>visualizer</code>/<code>visdom</code>. 3. Under it, you will see a boolean value called <code>enable</code>. Set this to <code>true</code>. </p> <p>The rest of the settings in that section can be used to configure the behavior of the Visdom-based visualizer.</p> <p>See here for more information.</p>"},{"location":"getting_started/cli/#testing","title":"Testing","text":""},{"location":"getting_started/cli/#reference-nectarganstarttestingpaired","title":"Reference: <code>nectargan.start.testing.paired</code>","text":"<p>Pix2pix model testing can be performed via CLI in NectarGAN with the following command:</p> <pre><code>nectargan-test-paired -e /path/to/experiment/directory -l 200 -d /path/to/dataset/root -i 30\n</code></pre> <p>The testing script has five arguments, two of which are required with the rest being optional:</p>"},{"location":"getting_started/cli/#-e-experiment_directory","title":"<code>-e</code> <code>--experiment_directory</code>","text":"<p>Type: <code>string</code> (required) Description: The directory of the experiment to load for testing.</p>"},{"location":"getting_started/cli/#-l-load_epoch","title":"<code>-l</code> <code>--load_epoch</code>","text":"<p>Type: <code>int</code> (required) Description: The checkpoint epoch number to load for testing (i.e. <code>epoch{load_epoch}_netG.pth.tar</code>).</p>"},{"location":"getting_started/cli/#-f-config_file_1","title":"<code>-f</code> <code>--config_file</code>","text":"<p>Type: <code>string</code> (optional) Description: The system path to config file to use for testing. If this is not set, the testing script will first look for the latest training config in the given experiment directory (i.e. <code>train{x}_config.json</code>), which is, by default, exported automatically at the start of each training session. If this is set, it will use this config file instead.</p>"},{"location":"getting_started/cli/#-d-dataroot","title":"<code>-d</code> <code>--dataroot</code>","text":"<p>Type: <code>string</code> (optional) Description: The path to the dataset root directory to use for testing. If this is not set, the dataset root specified in the config file (whether that be the automatically selected one, or the one defined by <code>--config_file</code>) will be used for testing.</p> <p>[!NOTE] The given dataset root must have a <code>test</code> subdirectory containing dataset images for testing!</p>"},{"location":"getting_started/cli/#-i-test_iterations","title":"<code>-i</code> <code>--test_iterations</code>","text":"<p>Type: <code>int</code> (optional) Description: The number of images from the <code>test</code> set to run the model on. If not set, the default <code>10</code> will be used. The test script will grab this number of random images from the <code>test</code> set, run the model inference on them, and export the results to a <code>test</code> subdirectory inside of the given experiment directory.</p>"},{"location":"getting_started/cli/#-help_1","title":"<code>--help</code>","text":"<p>Description: Shows information about the script's arguments.</p>"},{"location":"getting_started/cli/#utils","title":"Utils","text":""},{"location":"getting_started/cli/#reference-nectarganutilsrebuild_default_config","title":"Reference: <code>nectargan.utils.rebuild_default_config</code>","text":"<p>Currently, there is only one utility accessible via CLI: </p> <pre><code>nectargan-utils-restoreconfig\n</code></pre> <p>This utility will restore the default config file, with all of its original settings, at its original location (<code>nectargan/config/default.json</code>). This can be used in the event that the origin config file gets lost, deleted, or otherwise broken beyond repair.</p> <p>By default, the util will not allow overwriting of an existing default config, and will instead raise an exception if the file exists. However, you may use the [<code>-o</code>, <code>--overwrite_existing</code>] flag to allow it to overwrite the existing file.</p>"},{"location":"getting_started/docker/","title":"NectarGAN - Getting Started (Docker)","text":""},{"location":"getting_started/docker/#this-section-will-walk-you-through-the-process-of-using-nectargan-from-a-docker-container","title":"This section will walk you through the process of using NectarGAN from a Docker container.","text":"<p><code>Getting Started - Home</code></p>"},{"location":"getting_started/docker/#before-you-start","title":"Before you start...","text":""},{"location":"getting_started/docker/#docker","title":"Docker","text":"<p>To follow this guide, you must first have Docker desktop installed on your host machine.</p> <p>To get started with Docker, see here: https://www.docker.com/</p>"},{"location":"getting_started/docker/#visdom","title":"Visdom","text":"<p>NectarGAN uses Visdom for data visualization when running inside of a container. A Visdom server is set up automatically during during build/compose and configured to listen to the main NectarGAN <code>app</code> service, and also to route its output through an open port on the container (<code>8000</code> by default). This will be explained in greated detail below.</p>"},{"location":"getting_started/docker/#buildingrunning-the-container","title":"Building/Running the Container","text":"<p>First, clone the repository:</p> <pre><code>git clone https://github.com/ZacharyBork/NectarGAN.git\n</code></pre> <p>Next, run:</p> <pre><code>cd NectarGAN/docker\n</code></pre> <p>From here, we can run:</p> <pre><code>docker compose build --build-arg TORCH_TYPE=cpu\n</code></pre> <p>Setting the value of <code>TORCH_TYPE</code> will tell the Dockerfile which PyTorch compute platform you intend on using so that it can download the wheel from the correct place. The build process will also automatically set the device in the <code>docker_nectargan_config.json</code> (explained below) to the correct value for the given platform. Currently, the available options are:</p> <ul> <li><code>cpu</code>\u00a0\u00a0\u00a0 : CPU-only</li> <li><code>cu126</code> : CUDA 12.6</li> <li><code>cu128</code> : CUDA 12.8</li> </ul> <p>The image size for the <code>cpu</code> build is ~2GB. The <code>cu1**</code> builds are ~12GB. If this arugment is not set, the build will default to <code>cpu</code>.</p> <p>After the build has finished, we can run:</p> <pre><code>docker compose up -d\n</code></pre> <p>After a few seconds, you should see messages that both the <code>app</code> and <code>visdom</code> services have started successfully, after which time, you can navigate to <code>http://localhost:8000</code> on your host machine to view the Visdom output.</p>"},{"location":"getting_started/docker/#files-and-structure","title":"Files and Structure","text":"<p>Let's have a quick look at the file and directory structure for NectarGAN's Docker build:</p> <pre><code>NectarGAN/\n\u2514\u2500\u2500 docker/\n    \u251c\u2500\u2500 mount/\n    \u2502   \u251c\u2500\u2500 docker_nectargan_config.json\n    \u2502   \u251c\u2500\u2500 input\n    \u2502   \u2514\u2500\u2500 output\n    \u251c\u2500\u2500 scripts/\n    \u2502   \u251c\u2500\u2500 config_editor.py\n    \u2502   \u251c\u2500\u2500 nectargan.py\n    \u2502   \u2514\u2500\u2500 etc.\n    \u251c\u2500\u2500 visdom/\n    \u2502   \u251c\u2500\u2500 Dockerfile\n    \u2502   \u2514\u2500\u2500 start_server.sh\n    \u251c\u2500\u2500 .dockerignore\n    \u251c\u2500\u2500 .env\n    \u251c\u2500\u2500 docker-compose.yaml\n    \u251c\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 entrypoint.sh\n</code></pre> <p>There are a couple key things to note:</p> <ol> <li>The <code>mount</code> directory: This gets mounted as a runtime volume.</li> <li><code>docker_nectargan_config.json</code>: This is the training/testing config file used by the scripts inside of the container. Note that, since it is in the <code>mount</code> directory, you are able to make live edits to this file with the container running.</li> <li>The <code>mount/input</code> directory: This is the directory where you will place your dataset directories for training and testing models inside of the container. This will be explained in greater detail below.</li> <li>The <code>mount/output</code> directory: This is the directory where NectarGAN will export data to (checkpoints, example images, logs). This will also be explained in greater detail below.</li> <li>The <code>.env</code> file: This file currently is only used to configure the Visdom endpoint for the container.</li> </ol> <p>Everything else is related to the build/compose process. The above files/directories are the primary things you will be using to interact with and change the behavior of your container.</p>"},{"location":"getting_started/docker/#interacting-with-the-container","title":"Interacting with the Container","text":"<p>With our container running, we can now run:</p> <pre><code>docker exec -it nectargan-app-1 nectargan\n</code></pre> <p>After doing so, you will be greeted with a screen which looks like this:</p> <p></p> <p>This is the NectarGAN Docker CLI wrapper. From here, we can run commands to interact with NectarGAN inside of the container. Currently, these commands are:</p>"},{"location":"getting_started/docker/#train","title":"<code>train</code>","text":"<p>This command allows you to run model training from within the container. The configuration values for the training session will be pulled from the <code>docker_nectargan_config</code> file, but can be edited from the CLI wrapper (explained below).</p> <p>When run, a Visdom client will be created in the <code>app</code> service to send loss data and example images to the <code>visdom</code> service, a new experiment directory will be created in the mounted <code>output</code> directory, and training will be begin. Soon after, you should begin to see logging information in your console and updates in Visdom on your host machine.</p> <p></p> <p>As shown in the above image, all of the training outputs (i.e. checkpoints, logs, example images) will be saved to the mounted <code>output</code> directory, so that they can be accessed immediately on the host machine.</p> <p>Currently, the only way to stop training prematurely is via <code>Ctrl+C</code>. This also exits the container shell meaning you need to re-run the <code>exec</code> command after. This will be fixed in a future update though.</p>"},{"location":"getting_started/docker/#test","title":"<code>test</code>","text":"<p>This command will run a multi-step guided process for initializing a model testing session. When you run it, you will be asked to:</p> <ol> <li>Choose an experiement to test. (from the mounted <code>output</code> directory)</li> <li>Choose an epoch to load.</li> <li>Choose a dataset to test the model on. (from the mounted <code>input</code> directory)</li> <li>Choose a number of test iterations to run. Each iteration will test the model's interence on a single image from the given dataset's <code>test</code> directory.</li> </ol> <p>All testing results (logs, output images) are exported to the experiment directory of the experiment being tested, in a subdirectory called <code>test</code>, inside of the mounted <code>output</code> directory.</p>"},{"location":"getting_started/docker/#dataset-set","title":"<code>dataset-set</code>","text":"<p>This command allows you to set the currently active dataset (<code>dataroot</code>) in the <code>docker_nectargan_config</code> file to any dataset directory in the mounted input directory.</p> <p>This is done by running the base command, followed by the name of the dataset directory. For example, if you had a dataset called <code>facades</code>, and you would like to train with this dataset inside of the container, first you would place your <code>facades</code> directory inside of the mouted input directory, giving you a directory structure like so:</p> <pre><code>NectarGAN/\n\u2514\u2500\u2500 docker/\n    \u251c\u2500\u2500 Dockerfile\n    \u251c\u2500\u2500 docker-compose.yaml\n    \u2514\u2500\u2500 mount/\n        \u251c\u2500\u2500 docker_nectargan_config.json &lt;--------- NectarGAN Docker training/testing config\n        \u2514\u2500\u2500 input/\n            \u2514\u2500\u2500 facades/ &lt;------------------------- Your dataset\n                \u251c\u2500\u2500 train/\n                \u2502   \u2514\u2500\u2500 training_images\n                \u251c\u2500\u2500 test/\n                \u2502   \u2514\u2500\u2500 traing_images\n                \u2514\u2500\u2500 val/\n                    \u2514\u2500\u2500 validation_images\n</code></pre> <p>Then, in the NectarGAN Docker CLI wrapper, we can run:</p> <pre><code>dataset-set facades # Replace \"facades\" with the name of your actual dataset directory.\n</code></pre> <p>This will set the config <code>dataroot</code> path to the facades directory in the mounted volume, if it exists. You will recieve a confirmation message in the status field if it is successful, as shown in the image below.</p> <p></p>"},{"location":"getting_started/docker/#config-edit","title":"<code>config-edit</code>","text":"<p>This command will begin a guided multi-step process which allows you to edit the current value of a field in the <code>docker_nectargan_config</code> file.</p> <p>Please note: since the <code>docker_nectargan_config</code> file is located inside of the mount directory, you may also edit the file in a text editor on your host machine, and updates will be picked up immediately. This is much easier for large changes. This command is largely provided as a convenience.</p>"},{"location":"getting_started/docker/#config-print","title":"<code>config-print</code>","text":"<p>Running this command will print the current config file in raw JSON format to the console.</p>"},{"location":"getting_started/docker/#shell","title":"<code>shell</code>","text":"<p>This will stop the NectarGAN Docker CLI wrapper, and drop you in to the container's native shell. The base image for the <code>nectargan-app</code> image is <code>python:3.12-slim-bookworm</code>, so it is a Debian shell. </p> <p>To return to the NectarGAN Docker CLI wrapper, you can run the command <code>nectargan</code>. Please note, though, that after doing this, the next time you run the <code>exit</code> command, rather than exiting the container, you will instead be placed back in the container shell, as that is the new parent process. Running <code>exit</code> again, though, will exit the container.</p> <p>This is also mostly provided as a convenience. It is the same as running <code>docker exec -it nectargan-app-1 /bin/sh</code> from the host machine.</p>"},{"location":"getting_started/docker/#exit","title":"<code>exit</code>","text":"<p>This will exit the interactive container shell (or exit to the interactive container shell, as noted above).</p>"},{"location":"getting_started/toolbox_review/","title":"NectarGAN - Getting Started (Toolbox | Review)","text":""},{"location":"getting_started/toolbox_review/#reference-nectargan-toolbox-documentation","title":"Reference: <code>NectarGAN Toolbox Documentation</code>","text":"<p>[!NOTE] This is part two of the three-part Toolbox quickstart guide.</p> <p>Click here for part one. Click here for part two.</p> <p>[!IMPORTANT] The review panel is still relatively early in development. Some features may be a little rough around the edges, and you may experience stalls when loading large experiment outputs. It is functional in its current state, but it will likely be overhauled at some point in the future.</p>"},{"location":"getting_started/toolbox_review/#the-review-panel","title":"The Review Panel","text":""},{"location":"getting_started/toolbox_review/#reference-toolbox-review-panel","title":"Reference: <code>Toolbox Review Panel</code>","text":"<p>The toolbox review panel can be accessed by clicking on the teal <code>Review</code> button on the left-hand bar. When we do, we are presented with a screen which looks like this:  Here, we can load the example images and graph the loss log data from a previous train.</p> <p>We will start by inputting the experiment directory for the model we trained in part 1. After that, we can click the <code>Load Experiment</code> button to load all the data from our training session. After just a second, we should see the training data loaded up like so: </p> <p>On the left side of the results panel, we can see that it has loaded all of the [<code>A_real</code>, <code>B_fake</code>, <code>B_real</code>] example image sets.</p> <p>Then, on the right, it has also gone through the loss log and graphed all of the loss data for each loss in the loss log, with the red line being the mean loss value, and the green line being the weight of the loss (more useful if you are scheduling loss weights, here it is just a flat line for each loss).</p> <p>[!NOTE] Graphing every single entry for every single loss can be a heavy task, especially for longer trains and/or larger datasets. For this reason, by default, the review panel will only graph every 50<sup>th</sup> value. This can be changed in the main settings panel, accessed via the red <code>Settings</code> button on the left-hand bar.</p> <p>We also see that, in the review settings panel, below where we input our experiment directory path, it has loaded the train config file from the experiment directory for viewing. If there are multiple train configs in the experiement directory, all of them will be loaded and you can switch between them with the dropdown menu in the top right of the <code>Train Config</code> display.</p>"},{"location":"getting_started/toolbox_review/#conclusion","title":"Conclusion","text":"<p>And that's it really, those are the core components of the NectarGAN Toolbox. Thank you for reading, I hope you enjoy working with it :)</p> <p>If you have suggestions or find any issues, please feel free to let me know!</p> <p>For further reading, please see the NectarGAN Toolbox Documentation.</p>"},{"location":"getting_started/toolbox_testing/","title":"NectarGAN - Getting Started (Toolbox | Testing)","text":""},{"location":"getting_started/toolbox_testing/#reference-nectargan-toolbox-documentation","title":"Reference: <code>NectarGAN Toolbox Documentation</code>","text":"<p>[!NOTE] This is part two of the three-part Toolbox quickstart guide.</p> <p>Click here for part one. Click here for part three.</p>"},{"location":"getting_started/toolbox_testing/#the-testing-panel","title":"The Testing Panel","text":"<p>[!IMPORTANT] The <code>Testing</code> panel will use the generator architecture and dataset settings defined in their repective panels (i.e. <code>Experiment</code> and <code>Dataset</code>).</p> <p>If these UI settings have changed since you trained the model, you can load the settings used for training by clicking the <code>Load Setting from Config File</code> button on the bottom of the <code>Experiment</code> panel, and selecting the config file from the given experiment directory, which is exported automatically at the beginning of each train.</p>"},{"location":"getting_started/toolbox_testing/#reference-toolbox-testing-panel","title":"Reference: <code>Toolbox Testing Panel</code>","text":""},{"location":"getting_started/toolbox_testing/#testing-a-model","title":"Testing a Model","text":"<p>The <code>Testing</code> panel can be accessed by clicking on the green <code>Testing</code> button on the left-hand bar. After doing so, you will be presented with a screen which looks like this:  Here, you can load your generator checkpoints which were exported during training and evaluate them on real test images.</p> <p>To test our model, we will: 1. Input the path to our experiment directory from the training session we ran in the previous chapter (the experiment directory is the root directory containing the <code>.pth.tar</code> checkpoint files, <code>examples</code> directory, etc.). 2. Click on the <code>Most Recent</code> button to have the toolbox grab the most recent generator checkpoint in the experiment directory. Or, alternatively, manually inputting the epoch number of a checkpoint we would like to test. 3. Increase the value of <code>Test Iterations</code> to 30. This will tell the <code>Tester</code> that we would like it to randomly select 30 images from the <code>test</code> directory. 4. Click <code>Begin Test</code>. </p>"},{"location":"getting_started/toolbox_testing/#test-results","title":"Test Results","text":"<p>After a couple seconds, you should be presented with a screen which looks like this: </p> <p>[!NOTE] Your images may be larger. Image scale can be controlled with the slider on the bottom of the results panel.</p> <p>Here we can see that, for each testing iteration, we are displaying the example input and ground truth images, as well as the generators fake image.</p> <p>We also have some loss values. For each test iteration, before converting the images for display, we also take the tensors and run a few loss functions on them. You can sort the test results by these metrics, as well as a few others, using the dropdown menus on the bottom right of the results panel.</p> <p>Now, let's hit <code>Begin Test</code> one more time. When we do, we see that the model has been run on another random selection of images from the <code>test</code> set:  Now, if we click on the dropdown menu called <code>Test Version</code>, located in the bottom left of the results panel, we will see that we have two items, <code>test_01</code> and <code>test_02</code>. This allows you to switch back and forth between the tests to compare. Note that sorting settings will also be preserved when switching between them</p>"},{"location":"getting_started/toolbox_testing/#the-test-directory","title":"The <code>test</code> Directory","text":"<p>Now, let's quickly have a look at our experiment output directory for the experiment we are testing. In doing so, we notice a new subdirectory called <code>test</code>. Diving inside this new directory, we can see that, for each test we ran, asubdirectory was created inside. </p> <p>Then, in these test subdirectories, the [<code>A_real</code>, <code>B_fake</code>, <code>B_real</code>] image sets for each test iteration were exported, and a <code>log.json</code> was created which contains all of the data related to that test, including: - Experiment name. - Experiment version. - Path to the experiment directory. - Test version. - Path to the test output directory.</p> <p>And then, for each test iteration: - Iteration number - Path to the input image which was used for that iteration. - Paths to the three output images from that iteration. - Loss values tagged by name.</p> <p>This is how the test data is loaded into the Toolbox interface. It also makes it extremely easy to track all data related to model testing and integrate it quickly into automated workflows.</p> <p>And that's really all there is to testing models with the NectarGAN Toolbox. </p>"},{"location":"getting_started/toolbox_testing/#next-lets-move-on-to-reviewing-training-results-using-the-review-panel","title":"Next, let's move on to reviewing training results using the <code>Review</code> panel.","text":""},{"location":"getting_started/toolbox_training/","title":"NectarGAN - Getting Started (Toolbox | Training)","text":""},{"location":"getting_started/toolbox_training/#a-graphical-tool-for-training-and-testing-models-reviewing-results-of-previous-tests-converting-models-to-onnx-and-testing-the-resulting-model-and-processing-dataset-files-all-packaged-into-a-single-modern-and-easy-to-use-interface","title":"A graphical tool for training and testing models, reviewing results of previous tests, converting models to ONNX and testing the resulting model, and processing dataset files, all packaged into a single modern and easy to use interface..","text":""},{"location":"getting_started/toolbox_training/#reference-nectargan-toolbox-documentation","title":"Reference: <code>NectarGAN Toolbox Documentation</code>","text":"<p>[!NOTE] This is part one of the three-part Toolbox quickstart guide.</p> <p>Click here for part two. Click here for part three.</p> <p>For instructions on how to install NectarGAN, please see here.</p> <p>[!TIP] For this walkthrough, we will be using the ubiquitous <code>facades</code> Pix2pix dataset, kindly provided by the University of California, Berkeley. If you would like to follow along using this dataset, please see here for information regarding the dataset download script which will allow you to automatically download this and other Pix2pix-style datasets.</p> <p>[!IMPORTANT] Currently, it is not possible to set the PyTorch target device from the Toolbox UI. The default is <code>cuda</code>. If you intend to do your training on the CPU only, please first find the config file located at <code>nectargan/config/default.json</code> and set the value of <code>config.common.device</code> to <code>cpu</code>.</p> <p>This was an oversight and will be fixed in a future UI update.</p>"},{"location":"getting_started/toolbox_training/#launching-the-toolbox","title":"Launching the ToolBox","text":"<p>With NectarGAN installed in your Python environment via your preferred method, you can launch the Nectargan Toolbox by running the following command inside of the Python environment where NectarGAN is installed:</p> <pre><code>nectargan-toolbox\n</code></pre> <p>Or, alternatively, by this command from the repository root (i.e. <code>NectarGAN/</code>):</p> <pre><code>python -m nectargan.start.toolbox\n</code></pre> <p>The Toolbox will open to a screen which looks like this: </p>"},{"location":"getting_started/toolbox_training/#experiment-settings","title":"Experiment Settings","text":""},{"location":"getting_started/toolbox_training/#reference-toolbox-experiment-panel","title":"Reference: <code>Toolbox Experiment Panel</code>","text":"<p>On this screen, we can define output settings for our current experiment, and also define architecture options for our generator and discriminator, if we so choose.</p> <p>For now, we will: 1. Select an output root, either by pasting the path into the <code>Output Root</code> or by using the <code>Browse</code> button and selecting a directory with the file explorer. The output root is, as the name suggests, the root output directory, wherein a unique subdirectory will created for each experiment which is run. 2. Set an <code>Experiment Name</code>. This will be used to name the experiment output subdirectory. For now, we will set it to <code>facades</code>.  3. As we have no other versions yet, we will leave <code>Version</code> at <code>1</code>, though it should be noted that <code>Version</code> is always kind of optional, as experiment outputs will be versioned automatically regardless.</p>"},{"location":"getting_started/toolbox_training/#dataset-settings","title":"Dataset Settings","text":"<p>Then, we will move on to <code>Dataset</code> options by clicking the blue <code>Dataset</code> button on the left hand bar. After doing so, you will be presented with a screen that looks like this:  Here, you can control settings related to your dataset, including load and crop size, load direction, and a full suite of built in augmentation options. Clicking on the check box next to the augmentation type will expand the augmentation options for that type. The names are fairly self-explanatory, but just to be sure: - <code>Input</code> : Augmentations applied only to the input image (<code>A</code> in <code>AtoB</code>, <code>B</code> in <code>BtoA</code>) - \u00a0<code>Both</code>\u00a0 : Augmentation applied to both the input and the target image.</p> <p>For now, we will: 1. Unzip the <code>facades</code> dataset (twice), if we haven't already. 2. Set the <code>Dataset Root</code> to the path to the <code>facades</code> dataset folder.</p> <p>[!IMPORTANT] The path we input for <code>Dataset Root</code> should be the root directory for the dataset, i.e. the directory which contains the <code>train</code>, <code>test</code>, and <code>val</code> subdirectories. 3. If we look at one of our dataset images, we can see it has a resolution of <code>[512x256]</code>, meaning each image (i.e. <code>input</code>, <code>target</code>) has a resolution of <code>[256x256]</code>. As such, we do not need to change our <code>Load Size</code> or <code>Crop Size</code> here, as the default values are set to both load and random crop at <code>256^2</code> which, with this dataset, will just load images at full resolution and apply no cropping. 4. We will change the <code>Direction</code> to <code>BtoA</code>. Looking again at our dataset images, we can see that the image on the right, the <code>B</code> image, is the semantic mask. We want to turn that in to the image on the left (<code>A</code>), the building facade image. If you'd rather see the model try to learn the semantic segmentation task instead though, feel free to leave it at <code>AtoB</code>.</p> <p>[!NOTE] A quick note on Pix2pix datasets, and their practical application in NectarGAN:</p> <p>Looking at our <code>facades</code> dataset directory, we can see that it has three subdirectories: <code>train</code>, <code>test</code>, and <code>val</code>. This is standard for Pix2pix datasets, though the percentage splits vary based on a number of factors including model application and dataset size.</p> <p>NectarGAN uses these in a very standard way as well: | Set | Usage | | :---: | --- | <code>train</code> | The images in this set are used during training to condition the generator and discriminator.  <code>val</code> | The images in this set are used during training to generate example outputs. They provide a set of \"clean data\" which the generator has never seen, upon which to test the generator's inference capabilities. <code>test</code> | This set is optional. It is used during testing (addressed in the next section) as another \"clean\" dataset on which to test previously trained models.</p>"},{"location":"getting_started/toolbox_training/#training-settings","title":"Training Settings","text":"<p>And finally, we're ready to move on to the training settings, which can be done by clicking on the purple <code>Training</code> button on the left-hand bar. After doing so, you will be greeted with a window which looks like this:  Here, you can set various options related to learning rate schedules for the generator and discriminator, settings related to loss weighting and logging, and also some options related to checkpoint and example image saving during training.</p> <p>Also of note is the <code>Continue Train</code> checkbox. Clicking this will present you with the option to select an epoch to load, and then when you click train, the checkpoint file for that epoch will be loaded to continue model training.</p> <p>For now, we don't need to change anything here. The default settings will initialize the learning rate schedule and loss weights as they were outlined in the original Pix2pix paper[^1], but you are encouraged to come back after this walkthrough to start playing around with the settings here, they can change the behaviour of the generator in lots of fun and interesting ways depending on the task.</p>"},{"location":"getting_started/toolbox_training/#start-training","title":"Start Training","text":"<p>So then with all that done, we are ready to start training! This can be done by clicking the <code>Begin Training</code> training button. When we do, a few things will happen: 1. The UI will lock. Most settings can't be changed during training with the exception of some in the main settings panel (accessed via the red <code>Settings</code> button on the left-hand bar.) 2. A new subdirectory will be created in the <code>Output Root</code> named <code>{experiment_name}_v{version}</code>. 3. In that subdirectory, a few things will be created: a <code>loss_log.json</code>, a <code>train1_config.json</code>, and a subdirectory called  <code>examples</code> for exporting example images to during training. 4. We will be presented with two new buttons, <code>Pause Training</code>, to pause (and continue) training at any point, and <code>Stop Training</code>, which will stop training altogether and return the UI to its previous state. 5. After just a second, we will start seeing example output images, and losses will start being graphed. The rate at which these updates happen can be changed (even during training) in the main <code>Settings</code> panel. 6. After each epoch, the model will be loaded automatically in eval mode and run on an image from the <code>val</code> set, and an example image set will be exported to the <code>examples</code> directory in the experiment directory. 7. After a few epochs (5 if you have followed this guide exactly up until this point), cached loss data will be exported to the loss log.</p> <p>And that's really all there is to training with the NectarGAN Toolbox. Let's let the model train for a bit (maybe 10 or 20 epochs, shown by the epoch counter in the bottom right), then we'll come back once we have some checkpoint files and example images to play with. When you are finished, just click the big red <code>Stop Train</code> button and the current train will be halted.</p>"},{"location":"getting_started/toolbox_training/#toolbox-training-with-the-facades-dataset-epoch-196","title":"Toolbox training with the <code>facades</code> dataset (epoch 196).","text":""},{"location":"getting_started/toolbox_training/#output","title":"Output","text":"<p>If we have a quick look at our experiment output directory, we should see something like this: </p> <p>These are the files which have been created by our training session: 1. Every 5 epochs (based on our settings in the <code>Training</code> panel), two <code>.pth.tar</code> files were exported. These are tar archives containing the model parameters for the generator and discriminator respectively, as denoted by the <code>G</code> and <code>D</code> in the file names. 2. <code>loss_log.json</code>, a file containing all of the logged loss information (namely mean values and weights, both at <code>float32</code>). 3. <code>train1_config.json</code>, a carbon copy of the config that was used to run the training. Were we to continue training on the model, a new file, <code>train2_config.json</code> would be created. 4. A subdirectory called <code>examples</code>. Looking inside... </p> <pre><code>... we see that each epoch (at the end of the given epoch), we also were evaluating the model on images from the `eval` set, and then exporting the example [`A_real`, `B_fake`, `B_real`] set.\n</code></pre> <p>In a later section, we will see how we can use the Toolbox to load all of this data for review. For now though, since we have our <code>.pth.tar</code> checkpoint files, let's move on to...</p>"},{"location":"getting_started/toolbox_training/#model-testing","title":"Model Testing","text":"<p>Click here to proceed to the Toolbox testing quickstart guide.</p> <p>[^1]: Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2017)</p>"},{"location":"toolbox/dataset/","title":"NectarGAN Toolbox - Dataset","text":"<p><code>NectarGAN Toolbox - Home</code></p>"},{"location":"toolbox/dataset/#this-section-houses-settings-related-to-dataset-loading-and-train-time-data-augmentations","title":"This section houses settings related to dataset loading and train-time data augmentations.","text":"<p>[!IMPORTANT] All data augmentations have a chance (defined by their percentage parameter) of being applied any time a dataset image is loaded, Not just the first time a given image is loaded. So a dataset image may be loaded with one set of augmentations one epoch, and a completely different set the next epoch.</p>"},{"location":"toolbox/dataset/#dataset-files","title":"Dataset Files","text":"Setting Description Dataset Root The root directory for your dataset files. This directory should have subdirectories called <code>train</code>, <code>val</code>, and, optionally, <code>test</code>. These subdirectories should contain your paired (A Load Size The resolution (^2) to load each dataset image at. This is separate from <code>Crop Size</code>, which is the actual resolution of the images used during training, although they can be the same value if you don't want to use random cropping. Crop Size The resolution (^2) of the images used during training. If this is smaller than <code>Load Size</code>, an Albumentations RandomCrop operation will be performed to reach this target size. ### Loading Setting Description :---: --- Direction What direction to load the dataset (or, in simpler terms, which image is input and which is output where A is the left image, and B is the right image.) Batch Size The batch size to use during training. ### Augmentations (Input) Setting Description :---: --- Colorjitter The percentage chance to apply random colorjitter to any given input image. Colorjitter Range The min and max allowed values of the colorjitter if it is applied. Gaussian Noise The percentage chance to apply gaussian noise to any given input image. Gaussian Range The min and max range of the gaussian noise. Motion Blur The percentage chance to apply a pseudo-motion blur effect to any given input image. Motion Blur Limit The maximum kernal size for the blur operation. The minimum allowed kernal size is 3 so this value should be &gt;3. Random Gamma The percentage chance to randomize the gamma of any given input image. Gamma Limits The upper and lower bounds (in %) of the random gamma operation. Grayscale The percentage chance to convert any image to grayscale Grayscale Method What method to use when converting dataset images to grayscale. Compression The percentage chance to apply simulated compression artifacts to any given dataset image. Compression Type What type of compression artifacts to apply (e.g. JPEG, WEBP). Compression Quality Min and max quality for the applied compression operation. ### Augmentations (Both) Setting Description :---: --- Horizontal Flip The precentage chance to flip each image of any given dataset image along the horizontal axis. Vertical Flip The percentage chance to flip each image of any given dataset image along the vertical axis. 90\u00b0 Rotation The percentage chance to apply a random 90\u00b0 stepped rotation to each image of any given dataset pair. Elastic Xform The percentage chance to apply an elastic transformation to each image of any given dataset pair. Elastic Alpha Scaling factor for the elastic distortion. Higher values will produce more deformation. Elastic Sigma Smoothing factor for the distortions. Higher values will produce smoother results. Optical Distort Percentage chance to apply an optical distortion effect to any given dataset image. Distortion Limits Min and max range of the applied distortions. Distortion Mode What type of optical distortion to apply (e.g. camera or fisheye distortion). Coarse Dropout The percentage chance to apply coarse dropout (i.e. random rectangular cutouts) to any given dataset image. Hole Count The min and max number of holes to add if coarse dropout is applied to an image. Hole Height Min and max allowable height of each hole. Hole Width Min and max allowable width of each hole. <p>See Also: | Toolbox - Home | Experiment | Training | Testing | Review | Utilities | Settings |</p>"},{"location":"toolbox/experiment/","title":"NectarGAN Toolbox - Experiment","text":"<p><code>NectarGAN Toolbox - Home</code></p>"},{"location":"toolbox/experiment/#here-you-will-initialize-output-settings-and-architectural-settings-for-the-generator-and-discriminator","title":"Here you will initialize output settings and architectural settings for the generator and discriminator.","text":"Setting Description Output Root Root output directory. A new experiment directory will be created in this directory each time you hit train. Experiment Name The name of the experiment. This will be used to name the experiment output directory. Version The version number to append to the experiment number. When conducting multiple experiments with the same name, this parameter is optional. Experiment output directories will be versioned automatically. Features Number of output features on the first downsampling layer. The generator model has a feature cap equal to this value * 8. Down Layers Number of downsampling layers (and matching upsampling layers) to add when assembling the generator. Block Type What block type to use for the generator (i.e. UNet block, Residual UNet block). Upsampling Type What type of upsampling to perform in the generator (i.e. TransposedConv2D or bilinear upsampling + Conv2D). Transposed convolution is oftentimes responsible for the \"checkerboard\" artifacting that is notorious in pixel-to-pixel GANs. See here for more info. Layer Count Number of downsampling layers to add to the discriminator. Base Channels Number of output channels on the first discriminator layer. Max Channels Maximum allowed channel count for any given layer in the disriminator. <p>See Also: | Toolbox - Home | Dataset | Training | Testing | Review | Utilities | Settings |</p>"},{"location":"toolbox/review/","title":"NectarGAN Toolbox - Review","text":"<p><code>NectarGAN Toolbox - Home</code></p>"},{"location":"toolbox/review/#this-section-is-used-for-reviewing-results-of-previous-experiments","title":"This section is used for reviewing results of previous experiments.","text":"<p>You point it to a previous experiment directory, and it will load all of the example image sets and graph all of the loss log data for review.</p> Setting Description Experiment Path The system path to the experiment directory for the experiment you would like to review. Load Experiment Pressing this button will load the experiment review data for the experiment defined by <code>Experiment Path</code>. Config Selector If there are multiple train config files in the given experiment directory (e.g. <code>train1_config.json</code>, <code>train2_config.json</code>, etc.), this dropdown will let you switch between them in the config file viewer. <p>See Also: | Toolbox - Home | Experiment | Dataset | Training | Testing | Utilities | Settings |</p>"},{"location":"toolbox/settings/","title":"NectarGAN Toolbox - Settings","text":"<p><code>NectarGAN Toolbox - Home</code></p>"},{"location":"toolbox/settings/#this-panel-contains-general-interface-settings-there-is-not-much-here-currently-but-more-will-likely-be-added-in-the-future","title":"This panel contains general interface settings. There is not much here currently, but more will likely be added in the future.","text":"<p>[!IMPORTANT] Any value in this panel can be changed during training. For some, this has no effect but for settings like <code>Training Update Rate</code>, this will change the rate at which the UI is updated live during training. One small thing to note is that for the live loss graphs, this will also change the rate at which values are added which can look a little strange so if you are relying on the live loss graphs, set this value prior to training and then leave it unchanged for the duration of the training session.</p> Setting Description Always on Top If checked, the Toolbox window will always remain on top of other windows. Review Graph Sample Rate When loading loss log data from the review panel, it is generally not advisable to load every single datapoint from the log, as that can take an unreasonable long time, especially for logs from longer training sessions. This value allows you to set a sample rate to be used when loading that data, and the graphing backend will skip this number of datapoints from the log in between each datapoint that does get added to the graph. Setting this to <code>1</code> will tell the Toolbox to load every datapoint, but expect the UI to lock while it does this. In the future, I may look in to offloading this to another thread with some progress feedback. Training Update Rate Using Toolbox, model training is run in a separate thread from the main program. Every so often, the worker in that thread will send a signal back to the main thread with some data (loss data, timing information, example image tensors, etc.). This value lets you set the frequency (in iterations) that the signal is sent. This will chance the rate at which the example images are updates, and which losses are printed to the console and added to their respective graphs. Higher values can speed up training at the cost of reduced real-time feedback. <p>See Also: | Toolbox - Home | Experiment | Dataset | Training | Testing | Review | Utilities |</p>"},{"location":"toolbox/testing/","title":"NectarGAN Toolbox - Test","text":"<p><code>NectarGAN Toolbox - Home</code></p>"},{"location":"toolbox/testing/#here-you-will-find-settings-related-to-testing-previously-trained-models","title":"Here you will find settings related to testing previously trained models.","text":"Setting Description Experiment The system path to the experiment directory for the experiment you would like to test. Load Epoch The checkpoint epoch you would like to load for testing. Test Iterations The number of images from the test dataset to run the model on. Override Dataset If enabled, this will allow you to input the path to a dataset to use from testing, rather than using the one defined on the <code>Dataset</code> tab. Override Dataset Path If override dataset is enabled, this path will be used for the test dataset rather than the one in the <code>Dataset</code> tab. Test Version Each time a test is run on an experiment, it will be added to this dropdown menu so that you can switch between them. Image Scale Scale multiplier for the displayed images. Reset Scale Resets images back to their initial scale. Sort By Sorts the test results by various metrics. Sort Direction Changes the direction of the test result sorting (i.e. Ascending, Descending). <p>See Also: | Toolbox - Home | Experiment | Dataset | Training | Review | Utilities | Settings |</p>"},{"location":"toolbox/training/","title":"NectarGAN Toolbox - Training","text":"<p><code>NectarGAN Toolbox - Home</code></p>"},{"location":"toolbox/training/#here-you-will-find-settings-related-to-model-training-loss-weighting-and-logging-and-example-and-checkpoint-saving","title":"Here you will find settings related to model training, loss weighting and logging, and example and checkpoint saving.","text":""},{"location":"toolbox/training/#continue-training","title":"Continue Training","text":"Setting Description Continue Train If checked, training will be continued from a previous set of checkpoint files. Load Epoch What epoch checkpoints to load when continuing training from previously trained weights. ### Generator Setting Description :---: --- Epochs How many epochs to run at the <code>Initial</code> learning rate. Epochs Decay Over how many epochs to decay from the <code>Initial</code> learning rate, down to the <code>Target</code> learning rate. Initial The initial learning rate to use when training the model. The model will be trained at this learning rate for a number of epochs defined by <code>Epochs</code>. Target The learning rate which, after the initial epochs are completed, the scheduler will decay the generator's learning rate to over a number of epochs defined by <code>Epochs Decay</code>. Beta1 The momentum term for the optimizer (Adam in this case). Higher values will allow the optimizer to more rapidly respond to bad habits learned by the generator during training, but can cause also cause instability. ### Discriminator Setting Description :---: --- Separate Learning Schedules If checked, the discriminator can be assigned a different learning rate schedule than the one used for the generator. Epochs See Generator options. Epochs Decay See Generator options. Initial See Generator options. Target See Generator options. Beta1 See Generator options. ### Loss Setting Description :---: --- GAN The weight value to apply to the generators adversarial loss. Higher values will cause the generator to try harder to fool the discriminator, at the cost of higher deviation from the ground truth. L1 The weight value to apply to the generators L1 Loss. Higher values will penalize the generator more harshly for pixel-wise devations from the ground truth. L2 MSELoss, basically <code>L1^2</code>. The behaves very similarly to L1 except the bigger the pixel-wise error, the more, relatively speaking, the generator is punished. Sobel Sobel structure loss. Derives a loss value for the generator by applying a Sobel operation to the generated fake and the ground truth, and comparing the two results with a pixel-wise loss function (L1 in this implementation). Higher values can cause the generator to better preserve large scale structural detail. Laplacian Laplacian structure loss. Similar to Sobel loss but with a Laplacian kernel instead of a Sobel kernel. Higher values can cause the generator to preserve more fine, textural detail from ground truths. VGG Perceptual loss with VGG19. Takes the generated image and the ground truth, feeds each to the pre-trained VGG19 image classification model and extracts feature maps at various depths, then compares the feature maps for each with L1 loss. This creates a cost function that encourages the generator to create images that are visually similar to the ground truth images, while punishing it far less harshly for small, pixel-level deviations. This can sometimes encourage the generator create images with extremely high visual realism, and also help reduce the blurring (or averaging) you sometimes see with traditional L1 loss in pixel to pixel models. Log Losses During Training If enables, loss data will be cached during training and periodically dumped to the loss log in the experiment directory. Log Dump Frequency (epochs) The frequency, in epochs, to dump the cached loss values to the log file. Realistically, unless you are working on very large datasets with relatively high epoch counts, it is safe to have the value fairly high. <p>See Also: | Toolbox - Home | Experiment | Dataset | Testing | Review | Utilities | Settings |</p>"},{"location":"toolbox/utilities/","title":"NectarGAN Toolbox - Utilities","text":"<p><code>NectarGAN Toolbox - Home</code></p>"},{"location":"toolbox/utilities/#here-you-will-find-some-general-utitilies-related-to-onnx-conversion-and-testing-and-various-dataset-processing-tools","title":"Here you will find some general utitilies related to ONNX conversion and testing, and various dataset processing tools.","text":""},{"location":"toolbox/utilities/#convert-to-onnx","title":"Convert to ONNX","text":"<p>This utility allows you to convert your trained model to ONNX format to allow for easier deployment into pipelines which support the ONNX runtime. </p> <p>[!NOTE] The Pix2pix model uses instance normalization, but PyTorch considers instanceNorm as training so it will print a warning to the console during conversion. This is expected and does not affect the model's inference ability or quality. As such, it is silenced in the Toolbox implementation, along with another irrelevant opset-based constant folding warning. If you are using the ONNXConverter class to convert your model, you can choose to disable the silencing of these warnings with: <code>ONNXConverter.convert_model(suppress_onnx_warnings: bool=True)</code>.</p> <p>See here for more information on the supressed warning related to instance normalization.</p> Setting Description Experiment The system path to the experiment directory of the model you would like to convert. Load Epoch What epoch checkpoint to load for conversion. In Channels Number of input channels for the model. Currently, the only supported value is 3 but this will be expanded in the future Width/Height The desired width and height of the converted model's model's input tensor. Target Device What device the ONNX model should target (i.e. CPU, CUDA). Opset Version What ONNX opset version to use. See here for more information. Export Params Decides whether model weights should be exported. Fold Constants Decides whether constants should be folded during conversion for optimization. Convert Model Runs the ONNX conversion process and exported the converted model. ## Test ONNX Model This utility allows you to test the inference capabilities of your converted ONNX model. By feeding it a path to a model and hitting <code>Test Model</code>, Toolbox will load the ONNX model in inference mode and run it on every image in the provided <code>Test Images</code> directory, displaying the (input, output) results in the interface. Setting Description :---: --- Model The system path to the <code>.onnx</code> file to run inference testing on. Test Images The system path to a directory of test images. These should not be paired training-style images. They should just be model input images, on their own, with no ground truth example. Test Model Begins the model testing process. This generally only takes a couple seconds for ~50 test images, though it will depend on the input width and height of your specific model. ### Pair Images This is an extemely fast pairing utility for (A, B) dataset images. It will take two input directories, one with the input images and the other with the ground truth images, a pairing direction, and, optionally, a target resolution. Then Toolbox will send the task to a worker in another thread which pools the processes to very rapidly scale, if enabled, and pair the images into Pix2pix ready training data. This was very fun to build. <p>[!NOTE] This utility expects that the two input directories contain nothing but the image files to be used in the pairing operation, and that the A input images have the same file name as their B input counterpart.</p> Setting Description Input A The system path to the directory containing the first set of images to pair. Input B The system path to the directory containing the second set of images to pair. Output The system path to the directory that you would like the utility to output the paried images to. Direction What direction to pair the images. As the names suggest, <code>AtoB</code> will put the input A images on the left and the input B images on the right. <code>BtoA</code> will do the opposite. This is sort of arbitrary if you're training with NectarGAN since you can select the direction at train time, but some may find it useful for their pipelines so I've decided to include it. Scale Images If this checkbox is ticked, the images will first be scaled to the resolution defined by the accompanying dropdown list before being paired. Scaling is applied to an image copy in memory at script runtime and will not affect your original image files. Image Scale If the <code>Scale Images</code> checkbox is enabled, this dropdown allows you to select a resolution to scale each image to before pairing. The final resolution of each paired output image will be (<code>Image Scale</code>*2, <code>Image Scale</code>). ## Image Sorting Utilities These three utilities, <code>Sort Images</code>, <code>Remove Sorting Tags</code>, and <code>Copy Sort</code>, allow you to sort image files non-destructively by various metrics, unsort them back to their original order, or copy the sorting order of one directory to another. This is useful for managing very large datasets and helping to pull bad images to the top so you can more easily find ones that should be discarded. This tool functions in the same way as the pairing utility and as such, is very efficient, even on extremely large datasets. <p>Some things to be aware of when using these tools:</p> <p>[!WARNING] These tools rename files in-place and can modify many files very quickly. Please read this section fully and use the Preview first. Before working with these tools on real data for the first time, it is advised to duplicate a small portion of your dataset to test their functionality. 1. Renaming behavior (and undo).    The sorter renames files in your <code>Input</code> directory by prepending a tag that reflects their position in the sorted list. You can reverse this with Remove Sorting Tags, which strips the sorting tag from the files, effectively unsorting them, even if some files were removed.</p> <ol> <li> <p>Always run a Preview first (dry run).    Every tool has a Preview button. The preview shows exactly what would be changed without touching your files. Run Preview before Start. These tools are fast and perform bulk file operations. Most actions are reversible via Remove Sorting Tags, but it\u2019s always safer to confirm first.</p> </li> <li> <p>Only include the images you intend to sort.    The directory should contain image files only. Non-image files currently trigger an exception that isn\u2019t handled by the UI, requiring an app restart. This will be improved in a future update. For now, make sure the folder contains only the images you want to sort.</p> </li> </ol>"},{"location":"toolbox/utilities/#sort-images","title":"Sort Images","text":"Setting Description Input The system path to the directory containing the image files you would like to sort. Type What image metric to sort by (e.g. # of white or black pixels, mean pixel value, RMS contrast, etc.). These are just a collection of sorting functions that I have found useful in my own time spent processing datasets. When I find more useful ones, they will likely get added as well. Direction What direction to sort (i.e. Ascending, Descending) Start Begins the sorting operation. Preview Displays a preview in the interface of what the results would be if the current sorting operation was run. ### Remove Sorting Tags Setting Description :---: --- Input The system path to the directory containing images which have previously been sorted with <code>Sort Images</code>, and which currently have sorting tags which you wish to remove (e.g. <code>1_myfile543.png</code>, <code>2_myfile198.png</code>, <code>3_myfile31.png</code>). Start Start the remove tags operation. This is not threaded and will lock the UI until it is completed, but behind the scenes, it is just a sequential pathlib rename operation so even for fairly large datasets, it should be very quick. Preview Display a preview of what the result of the current <code>Remove Tags</code> operation would be. ### Copy Sort This utility allows you to copy the sorting order from one directory of pre-sorted (with <code>Sort Images</code>) images, to the paired set of images. Helpful to see how eliminating some datapoints from A would affect the data in B. &gt; [!NOTE] &gt; This utility relies on the original file names (before sorting tags are prepended) of the images in the input and target directory to be the same, i.e. if an image from the input directory was original called <code>myfile153.png</code> before it was sorted, its twin in the target directory should also be called <code>myfile153.png</code>. Setting Description From The system path to the directory of sorted images to copy the sorting order from. To The system path to the directory of unsorted images to copy the sorting order to. Start Begins the copy sort operation. This will lock the UI briefly while it processes, it is generally fairly quick even on large datasets, though. Preview Preview the result of the current copy operation."},{"location":"toolbox/utilities/#split-dataset-images","title":"Split Dataset Images","text":"<p>This utility takes a directory of pre-paired dataset images, an <code>Output</code> directory, and a percentage for each of <code>train</code>, <code>test</code>, <code>val</code>. It then creates new directories for each category inside out the <code>Output</code> directory and roughly splits the provided dataset images into the new directories based on their respective percentage chance.</p> <p>[!NOTE] There are three percentage sliders, each with a corresponding spinbox. All three go up to 100%. In the split script, they are normalized based on the sum total of the three values, so you can use the sliders to get a rough estimate, or enter precise values in the spinboxes to get a split closer to the exact percentages. The split process is entirely stochastic so the resulting datasets likely will not be split in the exact specified percentage, but it will be within a relatively small margin of error.</p> Setting Description Input The system path to the input directory containing the dataset images you would like to split. Nothing else should be present in the directory apart from the dataset images. Output The system path to the root output directory where you would like the split dataset to be exported to. A subdirectory each for <code>train</code>, <code>test</code>, and <code>val</code> will be created inside of this directory, into which the images will be placed. Test Split percentage for the <code>test</code> set. Train Split percentage for the <code>train</code> set. Validate Split percentage for the <code>val</code> set. Start Begins the splitting operation. This is also not threaded and, technically speaking, does lock the UI. But it is extremely fast. With smaller datasets, it oftentimes doesn't even appear that the UI locks because the copy operation is over so quickly. Preview Displays a preview of the results of the current copy operation were it to be run. <p>See Also: | Toolbox - Home | Experiment | Dataset | Training | Testing | Review | Settings |</p>"}]}